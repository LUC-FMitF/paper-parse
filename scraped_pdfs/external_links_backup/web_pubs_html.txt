Source URL: http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html#paxos-commit
Final URL: https://lamport.azurewebsites.net/pubs/pubs.html#paxos-commit
================================================================================

<HTML> <HEAD>
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1"> </HEAD>

<BODY style="max-width:700px; padding:10px; background-color: #fffff0;">


<title>The Writings of Leslie Lamport</title>
  <H1>My Writings </H1> 
  <H2>Leslie Lamport</H2>
  <I>Last modified 15 May 2025</I>


<P>This document is a sort of scientific
autobiography.&nbsp; It not only lists the papers I have written, but also
describes them and explains how I came to write some of them.&nbsp; I have
included almost all my technical papers and electronic versions
of many of them for downloading.&nbsp; Omitted are some papers for which I
no longer have copies and papers that are incomplete.&nbsp; I have also
omitted early versions of some of these papers--even in cases where
the title changed.&nbsp; Included are some initial drafts of papers that I
abandoned before fixing errors or other problems in them.&nbsp; A table of
contents precedes the descriptions.&nbsp; I also include a brief
<i>curriculum vitae</i>.&nbsp; A printable version of this document
is available as a <A HREF ="pubs.pdf">pdf file</A>.&nbsp;

<P> Each description attempts to explain the genesis of the
work.&nbsp; However, I have forgotten how I came to write most of my
papers.&nbsp; Even when I discourse at length about the development of the
ideas, I am giving only a subjective view based on my unreliable
memory.&nbsp; Whenever possible, I have asked other people involved to
check the accuracy of what I've written.&nbsp; However, what I have most
often forgotten is the work of others that influenced my own work.&nbsp;
This may give the impression that I am claiming more credit for ideas
than I deserve, for which I apologize.&nbsp;

<P> 
Where I think it's interesting, I give the story behind the
publication or non-publication of a paper.&nbsp; Some of the stories read
like complaints of unfair treatment by editors or referees.&nbsp; Such
cases are bound to arise in any activity based on human judgment.&nbsp; On
the whole, I have had little trouble getting my papers published.&nbsp; In
fact, I have profited from the natural tendency of editors and
referees to be less critical of the work of established scientists.&nbsp;
But I think it's worth mentioning the cases where the system didn't
work as it should.&nbsp;

<P>
I would like to have ordered my papers by the date they were written.&nbsp;
However, I usually have no record of when I actually wrote something.&nbsp;
So, I have ordered them by date of publication or, for unpublished
works, by the date of the version that I have.&nbsp; Because of long and
variable publication delays, this means that the order is only
approximately chronological.&nbsp;

<P>
Whenever possible, 
 I have included
electronic versions of the works.&nbsp; I have electronic versions of most
works written after about 1985.&nbsp; For journal articles, these may be
"preprint" versions, formatted differently and sometimes differing
slightly from the published versions.&nbsp; I have been scanning papers for
which I don't have source files as I find copies of them.&nbsp;

<HR><H3>Education</H3><UL>
<LI> B.S. -- M.I.T., 1960. (Mathematics) 
<LI> M.A. -- Brandeis University, 1963. (Mathematics) 
<LI> Ph.D. -- Brandeis University, 1972. (Mathematics)
</UL>

<H3>Employment</H3><UL>
<LI> Mitre Corporation (part-time, 1962-1965)
<LI> Marlboro College (1965-1969)
<LI> Massachusetts Computer Associates (1970-1977)
<LI> SRI International (1977-1985)
<LI> Digital Equipment Corporation / Compaq (1985-2001)
<LI> Microsoft Research (2001-present)
</UL>

<H3>Honors</H3><UL>
<LI> National Academy of Engineering (1991)
<LI> PODC Influential Paper Award (2000) 
                    (for paper <A HREF = "#time-clocks">[27]</A>)
<LI> Honorary Doctorate, University of Rennes (2003)
<LI> Honorary Doctorate, Christian Albrechts University,
                  Kiel (2003)
<LI> Honorary Doctorate, Ecole Polytechnique
                  F�d�rale de Lausanne (2004)
<LI> IEEE Piore Award (2004) 
<LI> Edsger W. Dijkstra Prize in Distributed Computing (2005)
                    (for paper <A HREF = "#reaching">[41]</A>)
<LI> Honorary Doctorate, Universit� della Svizzera Italiana,
Lugano (2006)
<LI> ACM SIGOPS Hall of Fame Award (2007) 
                     (for paper <A HREF = "#time-clocks">[27]</A>)
<LI> Honorary Doctorate, 
                  Universit� 
                    Henri Poincar�, Nancy (2007)
<LI> LICS 1988 Test of Time Award (2008) 
                     (for paper <A HREF = "#abadi-existence">[92]</A>)
<LI> IEEE John von Neumann Medal (2008) 
<LI> National Academy of Sciences (2011) 
<LI> ACM SIGOPS Hall of Fame Award (2012) 
                     (for paper <A HREF = "#lamport-paxos">[123]</A>)
<LI> Jean-Claude Laprie Award in Dependable Computing (2013) 
                     (for paper <A HREF = "#byz">[46]</A>)
<LI> ACM SIGOPS Hall of Fame Award (2013) 
                     (for paper <A HREF = "#chandy">[66]</A>)

<LI> 2013 ACM Turing Award (2014) 

<LI> American Academy of Arts and Sciences (2014) 

<LI> Jean-Claude Laprie Award in Dependable Computing (2014) 
                     (for paper <A HREF = "#sift">[30]</A>)

<LI> Edsger W. Dijkstra Prize in Distributed Computing (2014)
                    (for paper <A HREF = "#chandy">[66]</A>)
<LI> Honorary Doctorate, Brandeis University (2017)
<LI> Fellow of the Computer History Museum (2019)
<LI> NEC C&C Prize (2019)
<LI> NSA Annual Best Scientific Cybersecurity Paper Competition
award (2022) (for paper <A HREF = "#hyper2">[191]</A>)
                    
</UL>





<HR><small><P><A NAME = "endofpage">When I created this Web page,
search engines weren't very good and vandals had not yet invaded
the Web.&nbsp;  I put the search string
  <tt>alllamportspubsontheweb</tt>
here to make this page easy to find, and I asked people not to put
that string on the Web.&nbsp; Search engines are now better and
vandals have put the string on other Web pages.&nbsp; These
days, searching for <q>lamport my writings</q> works fine.</small> 

<P><HR><H2>Contents</H2> <OL>
<A HREF = "#bxscience"><LI>Braid Theory</A>
<A HREF = "#summer-vision"><LI>Summer Vision Programs</A>
<A HREF = "#monitor1"><LI>Preliminary User's Guide to
Monitor&nbsp;1</A>
<A HREF = "#advanced-calculus"><LI>Untitled Draft of Advanced 
    Calculus Text</A>
<A HREF = "#geometry-of-space-time"><LI>The Geometry of Space and Time</A>
<A HREF = "#hash"><LI>Comment on Bell's Quadratic Quotient 
Algorithm</A>
<A HREF = "#thesis"><LI>The Analytic Cauchy Problem with Singular 
Data</A>
<A HREF = "#thesis-note"><LI>An Extension of a Theorem of Hamada on the Cauchy
Problem with Singular Data</A>
<A HREF = "#coordinate"><LI>The Coordinate Method for the Parallel
Execution of DO Loops</A>
<A HREF = "#do-loops"><LI>The Parallel Execution of DO Loops</A>
<A HREF = "#hyperplane"><LI>The Hyperplane Method for an Array
   Computer</A>
<A HREF = "#bakery"><LI>A New Solution of Dijkstra's Concurrent Programming
   Problem</A>
<A HREF = "#on-self-stabilizing-systems"><LI>On Self-stabilizing
Systems</A>
<A HREF = "#prog-parallel"><LI>On Programming Parallel
   Computers</A>
<A HREF = "#parallel-execution"><LI>Parallel Execution on Array and Vector
   Computers</A>
<A HREF = "#multiple-byte"><LI>Multiple Byte Processing with Full-Word
   Instructions</A>
<A HREF = "#synchronization"><LI>The Synchronization of Independent
   Processes</A>
<A HREF = "#comments"><LI>Comments on `A Synchronization
   Anomaly'</A>
<A HREF = "#multi-garbage"><LI>Garbage Collection with Multiple Processes:
   an Exercise in Parallelism</A>
<A HREF = "#coord-method"><LI>The Coordinate Method for the Parallel Execution of
   Iterative Loops</A>
<A HREF = "#multi-user"><LI>Towards a Theory of Correctness for Multi-User
   Data Base Systems</A>
<A HREF = "#glitch"><LI>On the Glitch Phenomenon</A>
<A HREF = "#proving"><LI>Proving the Correctness of Multiprocess
   Programs</A>
<A HREF = "#formal"><LI>Formal Correctness Proofs for Multiprocess
   Algorithms</A>
<A HREF = "#rd-wr"><LI>Concurrent Reading and Writing</A>
<A HREF = "#state-the-problem"><LI>State the Problem Before 
Describing the Solution</A>
<A HREF = "#time-clocks"><LI>Time, Clocks and the Ordering of Events in a
   Distributed System</A>
<A HREF = "#interactive"><LI>The Specification and Proof of Correctness of
   Interactive Programs</A>
<A HREF = "#implementation"><LI>The Implementation of Reliable Distributed
   Multiprocess Systems</A>
<A HREF = "#sift"><LI>SIFT: Design and Analysis of a Fault-Tolerant Computer for
   Aircraft Control</A>
<A HREF = "#garbage"><LI>On-the-fly Garbage Collection: an Exercise in
    Cooperation</A>
<A HREF = "#general-construction"><LI>A General Construction for
Expressing Repetition</A>
<A HREF = "#new-approach"><LI>A New Approach to Proving the Correctness of
   Multiprocess Programs</A>
<A HREF = "#howto"><LI>How to Present a Paper</A>
<A HREF = "#multi"><LI>How to Make a Multiprocessor Computer That Correctly
   Executes Multiprocess Programs</A>
<A HREF = "#dig-sig"><LI>Constructing Digital Signatures from a One Way
   Function</A>
<A HREF = "#calendar"><LI>On the Proof of Correctness of a Calendar
   Program</A>
<A HREF = "#letter-to-editor"><LI>Letter to the Editor</A>
<A HREF = "#sometime"><LI>`Sometime' is Sometimes `Not
   Never'</A>
<A HREF = "#hoare"><LI>The `Hoare Logic' of Concurrent Programs</A>
<A HREF = "#reaching"><LI>Reaching Agreement in the Presence of Faults</A>
<A HREF = "#program"><LI>Program Verification: An Approach to Reliable Hardware
   and Software</A>
<A HREF = "#password"><LI>Password Authentication with Insecure
   Communication</A>
<A HREF = "#lamport-timesets"><LI>TIMESETS--A New Method for Temporal 
   Reasoning About Programs</A>
<A HREF = "#trans"><LI>Byzantine Generals and Transaction Commit Protocols</A>
<A HREF = "#byz"><LI>The Byzantine Generals Problem</A>
<A HREF = "#liveness"><LI>Proving Liveness Properties of Concurrent
   Programs</A>
<A HREF = "#dist"><LI>An Assertional Correctness Proof of a Distributed
   Program</A>
<A HREF = "#nonatomic"><LI>Reasoning About Nonatomic
   Operations</A>
<A HREF = "#spec"><LI>Specifying Concurrent Program Modules</A>
<A HREF = "#spec-and-proof"><LI>Specification and Proof of a
   Fault-Tolerant Real-Time Algorithm</A>
<A HREF = "#weak-byz"><LI>The Weak Byzantine Generals Problem</A>
<A HREF = "#phil"><LI>PHIL: A Semantic Structural Graphical Editor</A>
<A HREF = "#what-good"><LI>What Good Is Temporal Logic?</A>
<A HREF = "#using-time"><LI>Using Time Instead of Timeout for
   Fault-Tolerant Distributed Systems</A>
<A HREF = "#ghl"><LI>The Hoare Logic Of CSP, and All That</A>
<A HREF = "#clocks2"><LI>Byzantine Clock Synchronization</A>
<A HREF = "#solved-and-unsolved"><LI>Solved Problems, Unsolved Problems 
    and NonProblems in Concurrency</A>
<A HREF = "#peterson-theorem"><LI>On a "Theorem" of Peterson</A>
<A HREF = "#buridan"><LI>Buridan's Principle</A>
<A HREF = "#mutual"><LI>The Mutual Exclusion Problem--Part I: A Theory of
   Interprocess Communication, Part II: Statement and
   Solutions</A>
<A HREF = "#clocks"><LI>Synchronizing Clocks in the Presence of Faults</A>
<A HREF = "#priority"><LI>What It Means for a Concurrent Program to Satisfy a
   Specification: Why No One Has Specified Priority</A>
<A HREF = "#alias"><LI>Constraints: A Uniform Approach to Aliasing and
   Typing</A>
<A HREF = "#recursive-compiling"><LI>Recursive Compiling and Programming
Environments (Summary)</A>
<A HREF = "#chandy"><LI>Distributed Snapshots: Determining Global States of a
   Distributed System</A>
<A HREF = "#munich"><LI>Formal Foundation for Specification and
   Verification</A>
<A HREF = "#semantics"><LI>An Axiomatic Semantics of Concurrent Programming
   Languages</A>
<A HREF = "#latex"><LI>LaTeX: A Document Preparation
   System</A>
<A HREF = "#interprocess"><LI>On Interprocess Communication--Part I: Basic 
   Formalism, Part II: Algorithms</A>
<A HREF = "#the-byz-generals"><LI>The Byzantine Generals</A>
<A HREF = "#formal-basis"><LI>A Formal Basis for the Specification of 
   Concurrent Systems</A>
<A HREF = "#fast-mutex"><LI>A Fast Mutual Exclusion Algorithm</A>
<A HREF = "#derivation-simple"><LI>Derivation of a Simple Synchronization
   Algorithm</A>
<A HREF = "#distributed-system"><LI>Distribution</A>
<A HREF = "#document-production"><LI>Document Production: Visual or
   Logical?</A>
<A HREF = "#synchronizing-time-servers"><LI>Synchronizing Time Servers</A>
<A HREF = "#control"><LI>Control Predicates Are Better than Dummy Variables for
   Representing Program Control</A>
<A HREF = "#lamport-ewd1013"><LI>"EWD 1013"</A>
<A HREF = "#position"><LI>Another Position Paper on Fairness</A>
<A HREF = "#welch-lattice"><LI>A Lattice-Structured Proof of a Minimum Spanning 
   Tree Algorithm</A>
<A HREF = "#simple-approach"><LI>A Simple Approach to Specifying Concurrent
   Systems</A>
<A HREF = "#pretending"><LI>Pretending Atomicity</A>
<A HREF = "#abadi-realizable"><LI>Realizable and Unrealizable Specifications of
   Reactive Systems</A>
<A HREF = "#old-tla-src"><LI>A Temporal Logic of Actions</A>
<A HREF = "#lamport-win"><LI><i>win</i> and <i>sin</i>: Predicate Transformers
   for Concurrency</A>
<A HREF = "#lamport-theorem"><LI>A Theorem on Atomicity in Distributed
   Algorithms</A>
<A HREF = "#lamport-chapter"><LI>Distributed Computing: Models and
   Methods</A>
<A HREF = "#lamport-completion"><LI>A Completeness Theorem for TLA</A>
<A HREF = "#lamport-concurrent-clocks"><LI>The Concurrent Reading and
   Writing of Clocks</A>
<A HREF = "#lamport-mutual-solved"><LI>The Mutual Exclusion Problem Has Been
   Solved</A>
<A HREF = "#abadi-existence"><LI>The Existence of Refinement 
  Mappings</A>
<A HREF = "#abadi-preserving"><LI>Preserving Liveness: Comments on `Safety and
   Liveness from a Methodological Point of
   View'</A>
<A HREF = "#lamport-critique"><LI>Critique of the Lake Arrowhead
   Three</A>
<A HREF = "#lamport-reduction-tlanote"><LI>The Reduction 
   Theorem</A>
<A HREF = "#engberg-mechanical"><LI>Mechanical Verification of Concurrent Systems
   with TLA</A>
<A HREF = "#reconfiguration-patent"><LI>Reconfiguration system and method for high-speed mesh
connected local area network</A>
<A HREF = "#abadi-composing"><LI>Composing
   Specifications</A>
<A HREF = "#kurshan-multiplier"><LI>Verification of a Multiplier: 64 Bits and
   Beyond</A>
<A HREF = "#lamport-verification"><LI>Verification and Specification of
   Concurrent Programs</A>
<A HREF = "#lamport-hybrid"><LI>Hybrid Systems in
   TLA+</A>
<A HREF = "#lamport-how-to-write"><LI>How to Write a Proof</A>
<A HREF = "#lamport-actions"><LI>The Temporal Logic of Actions</A>
<A HREF = "#abadi-decomposing"><LI>Decomposing Specifications of Concurrent
   Systems</A>
<A HREF = "#abadi-open"><LI>Open Systems in
   TLA</A>
<A HREF = "#tlz"><LI>TLZ (Abstract)</A>
<A HREF = "#lamport-old-fashioned"><LI>An Old-Fashioned Recipe for Real
   Time</A>
<A HREF = "#lamport-ftrtft94"><LI>Specifying and Verifying Fault-Tolerant
   Systems</A>
<A HREF = "#lamport-howtowrite"><LI>How to Write a Long Formula</A>
<A HREF = "#intro-to-tla"><LI>Introduction to TLA</A>
<A HREF = "#adding-process-algebra"><LI>Adding "Process Algebra" to 
    TLA</A>
<A HREF = "#ccs-proofs"><LI>What Process Algebra Proofs Use Instead of 
   Invariance</A>
<A HREF = "#abadi-conjoining"><LI>Conjoining
   Specifications</A>
<A HREF = "#lamport-pictures"><LI>TLA in Pictures</A>
<A HREF = "#broy-specification-problem"><LI>The RPC-Memory Specification Problem:
   Problem Statement</A>
<A HREF = "#abadi-dagstuhl"><LI>A TLA Solution to the RPC-Memory Specification
   Problem</A>
<A HREF = "#automobile"><LI>How to Tell a Program from an Automobile</A>
<A HREF = "#refinement"><LI>Refinement in State-Based Formalisms</A>
<A HREF = "#lamport-drummers"><LI>Marching to Many Distant 
   Drummers</A>
<A HREF = "#lamport-eye-of-beholder"><LI>Processes are in the Eye of the
   Beholder</A>
<A HREF = "#lamport-how-to-make"><LI>How to Make a Correct Multiprocess Program
   Execute Correctly on a Multiprocessor</A>
<A HREF = "#substitution"><LI>Substitution: Syntactic versus Semantic</A>
<A HREF = "#lamport-paxos"><LI>The Part-Time Parliament</A>
<A HREF = "#cohen-tlareduction"><LI>Reduction in TLA</A>
<A HREF = "#lamport-composition"><LI>Composition: A Way to Make Proofs
   Harder</A>
<A HREF = "#lamport-possibility"><LI>Proving Possibility
   Properties</A>
<A HREF = "#ladkin-gerth"><LI>A Lazy Caching Proof in TLA</A>
<A HREF = "#lamport-spec-tla-plus"><LI>Specifying Concurrent Systems with
   TLA+</A>
<A HREF = "#fm99"><LI>TLA+ Verification of Cache-Coherence
  Protocols</A>
<A HREF = "#lamport-types"><LI>Should Your Specification Language Be
   Typed?</A>
<A HREF = "#yuanyu-model-checking"><LI>Model Checking TLA+
   Specifications</A>
<A HREF = "#lamport-latex-interview"><LI>How (La)TeX changed the face of
   Mathematics</A>
<A HREF = "#lamport-fairness"><LI>Fairness and Hyperfairness</A>
<A HREF = "#www9"><LI>Archival References to Web Pages</A>
<A HREF = "#disk-paxos"><LI>Disk Paxos</A>
<A HREF = "#disk-paxos-disc"><LI>Disk Paxos (Conference Version)</A>
<A HREF = "#when-mutex"><LI>When Does a Correct Mutual Exclusion Algorithm
  Guarantee Mutual Exclusion</A>
<A HREF = "#consensus-bounds"><LI>Lower Bounds on Consensus</A>
<A HREF = "#wildfire-challenge"><LI>The Wildfire Challenge Problem</A>
<A HREF = "#paxos-simple"><LI>Paxos Made Simple</A>
<A HREF = "#spec-and-verifying"><LI>Specifying and Verifying Systems with 
  TLA+</A>
<A HREF = "#arbiter-free"><LI>Arbiter-Free Synchronization</A>
<A HREF = "#ds-interview"><LI>A 
Discussion With Leslie Lamport</A>
<A HREF = "#bertinoro"><LI>Lower Bounds for Asynchronous
Consensus</A>
<A HREF = "#tla+-book"><LI>Specifying Systems: The TLA+
Language and Tools for Hardware and Software Engineers</A>
<A HREF = "#fmsd"><LI>Checking Cache-Coherence Protocols with
TLA+</A>
<A HREF = "#high-level"><LI>High-Level Specifications: Lessons from
Industry</A>
<A HREF = "#future-of-computing"><LI>The Future of Computing: Logic or 
Biology</A>
<A HREF = "#paxos-commit"><LI>Consensus on Transaction Commit</A>
<A HREF = "#hair"><LI>On Hair Color in France</A>
<A HREF = "#wsfm-web"><LI>Formal Specification of a Web Services
Protocol</A>
<A HREF = "#web-dsn-submission"><LI>Cheap Paxos</A>
<A HREF = "#combining"><LI>Implementing and Combining Specifications</A>
<A HREF = "#lower-bound"><LI>Lower Bounds for Asynchronous Consensus</A>
<A HREF = "#generalized"><LI>Generalized Consensus and Paxos</A>
<A HREF = "#real-simple"><LI>Real Time is Really Simple</A>
<A HREF = "#PaxosGST"><LI>How Fast Can Eventual Synchrony Lead to
Consensus?</A>
<A HREF = "#charme2005"><LI>Real-Time Model Checking is Really
Simple</A>
<A HREF = "#fast-paxos"><LI>Fast Paxos</A>
<A HREF = "#celebrity"><LI>Measuring Celebrity</A>
<A HREF = "#dcas"><LI>Checking a Multithreaded Algorithm with +CAL</A>
<A HREF = "#pluscal"><LI>The PlusCal Algorithm Language</A>
<A HREF = "#spec-book-chap"><LI>TLA+</A>
<A HREF = "#synchronizing"><LI>Implementing Dataflow With Threads</A>
<A HREF = "#commentary-web"><LI>Leslie Lamport: The Specification Language
TLA+</A>
<A HREF = "#state-machine"><LI>Computation and State Machines</A>
<A HREF = "#keappa08-web"><LI>A TLA+ Proof System</A>
<A HREF = "#mailbox-web"><LI>The Mailbox Problem</A>
<A HREF = "#teaching-concurrency"><LI>Teaching Concurrency</A>
<A HREF = "#vertical-paxos"><LI>Vertical Paxos and Primary-Backup
Replication</A>
<A HREF = "#deroever-festschrift"><LI>Computer Science and State
Machines</A>
<A HREF = "#reconfiguration-tutorial"><LI>Reconfiguring a State Machine</A>
<A HREF = "#stoppable"><LI>Stoppable Paxos</A>
<A HREF = "#pnueli"><LI>Temporal Logic: The Lesser of Three
  Evils</A>
<A HREF = "#verifying-safety"><LI>Verifying Safety Properties With the 
  TLA+ Proof System</A>
<A HREF = "#web-byzpaxos"><LI>Byzantizing Paxos by Refinement</A>
<A HREF = "#disc-leaderless-web"><LI>Leaderless Byzantine Paxos</A>
<A HREF = "#euclid"><LI>Euclid Writes an Algorithm: A
Fairytale</A>
<A HREF = "#proof"><LI>How to Write a 21st Century Proof</A>
<A HREF = "#tlaps"><LI>TLA+ Proofs</A>
<A HREF = "#wired"><LI>Why We Should Build 
Software Like We Build Houses</A>
<A HREF = "#adaptive"><LI>Adaptive Register Allocation with a Linear Number
of Registers</A>
<A HREF = "#coalescing"><LI>Coalescing: Syntactic Abstraction for Reasoning in
 First-Order Modal Logics</A>
<A HREF = "#blueprints"><LI>Who Builds a House without Drawing Blueprints?</A>
<A HREF = "#turing"><LI>The Computer Science of Concurrency: The Early Years</A>
<A HREF = "#auxiliary"><LI>Auxiliary Variables in TLA+</A>
<A HREF = "#eatcs"><LI>If You�re Not Writing a Program,
 Don�t Use a Programming Language</A>
<A HREF = "#recursive-ops"><LI>Recursive Operator Definitions</A>
<A HREF = "#toolbox"><LI>The TLA+ Toolbox</A>
<A HREF = "#simple"><LI>Prophecy Made Simple</A>
<A HREF = "#hyper2"><LI>Verifying Hyperproperties with TLA</A>
<A HREF = "#dbakery-cacm"><LI>Deconstructing the Bakery to Build a Distributed
State Machine</A>
<A HREF = "#dijkstra"><LI>Concurrent Algorithms</A>
<A HREF = "#rigor"><LI>Making Math More Rigorous</A>
<A HREF = "#statistics"><LI>Some Data on the Frequency of Errors in Mathematics
   Papers</A>
<A HREF = "#science-book"><LI>A Science of Concurrent Programs</A>
</OL><P><HR><h2>The Papers</h2><OL>


<LI> <A NAME = "bxscience"><B>Braid Theory</B><BR>Mathematics Bulletin of the Bronx High School of 
Science (1957), pages 6,7, and 9.<BR>
<A HREF = "bxscience.pdf">PDF</A><HR>


This appears to be my first publication, written when I was a high
school student.&nbsp; It shows that I was not a child prodigy.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "summer-vision"> <B>Summer Vision Programs</B><BR>Massachusetts
Institute of Technology, Project MAC Memorandum MAC-M-332, Artificial
Intelligence Project Memo Number Vision 111 (October 1966).<BR>
<A HREF = "summer-vision.pdf">PDF</A><HR>


In the summer of 1966, I worked at the M.I.T. Artificial Intelligence
Laboratory, doing Lisp programming for a computer vision project.&nbsp; I
have no memory of this document, but it appears to describe the
programs I wrote that summer.&nbsp; It's of no technical interest, but it
does show that, even in those days, I was writing precise
documentation.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "monitor1"><B>Preliminary User's Guide to
Monitor&nbsp;1</B>&nbsp; (with Roland Silver)<BR>Mitre Technical Report
(December 1966).<BR>
<A HREF = "monitor1.pdf">PDF</A><HR>


While in graduate school, I worked summers and part-time at the Mitre
Corporation from 1962 to 1965.&nbsp; I think I wrote three or four
technical reports there, but this is the only one the Mitre library
seems to have.&nbsp; A large part of my time at Mitre was spent working on
the operating system for a computer being built there called Phoenix.&nbsp;
This is the operating system's manual, apparently written by Silver
based on work we had both done.&nbsp; There is nothing of technical
interest here, but it provides a snapshot of what was going on in the
world of computers in the early 60s.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "advanced-calculus"><B>Untitled Draft of Advanced 
    Calculus Text</B><BR>Unpublished (<i>circa</i> 1967).<BR>
No electronic version available.<HR>


During the 1965-1969 academic years, I taught math at Marlboro
College.&nbsp; I don't remember exactly when or how the project got
started, but I wrote the first draft of an advanced calculus textbook
for Prentice-Hall, from whom I received an advance of 500
dollars.&nbsp; (That sum, which seems ridiculously small now, was a
significant fraction of my salary at the time.)&nbsp; The Prentice-Hall
reviewers liked the draft.&nbsp; I remember one reviewer commenting that
the chapter on exterior algebra gave him, for the first time, an
intuitive understanding of the topic.&nbsp; However, because of a letter
that was apparently lost in the mail, the Prentice-Hall editor and I
both thought that the other had lost interest in the project.&nbsp; By the
time this misunderstanding had been cleared up, I was ready to move on
to other things and didn't feel like writing a final draft.&nbsp; (This was
before the days of computer text processing, so writing a new draft
meant completely retyping hundreds of pages of manuscript.)&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "geometry-of-space-time"><B>The Geometry of Space and Time</B><BR>Unpublished
(<i>circa</i> 1968).<BR>
No electronic version available.<HR>


Marlboro College, where I taught math from 1965-1969, had a weekly
series of lectures for the general public, each given by a faculty
member or an outside speaker invited by a faculty member.&nbsp; I gave a
lecture about relativity that I later turned into this short
monograph.&nbsp; I made a half-hearted, unsuccessful effort to get it
published.&nbsp; But it was too short (75 pages) to be a "real" book, and
there was very little interest in science among the general public in
the late sixties.&nbsp; I think this monograph is still a very good
exposition of the subject.&nbsp; Unfortunately, the second half, on general
relativity, is obsolete because it says nothing about black holes.&nbsp;
While black holes appear in the earliest mathematical solutions to the
equations of general relativity, it was only in the late 60s that many
physicists began seriously to consider that they might exist and to
study their properties.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "hash"><B>Comment on Bell's Quadratic Quotient 
Algorithm</B><BR><i>Communications of the ACM 13</i>, 9 &nbsp; (September 1970).<BR>
<A HREF = "hash.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1970 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


This short note describes a minor inefficiency I noticed in a
hash-table algorithm published by James Bell.&nbsp; It got me thinking
about hash tables, and I invented what I called the linear quotient
algorithm--an algorithm that seems quite obvious in retrospect.&nbsp;
While I was running simulations to gather data for a paper on that
algorithm, the latest issue of <i>CACM</i> arrived with a paper by
Bell and Charles Kaman titled <i>The Linear Quotient Hash Code</i>.&nbsp; I
had devised three variants of the algorithm not contained in their
article.&nbsp; So, I wrote a paper about those three variants and submitted
it to <i>CACM</i>.&nbsp; The editor rejected it, without sending
it out for review, saying that it was too small a contribution to
merit publication.&nbsp; In the next few years, <i>CACM</i> published two
papers by others on the subject, each completely devoted to one of my
three variants.&nbsp; (Those papers had been submitted to different
editors.)&nbsp; My paper, which was probably a Massachusetts Computer
Associates (Compass) technical report, has been lost.&nbsp; (Compass went
out of business a few years ago, and I presume that its library was
destroyed.)&nbsp; The linear quotient method is probably the most common
hash-coding algorithm used today.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "thesis"><B>The Analytic Cauchy Problem with Singular 
Data</B><BR>Ph.D. Thesis, Brandeis University (1972).<BR>
<A HREF = "thesis.pdf">PDF</A><HR>


I left Marlboro College and went back to Brandeis in 1969 to complete
my Ph.D.&nbsp; At that time, I intended to study and write a
thesis in mathematical physics.&nbsp; However, I wound up doing a thesis in
pure mathematics, on analytic partial differential equations.&nbsp; I
learned nothing about analytic partial differential equations except
what was needed for my thesis research, and I have never looked at
them since then.&nbsp; The thesis itself was a small, solid piece of very
classical math.&nbsp; Had Cauchy arisen from the grave to read it, he would
have found nothing unfamiliar in the mathematics.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "thesis-note"><B>An Extension of a Theorem of Hamada on the Cauchy
Problem with Singular Data</B><BR><i>Bulletin of the Amer.&nbsp; Math.&nbsp; Society 79</i>, 4 (July 1973), 776-780.<BR>
<A HREF = "thesis-note.pdf">PDF</A><HR>




At the time, and perhaps still today, a math student "copyrighted"
his (seldom her) thesis results by announcing them in a short note in
the <i>Bulletin of the AMS</i>.&nbsp; Normally, a complete paper
would be published later.&nbsp; But I never did that, since I left math for
computer science after completing my thesis.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "coordinate"><B>The Coordinate Method for the Parallel
Execution of DO Loops</B><BR><i>Proceedings of the 1973 Sagamore
Conference on Parallel Processing</i>, T. Feng, ed., 1-12.<BR>
No electronic version available.<HR>


Compass (Massachusetts Computer Associates) had a contract to write
the Fortran compiler for the Illiac-IV computer, an array computer
with 64 processors that all operated in lock-step on a single
instruction stream.&nbsp; I developed the theory and associated algorithms
for executing sequential DO loops in parallel on an array computer
that were used by the compiler.&nbsp; The theory is pretty straightforward.&nbsp;
The creativity lay in the proper mathematical formulation of the
problem.&nbsp; Today, it would be considered a pretty elementary piece of
work.&nbsp; But in those days, we were not as adept at applying math to
programming problems.&nbsp; Indeed, when I wrote up a complete description
of my work for my colleagues at Compass, they seemed to treat it as a
sacred text, requiring spiritual enlightenment to interpret the occult
mysteries of linear algebra.&nbsp;

<p>
Anyway, this paper was my first pass at writing up the complete
version of the theory for publication.&nbsp; I strongly suspect that it has
never been read.&nbsp; No one seems to have noticed that, because of a
text-editing error, the description of the algorithm is missing the
punch line that says what can be executed in parallel.&nbsp; This paper is
superseded by the unpublished <A HREF = "#coord-method">[20]</A>.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "do-loops"><B>The Parallel Execution of DO Loops</B><BR><i>Communications of the ACM 17</i>, 2 &nbsp;
(February 1974), 83-93.<BR>
<A HREF = "do-loops.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1974 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


This is the only journal paper to come out of the work mentioned in
the description of <A HREF = "#coordinate">[9]</A>.&nbsp; It contains essentially the
special case of the results in <A HREF = "#coordinate">[9]</A> for a single tight
nesting of loops.&nbsp; It was one of the early articles on the topic, and
I believe it was cited fairly often.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "hyperplane"><B>The Hyperplane Method for an Array
   Computer</B><BR><i>Proceedings of the 1974 Sagamore Conference on
   Parallel Processing</i>, T. Feng, ed., Springer Verlag, 1-12.<BR><A HREF="https://link.springer.com/chapter/10.1007/3-540-07135-0_114">Available On-Line</A><HR>


In the late 19th century, the Vanderbilts (a rich American family)
owned 500 hectares of beautiful land in the central Adirondack
Mountains of New York State that included Sagamore Lake, which is
about a kilometer in length.&nbsp; There, they built a rustic summer
estate.&nbsp; In the 70s, and probably still today, the place was
completely isolated, away from any other signs of civilization.&nbsp; The
estate, with land and lake, was given to Syracuse University,
which operated it as a conference center.&nbsp; An annual conference on
parallel processing was held there late in the summers of 1973 through
1975.&nbsp; Sagamore was the most beautiful conference site I have ever
seen.&nbsp; The conference wasn't bad, with a few good people attending,
though it wasn't first rate.&nbsp; But I would have endured a conference on
medieval theology for the opportunity to canoe on, swim in, and walk
around the lake.&nbsp;

<p> 
To justify my attendance at Sagamore, I always submitted a paper.&nbsp;
But once I discovered that it was not a first-rate conference, I did not
submit first-rate papers.&nbsp; However, I don't republish old material
(except as necessary to avoid forcing people to read earlier papers),
so this paper must have included some new results about the hyperplane
method for parallelizing sequential loops.&nbsp; However, I don't have a
copy of the paper and don't remember what was in it.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "bakery"><B>A New Solution of Dijkstra's Concurrent Programming
   Problem</B><BR><i>Communications of the ACM 17</i>, 8 &nbsp; (August 1974), 453-455.<BR>
<A HREF = "bakery.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1974 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


This paper describes the bakery algorithm for implementing mutual
exclusion.&nbsp; I have invented many concurrent algorithms.&nbsp; I feel that I
did not invent the bakery algorithm, I discovered it.&nbsp; Like all
shared-memory synchronization algorithms, the bakery algorithm
requires that one process be able to read a word of memory while
another process is writing it.&nbsp; (Each memory location is written by
only one process, so concurrent writing never occurs.)&nbsp; Unlike any
previous algorithm, and almost all subsequent algorithms, the bakery
algorithm works regardless of what value is obtained by a read that
overlaps a write.&nbsp; If the write changes the value from 0 to 1, a
concurrent read could obtain the value 7456 (assuming that 7456 is a
value that could be in the memory location).&nbsp; The algorithm still
works.&nbsp; I didn't try to devise an algorithm with this property.&nbsp; I
discovered that the bakery algorithm had this property after writing a
proof of its correctness and noticing that the proof did not depend on
what value is returned by a read that overlaps a write.&nbsp;

<p> 
I don't know how many people realize how remarkable this
algorithm is.&nbsp; Perhaps the person who realized it better than anyone
is Anatol Holt, a former colleague at Massachusetts Computer
Associates.&nbsp; When I showed him the algorithm and its proof and pointed
out its amazing property, he was shocked.&nbsp; He refused to believe it
could be true.&nbsp; He could find nothing wrong with my proof, but he was
certain there must be a flaw.&nbsp; He left that night determined to find
it.&nbsp; I don't know when he finally reconciled himself to the
algorithm's correctness.&nbsp;

<p>
Several books have included emasculated versions of the algorithm in
which reading and writing are atomic operations, and called those
versions "the bakery algorithm".&nbsp; I find that deplorable.&nbsp; There's
nothing wrong with publishing a simplified version, as long as it's
called a simplified version.&nbsp;

<p>
What is significant about the bakery algorithm is that it implements
mutual exclusion without relying on any lower-level mutual exclusion.&nbsp;
Assuming that reads and writes of a memory location are atomic
actions, as previous mutual exclusion algorithms had done, is
tantamount to assuming mutually exclusive access to the location.&nbsp; So
a mutual exclusion algorithm that assumes atomic reads and writes is
assuming lower-level mutual exclusion.&nbsp; Such an algorithm cannot
really be said to solve the mutual exclusion problem.&nbsp; Before the
bakery algorithm, people believed that the mutual exclusion problem
was unsolvable--that you could implement mutual exclusion only by
using lower-level mutual exclusion.&nbsp; Brinch Hansen said exactly this
in a 1972 paper.&nbsp; Many people apparently still believe it.&nbsp; (See
<A HREF = "#lamport-mutual-solved">[91]</A>.)&nbsp;

<p> 
The paper itself does not state that it is a "true" mutual exclusion
algorithm.&nbsp; This suggests that I didn't realize the full significance
of the algorithm until later, but I don't remember.&nbsp;

<p> 
For a couple of years after my discovery of the bakery algorithm,
everything I learned about concurrency came from studying it.&nbsp; Papers
like <A HREF = "#rd-wr">[25]</A>, <A HREF = "#new-approach">[33]</A>, and <A HREF = "#interprocess">[70]</A>
were direct results of that study.&nbsp; The bakery algorithm was also
where I introduced the idea of variables belonging to a process--that
is, variables that could be read by multiple processes, but written by
only a single process.&nbsp; I was aware from the beginning that such
algorithms had simple distributed implementations, where the variable
resides at the owning process, and other processes read it by sending
messages to the owner.&nbsp; Thus, the bakery algorithm marked the
beginning of my study of distributed algorithms.&nbsp;

<p> 
The paper contains one small but significant error.&nbsp; In a
footnote, it claims that we can consider reads and writes of a single
bit to be atomic.&nbsp; It argues that a read overlapping a write must
get one of the two possible values; if it gets the old value, we can
consider the read to have preceded the write, otherwise to have
followed it.&nbsp; It was only later, with the work eventually described in
<A HREF = "#interprocess">[70]</A>, that I realized the fallacy in this reasoning.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "on-self-stabilizing-systems"><B>On Self-stabilizing
Systems</B><BR>Massachusetts Computer Associates Technical Report CA
7412-0511 (5 December 1974).<BR>
<A HREF = "on-self-stabilizing-systems.pdf">PDF</A><HR>


This note was written upon reading Dijkstra's classic paper
"Self-stabilizing Systems in Spite of Distributed Control" that
appeared in the November 1974 issue of <i>CACM</i> (see
<A HREF = "#solved-and-unsolved">[58]</A>).&nbsp; It generalizes one of the algorithms
in Dijkstra's paper from a line of processes to an arbitrary tree of
processes.&nbsp; It also discusses the self-stabilizing properties of the
bakery algorithm.&nbsp; I never tried to publish this note--probably
because I regarded it as too small a piece of work to be worth a paper
by itself.&nbsp;

<p>

The note contains the intriguing sentence: "There is a complicated
modified version of the bakery algorithm in which the values of all
variables are bounded." I never wrote down that version, and I'm not
sure what I had in mind.&nbsp; But I think I was thinking of roughly the
following modification.&nbsp; As a process waits to enter its critical
section, it keeps reducing its number, not entering the critical
section until its number equals one.&nbsp; A process <i>p</i> can reduce
its number by at most one, and only when the next lower-numbered
process's number is at least two less than <i>p</i>'s number, and the
next higher-numbered process is within one of <i>p</i>'s number.&nbsp; I
think I intended to use the techniques of <A HREF = "#rd-wr">[25]</A> to allow
reading and writing of numbers to remain non-atomic while maintaining
the order of waiting processes.&nbsp; (If eventually all processes stop
changing their numbers, then all processes will eventually read the
correct numbers, allowing some process to progress.)&nbsp; At one time, I
convinced myself that this algorithm is correct.&nbsp; But I never wrote a
rigorous proof, so I don't know if it really works.&nbsp; Filling in the
details and proving correctness should be a nice exercise.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "prog-parallel"><B>On Programming Parallel
   Computers</B><BR><i>Proceedings of a Conference on Programming Languages
   and Compilers for Parallel and Vector Machines</i>, published as
   <i>ACM SIGPLAN Notices 10</i>, 3 (March 1975), 25-33.<BR>
<A HREF = "prog-parallel.pdf">PDF</A><HR>


This is a position paper advocating the use of a higher-level language
that expresses what must be computed rather than how it is to be
computed.&nbsp; It argues that compilers are better than humans at
generating efficient code for parallel machines.&nbsp; I was so naive then!
I soon learned how bad compilers really were, and how trying to make
them smarter just made them buggier.&nbsp; But compiler writers have gotten
a lot better, so maybe this paper isn't as stupid now as it was then.&nbsp;



<BR>&nbsp;<P><LI> <A NAME = "parallel-execution"><B>Parallel Execution on Array and Vector
   Computers</B><BR><i>Proceedings of the 1975 Sagamore Conference on
   Parallel Processing</i>, T. Feng, ed., 187-191.<BR>
No electronic version available.<HR>


This paper considers the problem of efficiently executing a sequence
of explicitly parallel statements--ones requiring simultaneous
execution for all values of a parameter--when there are more
parameter values than there are processors.&nbsp; It is one of the lesser
papers that I saved for the Sagamore Conference.&nbsp; (See the discussion
of <A HREF = "#hyperplane">[11]</A>.)&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "multiple-byte"><B>Multiple Byte Processing with Full-Word
   Instructions</B><BR><i>Communications of the ACM 18</i>, 8 &nbsp; (August 1975), 471-475.<BR>
<A HREF = "multiple-byte.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1975 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


My algorithms for parallelizing loops, described in papers starting
with <A HREF = "#coordinate">[9]</A>, were rather inefficient.&nbsp; They could be sped
up with parallel execution on an array processor like the
Illiac-IV.&nbsp; But I realized one could do even better than the
64-times speedup provided by the Illiac's 64 processors.&nbsp; Each datum
being manipulated was just a few bits, so I had the idea of packing
several of the data into a single word and manipulating them
simultaneously.&nbsp; Not only could this speed computation on the Illiac,
but it allowed one to do array processing on an ordinary uniprocessor.&nbsp;
This paper describes general techniques for doing such parallel
computation on packed data.&nbsp; It's a neat hack, and it's more useful
now than it was then for two reasons.&nbsp; The obvious reason is that word
size is larger now, with many computers having 64-bit words.&nbsp; The less
obvious reason is that conditional operations are implemented with
masking rather than branching.&nbsp; Instead of branching around the
operation when the condition is not met, masks are constructed so the
operation is performed only on those data items for which the
condition is true.&nbsp; Branching is more costly on modern multi-issue
computers than it was on the computers of the 70s.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "synchronization"><B>The Synchronization of Independent
   Processes</B><BR><i>Acta Informatica 7</i>, 1 (1976), 15-34.<BR>
<A HREF = "synchronization.pdf">PDF</A><HR>


There are a class of synchronization problems that are direct
generalizations of mutual exclusion in that they assert constraints on
when a process is allowed to perform a task.&nbsp; They include the dining
philosophers problem and the readers/writers problem.&nbsp; This paper
shows how a modified version of the bakery algorithm can be used to
solve any such problem.&nbsp;

<p>
A referee said of the initial submission that the proofs were too
long, tediously proving the obvious.&nbsp; In fact, there was a bug in the
initial version, and at least one of those obvious proofs was of a
false statement.&nbsp; Needless to say, I corrected the algorithm and wrote
more careful proofs.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "comments"><B>Comments on `A Synchronization
   Anomaly'</B><BR><i>Information Processing Letters 4</i>, 4 (January 1976),
   88-89.<BR>
No electronic version available.<HR>


This is a comment on a short note by Richard Lipton and Robert Tuttle
claiming to find an inadequacy in Dijkstra's P and V synchronization
primitives.&nbsp; It points out that they had introduced a red herring
because the problem that those primitives couldn't solve could not be
stated in terms of entities observable within the system.&nbsp; As my note
states: "A system cannot be correct unless its correctness depends
only upon events and conditions observable within the system." That's
something worth remembering, since I've encountered that same sort of
red herring on other occasions.&nbsp; My observation is relevant to
<A HREF = "#priority">[63]</A>, but I had forgotten all about this note by the time
I wrote <A HREF = "#priority">[63]</A>.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "multi-garbage"><B>Garbage Collection with Multiple Processes:
   an Exercise in Parallelism</B><BR><i>Proceedings of the 1976 International 
   Conference on Parallel Processing</i>, T. Feng, ed., 50-54.<BR>
No electronic version available.<HR>


This is a minor extension to the concurrent garbage collection
algorithm of <A HREF = "#garbage">[31]</A>.&nbsp; That algorithm uses a single garbage
collector process running in parallel with the "mutator" process
that creates the garbage.&nbsp; This paper describes how to use multiple
processes to do the collection, and how to handle multiple mutator
processes.&nbsp; It is a minor work that I wrote up as an excuse for
going to the Sagamore conference.&nbsp; (See the discussion of
<A HREF = "#hyperplane">[11]</A>.)&nbsp; However, the conference had been renamed and
moved from Sagamore to a less attractive, mosquito-infected site in the
Michigan woods.&nbsp; It was the last time I attended.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "coord-method"><B>The Coordinate Method for the Parallel Execution of
   Iterative Loops</B><BR>SRI Technical Report, August 1976.<BR>
<A HREF = "coord-method.pdf">PDF</A><HR>


This is a totally revised version of <A HREF = "#coordinate">[9]</A>, complete
with proofs (which had been omitted from <A HREF = "#coordinate">[9]</A>).&nbsp; I
submitted it to <i>CACM</i>, but the editor of <i>CACM</i> decided
that it was more appropriate for <i>JACM</i>.&nbsp; When I
submitted it there, the editor of <i>JACM</i> rejected it without
sending it out for review because the basic ideas had already appeared
in the conference version <A HREF = "#coordinate">[9]</A>.&nbsp; I was young and
inexperienced, so I just accepted the editor's decision.&nbsp; I began to
suspect that something was amiss a year or two later, when a paper
from the same conference was republished verbatim in
<i>CACM</i>.&nbsp; By the time I realized how crazy the editor's
decision had been, it didn't seem worth the effort of resubmitting the
paper.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "multi-user"><B>Towards a Theory of Correctness for Multi-User
   Data Base Systems</B><BR>Rejected by the <i>1977 IFIP Congress</i> 
   (October 1976).<BR>
No electronic version available.<HR>
  

This paper was a rough draft of some ideas, not all of which were
correct.&nbsp; I don't remember how it became known, but I received
requests for copies for years afterwards.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "glitch"><B>On the Glitch Phenomenon</B>&nbsp; (with Richard Palais)<BR>Rejected
  by <i>IEEE Transactions on Computers</i> (November 1976).<BR>
<A HREF = "glitch.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "glitch.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "glitch.pdf">PDF</A><HR>


When I wrote <A HREF = "#bakery">[12]</A>, a colleague at Massachusetts Computer
Associates pointed out that the concurrent reading and writing of a
single register, assumed in the bakery algorithm, requires an
arbiter--a device for making a binary decision based on inputs that
may be changing.&nbsp; In the early 70s, computer designers rediscovered
that it's impossible to build an arbiter that is guaranteed to reach a
decision in a bounded length of time.&nbsp; (This had been realized in the
50s but had been forgotten.)&nbsp; My colleague's observation led to my
interest in the arbiter problem--or "glitch" problem, as it was
sometimes called.&nbsp;

<p> The basic proof that an arbiter cannot have a bounded
response time uses continuity to demonstrate that, if there are two
inputs that can drive a flip-flop into two different states, then
there must exist an input that makes the flip-flop hang.&nbsp; At the time,
it was very difficult to convince someone that this argument was
valid.&nbsp; They seemed to believe that, because a flip-flop has only
discrete stable states, continuity doesn't apply.&nbsp;

<p>
I described the arbiter problem to Palais, who had been my <i>de
jure</i> thesis adviser and afterwards became a colleague and a friend.&nbsp;
He recognized that the correct mathematical way to view what was going
on is in terms of the compact-open topology on the space of flip-flop
behaviors.&nbsp; So, we wrote this paper to explain why the apparently
discontinuous behavior of an arbiter is actually continuous in the
appropriate topology.&nbsp;

<p>
This paper was rejected by the <i>IEEE Transactions on Computers</i>
because the engineers who reviewed it couldn't understand the
mathematics.&nbsp; Six years later, the journal apparently acquired more
mathematically sophisticated reviewers, and it published a less
general result with a more complicated proof.&nbsp; I believe someone has
finally published a paper on the subject that does supersede ours.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "proving"><B>Proving the Correctness of Multiprocess
   Programs</B><BR><i>IEEE Transactions on Software Engineering SE-3</i>, 2
   (March 1977), 125-143.<BR>
<A HREF = "proving.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1977 Personal use of this material is permitted.
However, permission to reprint/republish this
material for advertising or promotional purposes or
for creating new collective works for resale or
redistribution to servers or lists, or to reuse any
copyrighted component of this work in other works
must be obtained from the IEEE.
</FONT><HR>


When I first learned about the mutual exclusion problem, it seemed
easy and the published algorithms seemed needlessly complicated.&nbsp; So,
I dashed off a simple algorithm and submitted it to
<i>CACM</i>.&nbsp; I soon received a referee's report pointing
out the error.&nbsp; This had two effects.&nbsp; First, it made me mad enough at
myself to sit down and come up with a real solution.&nbsp; The result was
the bakery algorithm described in <A HREF = "#bakery">[12]</A>.&nbsp; The second effect
was to arouse my interest in verifying concurrent algorithms.&nbsp; This
has been a very practical interest.&nbsp; I want to verify the algorithms
that I write.&nbsp; A method that I don't think is practical for my
everyday use doesn't interest me.&nbsp;

<p> 

In the course of my work on parallelizing sequential code (see
<A HREF = "#do-loops">[10]</A>), I essentially rediscovered Floyd's method as a way
of extracting properties of a program.&nbsp; When I showed a colleague what
I was doing, he went to our library at Massachusetts Computer
Associates and gave me a copy of the original tech report version of
Floyd's classic paper <i>Assigning Meanings to Programs</i>.&nbsp; I don't
remember when I read Hoare's <i>An Axiomatic Basis for Computer
Programming</i>, but it was probably not long afterwards.&nbsp;

<p>
In the mid-70s, several people were thinking about the problem of
verifying concurrent programs.&nbsp; The seminal paper was Ed Ashcroft's
<i>Proving Assertions About Parallel Programs</i>, published in the
Journal of Computer and System Sciences in 1975.&nbsp; That paper
introduced the fundamental idea of invariance.&nbsp; I discovered how to
use the idea of invariance to generalize Floyd's method to
multiprocess programs.&nbsp; As is so often the case, in retrospect the
idea seems completely obvious.&nbsp; However, it took me a while to come to
it.&nbsp; I remember that, at one point, I thought that a proof would
require induction on the number of processes.&nbsp; 

<p>
This paper introduced the concepts of safety and liveness as the
proper generalizations of partial correctness and termination to
concurrent programs.&nbsp; It also introduced the terms "safety" and
"liveness" for those classes of properties.&nbsp; I stole those terms
from Petri nets, where they have similar but formally very different
meanings.&nbsp; (Safety of a Petri net is a particular safety property;
liveness of a Petri net is not a liveness property.)&nbsp; 

<p> 

At the same time I was devising my method, Susan Owicki was writing
her thesis at Cornell under David Gries and coming up with very much
the same ideas.&nbsp; The generalization of Floyd's method for proving
safety properties of concurrent programs became known as the
Owicki-Gries method.&nbsp; Owicki and Gries did not do anything comparable
to the method for proving liveness in my paper.&nbsp; (Nissim Francez and
Amir Pnueli developed a general proof method that did handle liveness
properties, but it lacked a nice way of proving invariance
properties.)&nbsp; My method had deficiencies that were corrected with the
introduction of temporal logic, discussed in <A HREF = "#liveness">[47]</A>.&nbsp;

<p>
The Owicki-Gries version of the method for proving safety properties
differed from mine in two ways.&nbsp; The significant way was that I made
the control state explicit, while they had no way to talk about it
directly.&nbsp; Instead, they introduced dummy variables to capture the
control state.&nbsp; The insignificant way was that I used a flowchart
language while they used an Algol-like language.&nbsp;

<p>
The insignificant syntactic difference in the methods turned out to
have important ramifications.&nbsp; For writing simple concurrent
algorithms, flowcharts are actually better than conventional toy
programming languages because they make the atomic actions, and hence
the control state, explicit.&nbsp; However, by the mid-70s, flowcharts were
pass&#233; and structured programming was all the rage,
so my paper was forgotten and people read only theirs.&nbsp; The paper was
rediscovered about ten years later, and is now generally cited
alongside theirs in the mandatory references to previous work.&nbsp;

<p>
More important though is that, because they had used a "structured"
language, Owicki and Gries thought that they had generalized Hoare's
method.&nbsp; To the extent that Floyd's and Hoare's methods are different,
it is because Hoare's method is based on the idea of hierarchical
decomposition of proofs.&nbsp; The Owicki-Gries method doesn't permit this
kind of clean hierarchical decomposition.&nbsp; Gries, commenting in 1999,
said: "We hardly ever looked at Floyd's work and simply did
everything based on Hoare's axiomatic theory." I suspect that,
because they weren't thinking at all about Floyd's approach, they
didn't notice the difference between the two, and thus they didn't
realize that they were generalizing Floyd's method and not Hoare's.&nbsp;

<p>
The result of presenting a generalization of Floyd's method in Hoare's
clothing was to confuse everyone.&nbsp; For a period of about ten years,
hardly anyone really understood that the Owicki-Gries method was just
a particular way of structuring the proof of a global invariant.&nbsp; I
can think of no better illustration of this confusion than the 
EWD
<i>A Personal Summary of the Owicki-Gries
Theory</i> that Dijkstra wrote and subsequently published in a book of his
favorite EWDs.( Throughout his career, Edsger Dijkstra
         wrote a series of notes identified by an "EWD" number.) 
If even someone as smart and generally clear-thinking as Dijkstra
could write such a confusing hodge-podge of an explanation, imagine
how befuddled others must have been.&nbsp; A true generalization of Hoare's
method to concurrent programs didn't come until several years later in
<A HREF = "#hoare">[40]</A>.&nbsp;

<p>
I think it soon became evident that one wanted to talk explicitly
about the control state.&nbsp; Susan Owicki obviously agreed, since we
introduced the <i>at</i>, <i>in</i>, and <i>after</i> predicates for
doing just that in <A HREF = "#liveness">[47]</A>.&nbsp; Quite a bit later, I had more
to say about dummy variables versus control state in <A HREF = "#control">[78]</A>.&nbsp;

<p>
Dummy variables were more than just an ugly hack to avoid control
variables.&nbsp; They also allowed you to capture history.&nbsp; Adding history
variables makes it possible to introduce behavioral reasoning into an
assertional proof.&nbsp; (In the limit, you can add a variable that
captures the entire history and clothe a completely behavioral proof
in an assertional framework.)&nbsp; What a program does next depends on its
current state, not on its history.&nbsp; Therefore, a proof that is based
on a history variable doesn't capture the real reason why a program
works.&nbsp; I've always found that proofs that don't use history variables
teach you more about the algorithm.&nbsp; (As shown in
<A HREF = "#abadi-existence">[92]</A>, history variables may be necessary if the
correctness conditions themselves are in terms of history.)&nbsp;

<p>
When we developed our methods, Owicki and I and most everyone else
thought that the Owicki-Gries method was a great improvement over
Ashcroft's method because it used the program text to decompose the
proof.&nbsp; I've since come to realize that this was a mistake.&nbsp; It's
better to write a global invariant.&nbsp; Writing the invariant as an
annotation allows you to hide some of the explicit dependence of the
invariant on the control state.&nbsp; However, even if you're writing your
algorithm as a program, more often than not, writing the invariant as
an annotation rather than a single global invariant makes things more
complicated.&nbsp; But even worse, an annotation gets you thinking in terms
of separate assertions rather than in terms of a single global
invariant.&nbsp; And it's the global invariant that's important.&nbsp; Ashcroft
got it right.&nbsp; Owicki and Gries and I just messed things up.&nbsp; It took
me quite a while to figure this out.&nbsp;

<p>
Sometime during the '80s, Jay Misra noticed that the definition
of well-foundedness (Definition 8 on page 136) is obviously incorrect.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "formal"><B>Formal Correctness Proofs for Multiprocess
   Algorithms</B><BR><i>Programmation-2me Colloque
   International</i>, B. Robinet, ed., Dunod, Paris (1977), 1-8.<BR>
No electronic version available.<HR>


This is an abbreviated, conference version of <A HREF = "#proving">[23]</A>.&nbsp; (In
those days, publication was fast enough that the journal version could
appear before the conference version.)&nbsp; I wrote this paper as an
excuse for attending a conference in Paris.&nbsp; However, it turned out
that I went to Europe for other reasons that made it impossible for me
to attend the conference.&nbsp; I tried to withdraw the paper, but it
was too late.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "rd-wr"><B>Concurrent Reading and Writing</B><BR><i>Communications of the ACM 20</i>, 11 &nbsp;
   (November 1977), 806-811.<BR>
<A HREF = "rd-wr.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1977 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


This paper came out of my study of the bakery algorithm of
<A HREF = "#bakery">[12]</A>.&nbsp; The problem with that algorithm is that it requires
unbounded state.&nbsp; To allow the state to be bounded in practice, I
needed an algorithm for reading and writing multidigit numbers, one
digit at a time, so that a read does not obtain too large a value if
it overlaps a write.&nbsp; This paper shows that this can be done by simply
writing the digits in one direction and reading them in the other.&nbsp; It
also has some other nice algorithms.&nbsp;

The paper assumes that reading and writing a single digit are atomic
operations.&nbsp; The original version introduced the notion of a regular
register and proved the results under the weaker assumption that the
individual digits were regular.&nbsp; However, the editor found the idea of
nonatomic reads and writes to individual digits too heretical for
<i>CACM</i> readers, and he insisted that I make the stronger
assumption of atomicity.&nbsp; So, the world had to wait another decade,
until the publication of <A HREF = "#interprocess">[70]</A>, to learn about regular
registers.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "state-the-problem"><B>State the Problem Before 
Describing the Solution</B><BR><i>ACM SIGSOFT Software Engineering Notes 
3</i>, 1 (January 1978) 26.<BR>
<A HREF = "state-the-problem.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1978 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


The title says it all.&nbsp; This one-page note is as relevant today as
when I wrote it.&nbsp; Replace "describing the solution" by "writing the
program" and it becomes a practical recipe for improving software.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "time-clocks"><B>Time, Clocks and the Ordering of Events in a
   Distributed System</B><BR><i>Communications of the ACM 21</i>, 7 &nbsp; (July 1978), 558-565.&nbsp; Reprinted in
   several collections, including <i>Distributed Computing:
   Concepts and Implementations</i>, McEntire et al., ed.&nbsp; IEEE Press, 1984.<BR>
<A HREF = "time-clocks.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1978 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


Jim Gray once told me that he had heard two different opinions of this
paper: that it's trivial and that it's brilliant.&nbsp; I can't argue with
the former, and I am disinclined to argue with the latter.&nbsp;

<p>
The origin of this paper was 
the
note 
<A HREF = "https://www.rfc-archive.org/getrfc.php?rfc=677">
  The Maintenance of Duplicate Databases</A>
by Paul Johnson and Bob Thomas.&nbsp; I believe their
note introduced the idea of using message timestamps in a distributed
algorithm.&nbsp; I happen to have a solid, visceral understanding of
special relativity (see <A HREF = "#geometry-of-space-time">[5]</A>).&nbsp; This
enabled me to grasp immediately the essence of what they were trying
to do.&nbsp; Special relativity teaches us that there is no invariant total
ordering of events in space-time; different observers can disagree
about which of two events happened first.&nbsp; There is only a partial
order in which an event <i>e1</i> precedes an event
<i>e2</i> iff <i>e1</i> can causally
affect <i>e2</i>.&nbsp; I realized that the essence of
Johnson and Thomas's algorithm was the use of timestamps to provide a
total ordering of events that was consistent with the causal order.&nbsp;
This realization may have been brilliant.&nbsp; Having realized it,
everything else was trivial.&nbsp; Because Thomas and Johnson didn't
understand exactly what they were doing, they didn't get the algorithm
quite right; their algorithm permitted anomalous behavior that
essentially violated causality.&nbsp; I quickly wrote a short note pointing
this out and correcting the algorithm.&nbsp;

<p> It didn't take me long to realize that an algorithm for
totally ordering events could be used to implement any distributed
system.&nbsp; A distributed system can be described as a particular
sequential state machine that is implemented with a network of
processors.&nbsp; The ability to totally order the input requests leads
immediately to an algorithm to implement an arbitrary state machine by
a network of processors, and hence to implement any distributed
system.&nbsp; So, I wrote this paper, which is about how to implement an
arbitrary distributed state machine.&nbsp; As an illustration, I used the
simplest example of a distributed system I could think of--a
distributed mutual exclusion algorithm.&nbsp;

<p>
This is my most often cited paper.&nbsp; Many computer scientists claim to
have read it.&nbsp; But I have rarely encountered anyone who was
aware that the paper said anything about state machines.&nbsp; People seem to
think that it is about either the causality relation on events in a
distributed system, or the distributed mutual exclusion problem.&nbsp;
People have insisted that there is nothing about state machines in the
paper.&nbsp; I've even had to go back and reread it to convince myself that
I really did remember what I had written.&nbsp;

<p>
The paper describes the synchronization of logical clocks.&nbsp; As
something of an afterthought, I decided to see what kind of
synchronization it provided for real-time clocks.&nbsp; So, I included a
theorem about real-time synchronization.&nbsp; I was rather surprised by
how difficult the proof turned out to be.&nbsp; This was an indication of
what lay ahead in <A HREF = "#clocks">[62]</A>.&nbsp;

<p>
This paper won the 2000 PODC Influential Paper Award (later
renamed the Edsger W. Dijkstra Prize in Distributed Computing).&nbsp;
It won an ACM SIGOPS Hall of Fame Award in 2007.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "interactive"><B>The Specification and Proof of Correctness of
   Interactive Programs</B><BR><i>Proceedings of the International
   Conference on Mathematical Studies of Information Processing</i> Kyoto,
   Japan (August, 1978), 477-540.<BR>
No electronic version available.<HR>


In the late 70s, people were talking about designing programming
languages that would make program verification easier.&nbsp; I didn't think
much of that idea.&nbsp; I felt that the difficulty in verification comes
from the algorithm, not the details of the programming language in
which it is described.&nbsp; To demonstrate this view, I published in this
paper a proof of correctness of a TECO program.&nbsp; 

<p>
TECO stands for Text Editing and Correction.&nbsp; It was the command
language for the TECO editor, and it was the underlying macro language
on which the original version of Emacs was built.&nbsp; It was an obscure,
low-level language whose goal was to perform powerful text editing
operations with the minimum number of keystrokes.&nbsp; A programming
language designed to make verification easy would be completely unlike
TECO.&nbsp; The paper shows that you verify a TECO program the
same way you verify a program written in a more conventional language.&nbsp;

<p>
The proof is perhaps also of some historical interest because it was an 
early example of a proof of an interactive program--that is, one that
interacts with the user instead of just producing an answer.&nbsp; Thus,
correctness had to be asserted in terms of the sequence of input
actions.&nbsp; The paper generalizes the Floyd/Hoare method to deal with
the history of environment actions.&nbsp; 



<BR>&nbsp;<P><LI> <A NAME = "implementation"><B>The Implementation of Reliable Distributed
   Multiprocess Systems</B><BR><i>Computer Networks 2</i> (1978), 95-114.<BR>
<A HREF = "implementation.pdf">PDF</A>
<BR><FONT SIZE = -2>All copyrights reserved by Elsevier Science 1978.</FONT><HR>


In <A HREF = "#time-clocks">[27]</A>, I introduced the idea of implementing any
distributed system by using an algorithm to implement an arbitrary
state machine in a distributed system.&nbsp; However, the algorithm in
<A HREF = "#time-clocks">[27]</A> assumed that processors never fail and all
messages are delivered.&nbsp; This paper gives a fault-tolerant algorithm.&nbsp;
It's a real-time algorithm, assuming upper bounds on message delays in
the absence of faults, and that nonfaulty processes had clocks
synchronized to within a known bound.&nbsp; 

<p>

To my knowledge, this is the first published paper to discuss
arbitrary failures (later called Byzantine failures).&nbsp; It actually
considered malicious behavior, not using such behavior simply as a
metaphor for completely unpredictable failures.&nbsp; Its algorithm was the
inspiration for the digital signature algorithm of <A HREF = "#reaching">[41]</A>.&nbsp;
With its use of real-time, this paper presaged the ideas in
<A HREF = "#using-time">[55]</A>.&nbsp;



<BR>&nbsp;<P><LI> <A NAME = "sift"><B>SIFT: Design and Analysis of a Fault-Tolerant Computer for
   Aircraft Control</B>&nbsp; (with John Wensley et al.)<BR><i>Proceedings of the IEEE
   66</i>, 10 (October 1978), 1240-1255.<BR>
<A HREF = "sift.pdf">PDF</A><HR>


When it became clear that computers were going to be flying commercial
aircraft, NASA began funding research to figure out how to make them
reliable enough for the task.&nbsp; Part of that effort was the SIFT
project at SRI.&nbsp; This project was perhaps most
notable for producing the Byzantine generals problem and its
solutions, first reported in <A HREF = "#reaching">[41]</A>.&nbsp;

<p> This paper gives an overview of the complete SIFT project,
which included designing the hardware and software and formally
verifying the system's correctness.&nbsp; It announces the results that
appear in <A HREF = "#reaching">[41]</A>.&nbsp; It also is a very early example of the
basic specification and verification method I still advocate: writing
a specification as a state-transition system and showing that each
step of the lower-level specification either implements a step of the
higher-level one or is a "stuttering" step that leaves the
higher-level state unchanged.&nbsp; The paper doesn't mention the use of an
invariant, but that was probably omitted to save space.&nbsp;

<p>
This paper was a group effort that I choreographed in a final frenzy
of activity to get the paper out in time for the issue's deadline.&nbsp; I
don't remember who wrote what, but the section on verification seems
to be my writing.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "garbage"><B>On-the-fly Garbage Collection: an Exercise in
    Cooperation</B>&nbsp; (with Edsger Dijkstra et al.)<BR><i>Communications of the ACM 21</i>, 11 &nbsp;
   (November 1978), 966-975.<BR>
<A HREF = "garbage.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1978 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


This paper presents the first concurrent garbage collection
algorithm--that is, an algorithm in which the collector operates
concurrently with the process that creates the garbage.&nbsp; The paper is
fairly well known; its history is not.&nbsp;

<p>
I received an early version of the paper from Dijkstra, and I made a
couple of suggestions.&nbsp; Dijkstra incorporated them and, quite
generously, added me to the list of authors.&nbsp; He submitted the paper
to <i>CACM</i>.&nbsp; The next I heard about it was when I received a
copy of a letter from Dijkstra to the editor withdrawing the paper.&nbsp;
The letter said that someone had found an error in the algorithm, but
gave no indication of what the error was.&nbsp; Since Dijkstra's proof was
so convincing, I figured that it must be a trivial error that could
easily be corrected.&nbsp;

<p>
I had fairly recently written <A HREF = "#proving">[23]</A>.&nbsp; So, I decided to
write a proof using that proof method, thinking that I would then find
and correct the error.&nbsp; In about 15 minutes, trying to write the proof
led me to the error.&nbsp; To my surprise, it was a serious error.&nbsp;

<p>
I had a hunch that the algorithm could be fixed by changing the order
in which two operations were performed.&nbsp; But I had no good reason to
believe that would work.&nbsp; Indeed, I could see no simple informal
argument to show that it worked.&nbsp; However, I decided to go ahead and
try to write a formal correctness proof anyway.&nbsp; It took me about two
days of solid work, but I constructed the proof.&nbsp; When I was through,
I was convinced that the algorithm was now correct, but I had no
intuitive understanding of why it worked.&nbsp;

<p> In the meantime, Dijkstra figured out that the algorithm
could be fixed by interchanging two other operations, and he wrote the
same kind of behavioral proof as before.&nbsp; His fix produced an arguably
more efficient algorithm than mine, so that's the version we used.&nbsp; I
sketched an assertional proof of that algorithm.&nbsp; Given the evidence
of the unreliability of his style of proof, I tried to get Dijkstra to
agree to a rigorous assertional proof.&nbsp; He was unwilling to do that,
though he did agree to make his proof somewhat more assertional and
less operational.&nbsp; Here are his comments on that, written in July,
2000:
 <blockquote><P> 
There were, of course, two issues at hand: (A) a witness showing that
the problem of on-the-fly garbage collection with fine-grained
interleaving could be solved, and (B) how to reason effectively about
such artifacts.&nbsp; I am also certain that at the time all of us were
aware of the distinction between the two issues.&nbsp; I remember very well
my excitement when we convinced ourselves that it could be done at
all; emotionally it was very similar to my first solutions to the
problem of self-stabilization.&nbsp; Those I published without proofs! It
was probably a period in my life that issue (A) in general was still
very much in the foreground of my mind: showing solutions to problems
whose solvability was not obvious at all.&nbsp; It was more or less my
style.&nbsp; I had done it (CACM, Sep. 1965) with the mutual
exclusion.&nbsp;
 </P></blockquote>
I, too, have always been most interested in showing that something
could be done, rather than in finding a better algorithm for doing
what was known to be possible.&nbsp; Perhaps that is why I have always been
so impressed by the brilliance of Dijkstra's work on concurrent
algorithms.&nbsp;

<p>
David Gries later published an Owicki-Gries style proof of the
algorithm that was essentially the same as the one I had sketched.&nbsp; He
simplified things a bit by combining two atomic operations into one.&nbsp;
He mentioned that in a footnote, but <i>CACM</i> failed to print the
footnote.&nbsp; (However, they did print the footnote number in the text.)&nbsp;

<p>
The lesson I learned from this is that behavioral proofs are
unreliable and one should always use state-based reasoning for
concurrent algorithms--that is, reasoning based on invariance.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "general-construction"><B>A General Construction for
Expressing Repetition</B><BR><i>ACM SIGPLAN Notices 14</i>, 3 (March 1979)
38-42.<BR>
<A HREF = "general-construction.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1979 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


Fortunately, this note has been completely forgotten.&nbsp; It was written
when people were still proposing new programming-language constructs.&nbsp;
The one presented here may have little to recommend it, but it was no
worse than many others.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "new-approach"><B>A New Approach to Proving the Correctness of
   Multiprocess Programs</B><BR><i>ACM Transactions on Programming Languages and Systems 1</i>, 1 (July 1979), 84-97.<BR>
<A HREF = "new-approach.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1979 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


A corrigendum was published in <i>ACM Transactions on Programming Languages and Systems 2</i>, 1 
(January 1980), 134 and is available 
   <a HREF="a_new_approach-corrigendum.pdf">here</a>.&nbsp;

<p>

Like everyone else at the time, when I began studying concurrent
algorithms, I reasoned about them behaviorally.&nbsp; Such reasoning
typically involved arguments based on the order in which events occur.&nbsp;
I discovered that proofs can be made simpler, more elegant, and more
mathematical by reasoning about operations (which can be composed of
multiple events) and two relations on them: <i>precedes</i> (denoted
by a solid arrow)
and <i>can affect</i> (denoted by a dashed arrow).&nbsp;
Operation <i>A</i> precedes operation <i>B</i> if all the events of
<i>A</i> precede all the events of <i>B</i>; and <i>A</i> can affect
<i>B</i> if some event in <i>A</i> precedes some event in <i>B</i>.&nbsp;
These relations obey some simple rules that can reduce behavioral
reasoning to mathematical symbol pushing.&nbsp;

<p>
This paper introduced the method of reasoning with the two arrow
relations and applied it to a variant of the bakery algorithm.&nbsp; In the
spring of 1976 I spent a month working with Carel Scholten at the
Philips <i>Nat Lab</i> in Eindhoven.&nbsp; Scholten was a colleague and
friend of Dijkstra, and the three of us spent one afternoon a week
working, talking, and drinking beer at Dijkstra's house.&nbsp; The
algorithm emerged from one of those afternoons.&nbsp; I think I was its
primary author, but as I mention in the paper, the beer and the
passage of time made it impossible for me to be sure of who was
responsible for what.&nbsp;

<p> 

The solid and dashed arrow formalism provides very elegant proofs for
tiny algorithms such as the bakery algorithm.&nbsp; But, like all
behavioral reasoning, it is hard to make completely formal, and it
collapses under the weight of a complex problem.&nbsp; You should use
assertional methods to reason about complex algorithms.&nbsp; However,
standard assertional reasoning requires that the algorithm be written
in terms of its atomic operations.&nbsp; The only assertional approach to
reasoning directly about nonatomic operations (without translating
them into sequences of atomic operations) is the one in
<A HREF = "#lamport-win">[86]</A>, which is not easy to use.&nbsp; The two-arrow
formalism is still good for a small class of problems.&nbsp;

<p> 

The formalism seems to have been almost completely ignored, even among
the theoretical concurrency community.&nbsp; I find this ironic.&nbsp; There is
a field of research known by the pretentious name of "true
concurrency".&nbsp; Its devotees eschew assertional methods that are based
on interleaving models because such models are not truly concurrent.&nbsp;
Instead, they favor formalisms based on modeling a system as a partial
ordering of events, which they feel is the only truly concurrent kind
of model.&nbsp; Yet, those formalisms assume that events are atomic and
indivisible.&nbsp; Atomic events don't overlap in time the way real
concurrent operations do.&nbsp; The two-arrow formalism is the only one I
know that is based on nonatomic operations and could therefore be
considered truly concurrent.&nbsp; But, as far as I know, the true
concurrency community has paid no attention to it.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "howto"><B>How to Present a Paper</B><BR>Unpublished note, 4 August 1979.<BR>
<A HREF = "howto.txt">Text File</A><HR>


This three-page note is about presenting a paper at a conference, but
it offers good advice for any talk.&nbsp; Except for a couple of
suggestions about hand-written slides, it still applies today.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "multi"><B>How to Make a Multiprocessor Computer That Correctly
   Executes Multiprocess Programs</B><BR><i>IEEE Transactions on Computers
   C-28</i>, 9 (September 1979) 690-691.<BR>
<A HREF = "multi.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1979 Personal use of this material is permitted.
However, permission to reprint/republish this
material for advertising or promotional purposes or
for creating new collective works for resale or
redistribution to servers or lists, or to reuse any
copyrighted component of this work in other works
must be obtained from the IEEE.
</FONT><HR>


I forget what prompted me to be thinking about memory caching, but it
occurred to me one day that multiprocessor synchronization algorithms
assume that each processor accesses the same word in memory, but each
processor actually accesses its own copy in its cache.&nbsp; It hardly
required a triple-digit IQ to realize that this could cause problems.&nbsp;
I suppose what made this paper worth reading was its simple, precise
definition of sequential consistency as the required correctness
condition.&nbsp; This was not the first paper about cache coherence.&nbsp;
However, it is the early paper most often cited in the cache-coherence
literature.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "dig-sig"><B>Constructing Digital Signatures from a One Way
   Function</B><BR>SRI International Technical Report CSL-98 (October 1979).<BR>
<A HREF = "dig-sig.pdf">PDF</A><HR>


At a coffee house in Berkeley around 1975, Whitfield Diffie described
a problem to me that he had been trying to solve: constructing a
digital signature for a document.&nbsp; I immediately proposed a solution.&nbsp;
Though not very practical--it required perhaps 64 bits of published
key to sign a single bit--it was the first digital signature
algorithm.&nbsp; Diffie and Hellman mention it in 
their classic paper:
 <blockquote><P> 
Whitfield Diffie and Martin E. Hellman.&nbsp;&nbsp;
 New Directions in Cryptography.&nbsp;&nbsp;
<i>IEEE Transactions on Information Theory IT-22</i>, 6
(1976), 644-654.&nbsp;
 </P></blockquote>
(I think it's at the bottom right of page 650.)&nbsp;

<p>
In 1978, Michael Rabin published a paper titled <i>Digitalized
Signatures</i> containing a more practical scheme for generating digital
signatures of documents.&nbsp; (I don't remember what other digital
signature algorithms had already been proposed.)&nbsp; However, his
solution had some drawbacks that limited its utility.&nbsp; This report
describes an improvement to Rabin's algorithm that eliminates those
drawbacks.&nbsp;

<p>
I'm not sure why I never published this report.&nbsp; However, I think it
was because, after writing it, I realized that the algorithm could be
fairly easily derived directly from Rabin's algorithm.&nbsp; So, I didn't
feel that it added much to what Rabin had done.&nbsp; However, I've been
told that this paper is cited in the cryptography literature and is
considered significant, so perhaps I was wrong.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "calendar"><B>On the Proof of Correctness of a Calendar
   Program</B><BR><i>Communications of the ACM 22</i>, 10 &nbsp; (October 1979), 554-556.<BR>
<A HREF = "calendar.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1979 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


In the May, 1978 <i>CACM</i>, Matthew Geller published a paper titled
<i>Test Data as an Aid in Proving Program Correctness</i>.&nbsp; He argued
that there were some programs whose correctness is so hard to state
formally that formally verifying them is useless because the
specification is likely to be wrong.&nbsp; He gave as an example a program
to compute the number of days between two dates in the same year,
claiming that it would be hard to check the correctness of a precise
statement of what it meant for the program to be correct.&nbsp; This paper
proved him wrong.&nbsp; (It also makes the amusing observation that
Geller's solution is wrong because it fails for dates before the
advent of the Gregorian calendar.)&nbsp; As a bonus, readers of this paper
were alerted well in advance that the year 2000 is a leap year.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "letter-to-editor"><B>Letter to the Editor</B><BR><i>Communications of the ACM 22</i>, 11 &nbsp;
(November 1979), 624.<BR>
<A HREF = "letter-to-editor.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1979 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


In the May, 1979 <i>CACM</i>, De Millo, Lipton, and Perlis published
an influential paper titled <i>Social Process and Proofs of
Theorems and Programs</i>.&nbsp; This paper made some excellent observations.&nbsp;
However, by throwing in a few red herrings, they came to some wrong
conclusions about program verification.&nbsp; More insidiously, they framed
the debate as one between a reasonable engineering approach that
completely ignores verification and a completely unrealistic view of
verification advocated only by its most naive proponents.&nbsp; (There
were, unfortunately, quite a few such proponents.)&nbsp;

At that time, some ACM publications had a special section on
algorithms.&nbsp; In an ironic coincidence, the same issue of <i>CACM</i>
carried the official ACM policy on algorithm submissions.&nbsp; It included
all sorts of requirements on the form of the code, and even on the
comments.&nbsp; Missing was any requirement that the correctness of the
algorithm be demonstrated in any way.&nbsp;

I was appalled at the idea that, ten years after Floyd and Hoare's
work on verification, the ACM was willing to publish algorithms with
no correctness argument.&nbsp; The purpose of my letter was to express my
dismay.&nbsp; I ironically suggested that they had succumbed to the
arguments of De Millo, Lipton, and Perlis in their policy.&nbsp; As a
result, my letter was published as a rebuttal to the De Millo, Lipton,
and Perlis paper.&nbsp; No one seems to have taken it for what it was--a
plea to alter the ACM algorithms policy to require that there be some
argument to indicate that an algorithm worked.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "sometime"><B>`Sometime' is Sometimes `Not
   Never'</B><BR><i>Proceedings of the Seventh ACM Symposium on Principles
   of Programming Languages</i>, ACM SIGACT-SIGPLAN (January 1980).<BR>
<A HREF = "sometime.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1980 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


After graduating from Cornell, Susan Owicki joined the faculty of
Stanford.&nbsp; Some time around 1978, she organized a seminar to study the
temporal logic that Amir Pnueli had recently introduced to computer
science.&nbsp; I was sure that temporal logic was some kind of abstract
nonsense that would never have any practical application, but it
seemed like fun, so I attended.&nbsp; I observed that people got very
confused because, in Pnueli's logic, the concepts of <i>always</i> and
<i>eventually</i> mean what they do to ordinary people.&nbsp; In
particular, something is not always true if and only if it is
eventually false.&nbsp; (It doesn't always rain means that it eventually
stops raining.)&nbsp; However, most computer scientists have a different
way of thinking.&nbsp; For them, something is not always true if and only
if it might possibly become false.&nbsp; (The program doesn't always
produce the right answer means that it might produce the wrong
answer.)&nbsp; 

<p>
I realized that there are two types of temporal logic: the one Pnueli
used I called <i>linear time</i> logic; the one most computer
scientists seemed to find natural I called <i>branching time</i>.&nbsp;
(These terms were used by temporal logicians, but they distinguished
the two logics by the axioms they satisfied, while I described them in
terms of different kinds of semantics.)&nbsp; Pnueli chose the right kind
of logic--that is, the one that is most useful for expressing
properties of concurrent systems.&nbsp; I wrote this paper to explain the
two kinds of logic, and to advocate the use of linear-time logic.&nbsp;
However, I figured that the paper wouldn't be publishable without some
real theorems.&nbsp; So, I proved some simple results demonstrating that
the two kinds of logic really are different.&nbsp;

<p>
I submitted the paper to the Evian Conference, a conference on
concurrency held in France to which it seems that everyone working in
the field went.&nbsp; I was told that my paper was rejected because they
accepted a paper by Pnueli on temporal logic, and they didn't feel
that such an obscure subject merited two papers.&nbsp; I then submitted the
paper to FOCS, where it was also rejected.&nbsp; I have very rarely
resubmitted a paper that has been rejected.&nbsp; Fortunately, I felt that
this paper should be published.&nbsp; It has become one of the most
frequently cited papers in the temporal-logic literature.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "hoare"><B>The `Hoare Logic' of Concurrent Programs</B><BR><i>Acta 
   Informatica 14</i>, 1 (1980), 21-37.<BR>
No electronic version available.<HR>


As explained in the discussion of <A HREF = "#proving">[23]</A>, the Owicki-Gries
method and its variants are generalizations of Floyd's method for
reasoning about sequential programs.&nbsp; Hoare's method formalizes
Floyd's with a set of axioms for deriving triples of the form
<tt>{P}S{Q}</tt>, which means that if statement
<i>S</i> is executed from a state in which <i>P</i> is true and
terminates, then <i>Q</i> will be true.&nbsp; This paper introduces the
generalization of Hoare's method to concurrent programs, replacing
Hoare triples with assertions of the form <tt>{I}S</tt>,
which means that the individual actions of statement <i>S</i> leave
the predicate <i>I</i> invariant.&nbsp; 

<p>
When I wrote this paper, I sent a copy to Tony Hoare thinking that he
would like it.&nbsp; He answered with a letter that said, approximately:
"I always thought that the generalization to concurrent programs
would have to look something like that; that's why I never did it."
(Unfortunately, I no longer have the letter.)&nbsp; At the time, I
dismissed his remark as the ramblings of an old fogey.&nbsp; I now think he
was right--though probably for different reasons than he does.&nbsp; As I
indicated in the discussion of <A HREF = "#proving">[23]</A>, I think Ashcroft was
right; one should simply reason about a single global invariant, and
not do this kind of decomposition based on program structure.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "reaching"><B>Reaching Agreement in the Presence of Faults</B>&nbsp; (with Marshall 
   Pease and Robert Shostak)<BR><i>Journal of the
   Association for Computing Machinery 27</i>, 2 (April 1980).<BR>
<A HREF = "reaching.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1980 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


Before this paper, it was generally assumed that a three-processor
system could tolerate one faulty processor.&nbsp; This paper shows that
"Byzantine" faults, in which a faulty processor sends inconsistent
information to the other processors, can defeat any traditional
three-processor algorithm.&nbsp; (The term <i>Byzantine</i> didn't appear
until <A HREF = "#byz">[46]</A>.)&nbsp; In general, 3<it>n</it>+1
processors are needed to tolerate <i>n</i> faults.&nbsp; However, if
digital signatures are used, 2<it>n</it>+1 processors
are enough.&nbsp; This paper introduced the problem of handling Byzantine
faults.&nbsp; I think it also contains the first precise statement of the
consensus problem.&nbsp;

<p> I am often unfairly credited with inventing the Byzantine
agreement problem.&nbsp; The problem was formulated by people working on
SIFT (see <A HREF = "#sift">[30]</A>) before I arrived at SRI.&nbsp; I had
already discovered the problem of Byzantine faults and written
<A HREF = "#implementation">[29]</A>.&nbsp; (I don't know if that was earlier than or
concurrent with its discovery at SRI.)&nbsp; However, the people
at SRI had a much simpler and more elegant statement of the problem
than was present in <A HREF = "#implementation">[29]</A>.&nbsp;

<p>

The 4-processor solution presented in this paper and the general
impossibility result were obtained by Shostak; Pease invented the
3<it>n</it>+1-processor solution.&nbsp; My contribution to
the work in this paper was the solution using digital signatures,
which is based on the algorithm in <A HREF = "#implementation">[29]</A>.&nbsp; It was my
work on digital signatures (see <A HREF = "#dig-sig">[36]</A>) that led me to think
in that direction.&nbsp; However, digital signatures, as used here, are a
metaphor.&nbsp; Since the signatures need be secure only against random
failure, not against an intelligent adversary, they are much easier to
implement than true digital signatures.&nbsp; However, this point seems to
have escaped most people, so they rule out the algorithms that use
digital signatures because true digital signature algorithms are
expensive.&nbsp; Thus, 3<it>n</it>+1-processor solutions are
used even though there are satisfactory
2<it>n</it>+1-processor solutions,

<p>
My other contribution to this paper was getting it written.&nbsp; Writing
is hard work, and without the threat of perishing, researchers outside
academia generally do less publishing than their colleagues at
universities.&nbsp; I wrote an initial draft, which displeased Shostak so
much that he completely rewrote it to produce the final version.&nbsp;

<p>
Over the years, I often wondered whether the people who actually build
airplanes know about the problem of Byzantine failures.&nbsp; In 1997, I
received email from John Morgan who used to work at Boeing.&nbsp; He told
me that he came across our work in 1986 and that, as a result, the
people who build the passenger planes at Boeing are aware of the
problem and design their systems accordingly.&nbsp; But, in the late 80s
and early 90s, the people at Boeing working on military aircraft and
on the space station, and the people at McDonnell-Douglas, did not
understand the problem.&nbsp; I have no idea what Airbus knows or when
they knew it.&nbsp;

<p>
This paper was awarded the 2005 Edsger W. Dijkstra Prize in
Distributed Computing.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "program"><B>Program Verification: An Approach to Reliable Hardware
   and Software</B>&nbsp; (with J S. Moore)<BR><i>Transactions of the American Nuclear
   Society 35</i> (November 1980), 252-253.<BR>
No electronic version available.<HR>


In 1980, J&nbsp;Moore and I were both at SRI and had been
involved in the verification of SIFT (see <A HREF = "#sift">[30]</A>).&nbsp; The nuclear
power industry was, for obvious reasons, interested in the correctness
of computer systems.&nbsp; I forget how it came to pass, but Moore and I
were invited to present a paper on verification at some meeting of
power industry engineers.&nbsp; This was the result.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "password"><B>Password Authentication with Insecure
   Communication</B><BR><i>Communications of the ACM 24</i>, 11 &nbsp; (November 1981), 770-772.<BR>
<A HREF = "password.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1981 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


Despite a casual interest in civilian cryptography going back to its
origins (see the discussion of <A HREF = "#dig-sig">[36]</A>), this is my only
publication in the field.&nbsp; It presents a cute hack for using a single
password to login to a system multiple times without allowing an
adversary to gain access to the system by eavesdropping.&nbsp; This hack is
the basis of Bellcore's S/KEY system and of the PayWord system of
Rivest and Shamir.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "lamport-timesets"><B>TIMESETS--A New Method for Temporal 
   Reasoning About Programs</B><BR><i>Logics of Programs</i>, Dexter Kozen
   editor, Springer-Verlag Lecture Notes in Computer Science 
   Volume 131 (1982), 177-196.<BR>
No electronic version available.<HR>


Pnueli's introduction of temporal logic in 1977 led to an explosion of
attempts to find new logics for specifying and reasoning about
concurrent systems.&nbsp; Everyone was looking for the silver-bullet logic
that would solve the field's problems.&nbsp; This paper is proof that I was
not immune to this fever.&nbsp; For reasons explained in the discussion of
<A HREF = "#spec">[50]</A>, it is best forgotten.&nbsp; Some people may find this of
historical interest because it is an early example of an interval
logic.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "trans"><B>Byzantine Generals and Transaction Commit Protocols</B>&nbsp; (with Michael 
  Fischer)<BR>Unpublished (April 1982).<BR>
<A HREF = "trans.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "trans.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "trans.pdf">PDF</A><HR>


I visited Michael Fischer at Yale in the spring of 1982.&nbsp; It was known
that solutions to the Byzantine generals problem that can handle
<i>n</i> Byzantine failures require <i>n</i>+1 rounds of
communication.&nbsp; While I was at Yale, Fischer and I proved that this
number of rounds were needed even to handle more benign failures.&nbsp;

<p>
On the trip back home to California, I got on an airplane at Laguardia
Airport in the morning with a snowstorm coming in.&nbsp; I got off the
airplane about eight hours later, still at Laguardia Airport, having
written this paper.&nbsp; I then sent it to Fischer for his comments.&nbsp; I
waited about a year and a half.&nbsp; By the time he finally decided that
he wasn't going to do any more work on the paper, subsequent work by
others had been published that superseded it.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "byz"><B>The Byzantine Generals Problem</B>&nbsp; (with Marshall Pease and Robert
   Shostak)<BR><i>ACM Transactions on Programming Languages and Systems 4</i>, 3 (July 1982), 382-401.<BR>
<A HREF = "byz.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1982 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>

   
I have long felt that, because it was posed as a cute problem about
philosophers seated around a table, Dijkstra's dining philosopher's
problem received much more attention than it deserves.&nbsp; (For example,
it has probably received more attention in the theory community than
the readers/writers problem, which illustrates the same principles and
has much more practical importance.)&nbsp; I believed that the problem
introduced in <A HREF = "#reaching">[41]</A> was very important and deserved the
attention of computer scientists.&nbsp; The popularity of the dining
philosophers problem taught me that the best way to attract attention
to a problem is to present it in terms of a story.&nbsp;

<p> There is a problem in distributed computing that is
sometimes called the Chinese Generals Problem, in which two generals
have to come to a common agreement on whether to attack or retreat,
but can communicate only by sending messengers who might never arrive.&nbsp;
I stole the idea of the generals and posed the problem in terms of a
group of generals, some of whom may be traitors, who have to reach a
common decision.&nbsp; I wanted to assign the generals a nationality that
would not offend any readers.&nbsp; At the time, Albania was a completely
closed society, and I felt it unlikely that there would be any
Albanians around to object, so the original title of this paper was
<i>The Albanian Generals Problem</i>.&nbsp; Jack Goldberg was smart enough
to realize that there were Albanians in the world outside Albania, and
Albania might not always be a black hole, so he suggested that I find
another name.&nbsp; The obviously more appropriate Byzantine generals then
occurred to me.&nbsp;

<p>
The main reason for writing this paper was to assign the new name to
the problem.&nbsp; But a new paper needed new results as well.&nbsp; I came up
with a simpler way to describe the general
3<it>n</it>+1-processor algorithm.&nbsp; (Shostak's
4-processor algorithm was subtle but easy to understand; Pease's
generalization was a remarkable tour de force.)&nbsp; We also added a
generalization to networks that were not completely connected.&nbsp; (I
don't remember whose work that was.)&nbsp; I also added some discussion of
practical implementation details.&nbsp;



<BR>&nbsp;<P><LI> <A NAME = "liveness"><B>Proving Liveness Properties of Concurrent
   Programs</B>&nbsp; (with Susan Owicki)<BR><i>ACM Transactions on Programming Languages and Systems 4</i>, 3 (July 1982), 455-495.<BR>
<A HREF = "liveness.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1982 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


During the late 70s and early 80s, Susan Owicki and I worked together
quite a bit.&nbsp; We were even planning to write a book on concurrent
program verification.&nbsp; But this paper is the only thing we ever wrote
together.&nbsp; 

<p>
In <A HREF = "#proving">[23]</A>, I had introduced a method of proving eventuality
properties of concurrent programs by drawing a lattice of predicates,
where arrows from a predicate <i>P</i> to predicates <i>Q1</i>, <tt>...</tt>, <i>Qn</i> mean that, if the
program reaches a state satisfying <i>P</i>, it must thereafter reach
a state satisfying one of the <i>Qi</i>.&nbsp; That method
never seemed practical; formalizing an informal proof was too much
work.&nbsp;

<p> 
Pnueli's introduction of temporal logic allowed the predicates in the
lattice to be replaced by arbitrary temporal formulas.&nbsp; This turned
lattice proofs into a useful way of proving liveness properties.&nbsp; It
permitted a straightforward formalization of a particularly style of
writing the proofs.&nbsp; I still use this proof style to prove leads-to
properties, though the proofs are formalized with TLA (see
<A HREF = "#lamport-actions">[103]</A>).&nbsp; However, I no longer bother drawing
pictures of the lattices.&nbsp; This paper also introduced <i>at</i>,
<i>in</i>, and <i>after</i> predicates for describing program control.&nbsp;

<p> 
It's customary to list authors alphabetically, unless one contributed
significantly more than the other, but at the time, I was unaware of
this custom.&nbsp; Here is Owicki's account of how the ordering of the
authors was determined.&nbsp;

<blockquote><P> As I recall it, you raised the
question of order, and I proposed alphabetical order.&nbsp; You
declined--I think you expected the paper to be important and didn't
think it would be fair to get first authorship on the basis of a
static property of our names.&nbsp; On the night we finished the paper, we
went out to dinner to celebrate, and you proposed that if the last
digit of the bill was even (or maybe odd), my name would be first.&nbsp;
And, indeed, that's the way it came out.&nbsp;
</P></blockquote>



<BR>&nbsp;<P><LI> <A NAME = "dist"><B>An Assertional Correctness Proof of a Distributed
   Program</B><BR><i>Science of Computer Programming 2</i>, 3 (December
   1982), 175-206.<BR><A HREF="https://www.sciencedirect.com/science/article/pii/016764238390014X">Available On-Line</A><HR>


I showed in <A HREF = "#time-clocks">[27]</A> that there is no invariant way of
defining the global state of a distributed system.&nbsp; Assertional
methods, such as <A HREF = "#proving">[23]</A>, reason about the global state.&nbsp; So,
I concluded that these methods were not appropriate for reasoning
about distributed systems.&nbsp; When I wrote this paper, I was at SRI and
partly funded by a government contract for which we had promised to
write a correctness proof of a distributed algorithm.&nbsp; I tried to
figure out how to write a formal proof without reasoning about the
global state, but I couldn't.&nbsp; The final report was due, so I decided
that there was no alternative to writing an assertional proof.&nbsp; I knew
there would be no problem writing such a proof, but I expected that,
with its reliance on an arbitrary global state, the proof would be
ugly.&nbsp; To my surprise, I discovered that the proof was quite elegant.&nbsp;
Philosophical considerations told me that I shouldn't reason about
global states, but this experience indicated that such reasoning
worked fine.&nbsp; I have always placed more reliance on experience than
philosophy, so I have written assertional proofs of distributed
systems ever since.&nbsp; (Others who were more inclined to philosophy spent
decades looking for special ways to reason about distributed systems.)&nbsp;



<BR>&nbsp;<P><LI> <A NAME = "nonatomic"><B>Reasoning About Nonatomic
   Operations</B><BR><i>Proceedings of the Tenth ACM Symposium on Principles
   of Programming Languages</i>, ACM SIGACT-SIGPLAN (January 1983), 28-37.<BR>
<A HREF = "nonatomic.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1983 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


 From the time I discovered the bakery algorithm (see <A HREF = "#bakery">[12]</A>),
I was fascinated by the problem of reasoning about a concurrent
program without having to break it into indivisible atomic actions.&nbsp;
In <A HREF = "#new-approach">[33]</A>, I described how to do this for behavioral
reasoning.&nbsp; But I realized that assertional reasoning, as described in
<A HREF = "#proving">[23]</A>, was the only proof method that could scale to more
complex problems.&nbsp; This paper was my first attempt at assertional
reasoning about nonatomic operations.&nbsp; It introduces the <i>win</i>
(weakest invariant) operator that later appeared in
<A HREF = "#lamport-win">[86]</A>, but using the notation of Pratt's dynamic logic
rather than Dijkstra's predicate transformers.&nbsp;

<p>
I have in my files a letter from David Harel, who was then an editor
of <i>Information and Control</i>, telling me that the paper was
accepted by the journal, after revision to satisfy some concerns of
the referees.&nbsp; I don't remember why I didn't submit a revised version.&nbsp;
I don't think I found the referees' requests unreasonable.&nbsp; It's
unlikely that I abandoned the paper because I had already developed
the method in <A HREF = "#lamport-win">[86]</A>, since that didn't appear as a SRC
research report until four years later.&nbsp; Perhaps I was just too busy.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "spec"><B>Specifying Concurrent Program Modules</B><BR><i>ACM Transactions on Programming Languages and Systems 5</i>, 2
   (April 1983), 190-222.<BR>
<A HREF = "spec.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1983 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


The early methods for reasoning about concurrent programs dealt with
proving that a program satisfied certain properties--usually
invariance properties.&nbsp; But, proving particular properties showed only
that the program satisfied those properties.&nbsp; There remained the
possibility that the program was incorrect because it failed to
satisfy some other properties.&nbsp; Around the early 80s, people working
on assertional verification began looking for ways to write a complete
specification of a system.&nbsp; A specification should say precisely what
it means for the system to be correct, so that if we prove that the
system meets its specification, then we can say that the system
really is correct.&nbsp; (Process algebraists had already been working on
that problem since the mid-70s, but there was--and I think still
is--little communication between them and the assertional
verification community.)&nbsp;

<p>
At SRI, we were working on writing temporal logic specifications.&nbsp; One
could describe properties using temporal logic, so it seemed very
natural to specify a system by simply listing all the properties it
must satisfy.&nbsp; Richard Schwartz, Michael Melliar-Smith, and I
collaborated on a paper titled <i>Temporal Logic Specification of
Distributed Systems</i>, which was published in the Proceedings of the
2nd International Conference on Distributed Computing Systems, held in
Paris in 1981.&nbsp; However, I became disillusioned with temporal logic
when I saw how Schwartz, Melliar-Smith, and Fritz Vogt were
spending days trying to specify a simple FIFO queue--arguing over
whether the properties they listed were sufficient.&nbsp; I realized that,
despite its aesthetic appeal, writing a specification as a conjunction
of temporal properties just didn't work in practice.&nbsp; So, I had my
name removed from the paper before it was published, and I set about
figuring out a practical way to write specifications.&nbsp; I came up with
the approach described in this paper, which I later called the
<i>transition axiom</i> method.&nbsp; 
Schwartz stopped working on specification and verification
in the mid-80s.&nbsp; He wrote recently (in June 2000):
<blockquote><P>
[T]he same frustration with the use of temporal logic led Michael,
Fritz Vogt and me to come up with Interval Logic as a higher level
model in which to express time-ordered properties of events.&nbsp; 
[See <A HREF = "#lamport-timesets">[44]</A>.]&nbsp; As you
recall, interval logic expressed constraints forward and backward
around significant events in order to more closely capture the way that
people describe event-driven behavior.&nbsp; Ultimately, I remain
unsatisfied with any of our attempts, from the standpoint of reaching
practical levels.&nbsp;
</P></blockquote> 


<p>
This paper is the first place I used the idea of describing a state
transition as a boolean-valued function of primed and unprimed
variables.&nbsp; However, by the early 80s, the idea must have been
sufficiently obvious that I didn't claim any novelty for it, and I
forgot that I had even used it in this paper until years later (see
the discussion of <A HREF = "#lamport-actions">[103]</A>).&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "spec-and-proof"><B>Specification and Proof of a
   Fault-Tolerant Real-Time Algorithm</B><BR>In <i>Highly Dependable
   Distributed Systems</i>, final report for SRI Project 4180 (Contract
   Number DAEA18-81-G-0062) (June 1983).<BR>
No electronic version available.<HR>


In the spring of 1983, I was called upon to contribute a chapter for
the final report on a project at SRI.&nbsp; I chose to write a
specification and correctness proof of a Byzantine general's
algorithm--a distributed, real-time algorithm.&nbsp; (Nonfaulty components
must satisfy real-time constraints, and the correctness of the
algorithm depends on these constraints.)&nbsp; I began the exercise on a
Wednesday morning.&nbsp; By noon that Friday, I had the final typeset
output.&nbsp; I presume there are lots of errors; after finishing it, I
never reread it carefully and I have no indication that anyone else
did either.&nbsp; But, I have no reason to doubt the basic correctness of
the proof.&nbsp; I never published this paper because it didn't seem worth
publishing.&nbsp; The only thing I find remarkable about it is that so many
computer scientists are unaware that, even in 1983, writing a formal
correctness proof of a distributed real-time algorithm was an
unremarkable feat.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "weak-byz"><B>The Weak Byzantine Generals Problem</B><BR><i>Journal of
   the Association for Computing Machinery 30</i>, 3 (July 1983), 668-676.<BR>
<A HREF = "weak-byz.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1983 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


This paper introduces a weaker version of the Byzantine generals
problem described in <A HREF = "#reaching">[41]</A>.&nbsp; The problem is "easier"
because there exist approximate solutions with fewer than
3<i>n</i> processes that can tolerate <i>n</i> faults,
something shown in <A HREF = "#reaching">[41]</A> to be impossible for the original
Byzantine generals problem.&nbsp; I don't remember how I came to consider
this problem.&nbsp;



<BR>&nbsp;<P><LI> <A NAME = "phil"><B>PHIL: A Semantic Structural Graphical Editor</B>&nbsp; (with Joseph 
   Goguen)<BR>SRI International Technical Report (August 1983).<BR>
No electronic version available.<HR>


SRI had a contract with Philips to design a graphical editor for
structured documents (such as programs).&nbsp; Goguen and I were the prime
instigators and principal investigators of the project.&nbsp; This is the
project's final report.&nbsp; Rindert Schutten of Philips visited SRI and
implemented a very preliminary version.&nbsp; I felt that our design was
neither novel enough to constitute a major contribution nor modest
enough to be the basis for a practical system at that time, and I
thought the project had been dropped.&nbsp; However, Goguen informed me
much later that some version of the system was still being used in the
early 90s, and that it had evolved into a tool for VLSI layout,
apparently called MetaView.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "what-good"><B>What Good Is Temporal Logic?</B><BR><i>Information
   Processing 83,</i> R. E. A. Mason, ed., Elsevier Publishers (1983),
   657-668.<BR>
<A HREF = "what-good.pdf">PDF</A><HR>


This was an invited paper.&nbsp; It describes the state of my views on
specification and verification at the time.&nbsp; It is notable for
introducing the idea of invariance under stuttering and explaining why
it's a vital attribute of a specification logic.&nbsp; It is also one of my
better-written papers.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "using-time"><B>Using Time Instead of Timeout for
   Fault-Tolerant Distributed Systems</B><BR><i>ACM Transactions on Programming Languages and Systems 6</i>, 2 (April 1984),
   254-280.<BR>
<A HREF = "using-time.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1984 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>



The genesis of this paper was my realization that, in a multiprocess
system with synchronized clocks, the absence of a message can carry
information.&nbsp; I was fascinated by the idea that a process could
communicating zillions of bits of information by <i>not</i> sending
messages.&nbsp; The practical implementation of Byzantine generals
algorithms described in <A HREF = "#byz">[46]</A> could be viewed as an application
of this idea.&nbsp; I used the idea as something of a gimmick to
justify the paper.&nbsp; The basic message of this paper should have been
pretty obvious: the state machine approach, introduced in
<A HREF = "#time-clocks">[27]</A>, allows us to turn any consensus algorithm into
a general method for implementing distributed systems; the Byzantine
generals algorithms of <A HREF = "#byz">[46]</A> were fault-tolerant consensus
algorithms; hence, we had fault-tolerant implementations of
arbitrary distributed systems.&nbsp; I published the paper because I had
found few computer scientists who understood this.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "ghl"><B>The Hoare Logic Of CSP, and All That</B>&nbsp; (with Fred Schneider)<BR>  
   <i>ACM Transactions on Programming Languages and Systems 6</i>, 2 (April 1984), 281-296.<BR>
<A HREF = "ghl.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1984 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


I felt that in <A HREF = "#hoare">[40]</A>, I had presented the right way to do
assertional (also known as Owicki-Gries style) reasoning about
concurrent programs.&nbsp; However, many people were (and perhaps still
are) hung up on the individual details of different programming
languages and are unable to understand that the same general
principles apply to all of them.&nbsp; In particular, people felt that
"distributed" languages based on rendezvous or message passing were
fundamentally different from the shared-variable language that was
considered in <A HREF = "#hoare">[40]</A>.&nbsp; For example, some people made the silly
claim that the absence of shared variables made it easier to write
concurrent programs in CSP than in more conventional languages.&nbsp; (My
response is the equally silly assertion that it's harder to write
concurrent programs in CSP because the control state is shared between
processors.)&nbsp;

<p> Schneider agreed with me that invariance was the central
concept in reasoning about concurrent programs.&nbsp; He was also an expert
on all the different flavors of message passing that had been
proposed.&nbsp; We demonstrated in this paper that the basic
approach of <A HREF = "#hoare">[40]</A> worked just was well with CSP; and we
claimed (without proof) that it also worked in other "distributed"
languages.&nbsp; I found it particularly funny that we should be the ones
to give a Hoare logic to CSP, while Hoare was using essentially
behavioral methods to reason about CSP programs.&nbsp; I'm still waiting
for the laughter.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "clocks2"><B>Byzantine Clock Synchronization</B>&nbsp; (with Michael 
   Melliar-Smith)<BR><i>Proceedings of the Third Annual ACM Symposium on
   Principles of Distributed Computing</i> (August, 1984), 68-74.<BR>
<A HREF = "clocks2.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1984 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


This is the preliminary conference version of <A HREF = "#clocks">[62]</A>.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "solved-and-unsolved"><B>Solved Problems, Unsolved Problems 
    and NonProblems in Concurrency</B><BR><i>Proceedings of the Third Annual ACM
   Symposium on Principles of Distributed Computing</i> (August, 1984)
   1-11.<BR>
<A HREF = "solved-and-unsolved.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1984 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


This is the invited address I gave at the 1983 PODC conference, which
I transcribed from a tape recording of my presentation.&nbsp; The first few
minutes of the talk were not taped, so I had to reinvent the
beginning.&nbsp; This talk is notable because it marked the rediscovery by
the computer science community of Dijkstra's 1974 <i>CACM</i> paper that
introduced the concept of self-stabilization.&nbsp; A self-stabilizing
system is one that, when started in any state, eventually "rights
itself" and operates correctly.&nbsp; The importance of self-stabilization
to fault tolerance was obvious to me and a handful of people, but went
completely over the head of most readers.&nbsp; Dijkstra's paper gave
little indication of the practical significance of the problem, and
few people understood its importance.&nbsp; So, this gem of a paper had
disappeared without a trace by 1983.&nbsp; My talk brought Dijkstra's paper
to the attention of the PODC community, and now self-stabilization is
a regular subfield of distributed computing.&nbsp; I regard the
resurrection of Dijkstra's brilliant work on self-stabilization to be
one of my greatest contributions to computer science.&nbsp;

<P>
The paper contains one figure--copied directly from a
transparency--with an obviously bogus algorithm.&nbsp; I tried to recreate
an algorithm from memory and wrote complete nonsense.&nbsp; It's easy to
make such a mistake when drawing a transparency, and I probably didn't
bother to look at it when I prepared the paper.&nbsp; To my knowledge, it
is the only incorrect algorithm I have published.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "peterson-theorem"><B>On a "Theorem" of Peterson</B><BR>Unpublished
  (October, 1984).<BR>
<A HREF = "peterson-theorem.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "peterson-theorem.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "peterson-theorem.pdf">PDF</A><HR>


This three-page note gives an example that appears to contradict a
theorem in a <i>TOPLAS</i> article by Gary Peterson.&nbsp; Whether or not
it does depends on the interpretation of the statement of the theorem,
which is given only informally in English.&nbsp; I draw the moral that
greater rigor is needed.&nbsp; When I sent this paper to Peterson, he
strongly objected to it.&nbsp; I no longer have his message and don't
remember exactly what he wrote, but I think he said that he knew what
the correct interpretation was and that I was unfairly suggesting that
his theorem might be incorrect.&nbsp; So, I never published this note.&nbsp;



<BR>&nbsp;<P><LI> <A NAME = "buridan"><B>Buridan's Principle</B><BR><i>Foundations
of Physics 42</i>, 8 (August 2012) 1056-1066.<BR>
<A HREF = "buridan.pdf">PDF</A><HR>


I have observed that the arbiter problem, discussed in
<A HREF = "#glitch">[22]</A>, occurs in daily life.&nbsp; Perhaps the most common
example is when I find myself unable to decide for a fraction of a
second whether to stop for a traffic light that just turned yellow or
to go through.&nbsp; I suspect that it is actually a cause of serious
accidents, and that people do drive into telephone poles because they
can't decide in time whether to go to the left or the right.&nbsp;

<p>
A little research revealed that psychologists are totally unaware of
the phenomenon.&nbsp; I found one paper in the psychology literature on the
time taken by subjects to choose between two alternatives based on how
nearly equal they were.&nbsp; The author's theoretical calculation yielded
a formula with a singularity at zero, as there should be.&nbsp; He compared
the experimental data with this theoretical curve, and the fit was
perfect.&nbsp; He then drew, as the curve fitting the data, a bounded
continuous graph.&nbsp; The singularity at zero was never mentioned in the
paper.&nbsp;

<p> I feel that the arbiter problem is important and should be
made known to scientists outside the field of computing.&nbsp; So, in
December of 1984 I wrote this paper.&nbsp; It describes the problem in its
classical formulation as the problem of Buridan's ass--an ass that
starves to death because it is placed equidistant between two bales of
hay and has no reason to prefer one to the other.&nbsp; Philosophers have
discussed Buridan's ass for centuries, but it apparently never
occurred to any of them that the planet is not littered with dead
asses only because the probability of the ass being in just the right
spot is infinitesimal.&nbsp;

<p>
I wrote this paper for the general scientific community.&nbsp; I
probably could have published it in some computer journal, but that
wasn't the point.&nbsp; I submitted it first to <i>Science</i>.&nbsp; The four
reviews ranged from "This well-written paper is of major
philosophical importance" to "This may be an elaborate joke." One
of the other reviews was more mildly positive, and the fourth said
simply "My feeling is that it is rather superficial." The paper was
rejected.&nbsp;

<p> Some time later, I submitted the paper to <i>Nature</i>.&nbsp;
I don't like the idea of sending the same paper to different journals
hoping that someone will publish it, and I rarely resubmit a rejected
paper elsewhere.&nbsp; So, I said in my submission letter that it had been
rejected by <i>Science</i>.&nbsp; The editor read the paper and sent me
some objections.&nbsp; I answered his objections, which were based on
reasonable misunderstandings of the paper.&nbsp; In fact, they made me
realize that I should explain things differently for a more general
audience.&nbsp; He then replied with further objections of a similar
nature.&nbsp; Throughout this exchange, I wasn't sure if he was taking the
matter seriously or if he thought I was some sort of crank.&nbsp; So, after
answering his next round of objections, I wrote that I would be happy
to revise the paper in light of this discussion if he would then send
it out for review, but that I didn't want to continue this private
correspondence.&nbsp; The next letter I received was from another
<i>Nature</i> editor saying that the first editor had been reassigned
and that he was taking over my paper.&nbsp; He then raised some objections
to the paper that were essentially the same as the ones raised
initially by the first editor.&nbsp; At that point, I gave up in disgust.&nbsp;

<p> 

I had no idea where to publish the paper, so I let it drop.&nbsp; In 2011,
a reader, Thomas Ray, suggested submitting it to <i>Foundations of
Physics</i>.&nbsp; I did, and it was accepted.&nbsp; For the published version, I
added a concluding section mentioning some of the things that had
happened in the 25 years since I wrote the original version and citing
this entry as a source of information about the paper's history.&nbsp;

<p> My problems in trying to publish this paper and
<A HREF = "#glitch">[22]</A> are part of a long tradition.&nbsp; According to one story
I've heard (but haven't verified), someone at G. E. discovered the
phenomenon in computer circuits in the early 60s, but was unable to
convince his managers that there was a problem.&nbsp; He published a short
note about it, for which he was fired.&nbsp; Charles Molnar, one of the
pioneers in the study of the problem, reported the following in a
lecture given on February 11, 1992, at HP Corporate Engineering in
Palo Alto, California:
 <blockquote><P> 
       One reviewer made
       a marvelous comment in rejecting one of the
       early papers, saying that if this problem <i>really</i>
       existed it would be so important that
       everybody knowledgeable in the field would
       have to know about it, and "I'm an expert and
       I don't know about it, so therefore it must not
       exist."
 </P></blockquote>
Another amusing example occurred in an article by Charles Seif titled
<i>Not Every Vote Counts</i> that appeared on the op-ed page of the
<i>New York Times</i> on 4 December 2008.&nbsp; Seif proposed that
elections be decided by a coin toss if the voting is very close,
thereby avoiding litigious disputes over the exact vote count.&nbsp; While
the problem of counting votes is not exactly an instance of Buridan's
Principle, the flaw in his scheme will be obvious to anyone familiar
with the principle and the futile attempts to circumvent it.&nbsp; I
submitted a letter to the <i>Times</i> explaining the problem and sent
email to Seif, but I received no response.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "mutual"><B>The Mutual Exclusion Problem--Part I: A Theory of
   Interprocess Communication, Part II: Statement and
   Solutions</B><BR><i>Journal of the Association for Computing Machinery
   33</i>, 2 (January 1986) 313-348.<BR>
Part 1: <A HREF = "mutual1.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "mutual1.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "mutual1.pdf">PDF</A><BR>
Part 2: <A HREF = "mutual2.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "mutual2.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "mutual2.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1985 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


For some time I had been looking for a mutual exclusion algorithm that
satisfied my complete list of desirable properties.&nbsp; I finally found
one--the <i>N</i>!-bit algorithm described in this paper.&nbsp;
The algorithm is wildly impractical, requiring <i>N</i>!
bits of storage for <i>N</i> processors, but practicality was not one
of my requirements.&nbsp; So, I decided to publish a compendium of
everything I knew about the theory of mutual exclusion.&nbsp;

<p>
The 3-bit algorithm described in this paper came about because of a
visit by Michael Rabin.&nbsp; He is an advocate of probabilistic
algorithms, and he claimed that a probabilistic solution to the mutual
exclusion problem would be better than a deterministic one.&nbsp; I believe
that it was during his brief visit that we came up with a
probabilistic algorithm requiring just three bits of storage per
processor.&nbsp; Probabilistic algorithms don't appeal to me.&nbsp; (This is a
question of aesthetics, not practicality.)&nbsp; So later, I figured out
how to remove the probability and turn it into a deterministic
algorithm.&nbsp;

<p>
The first part of the paper covers the formalism for describing
nonatomic operations that I had been developing since the 70s, and
that is needed for a rigorous exposition of mutual exclusion.&nbsp; (See
the discussion of <A HREF = "#interprocess">[70]</A>.)&nbsp;



<BR>&nbsp;<P><LI> <A NAME = "clocks"><B>Synchronizing Clocks in the Presence of Faults</B>&nbsp; (with Michael 
  Melliar-Smith)<BR> <i>Journal of the Association for
  Computing Machinery 32</i>, 1 (January 1985), 52-78.<BR>
<A HREF = "clocks.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1985 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


Practical implementation of Byzantine agreement requires synchronized
clocks.&nbsp; For an implementation to tolerate Byzantine faults, it needs
a clock synchronization algorithm that can tolerate those faults.&nbsp;
When I arrived at SRI, there was a general feeling that we could
synchronize clocks by just having each process use a Byzantine
agreement protocol to broadcast its clock value.&nbsp; I was never
convinced by that hand waving.&nbsp; So, at some point I tried to write
down precise clock-synchronization algorithms and prove their
correctness.&nbsp; The two basic Byzantine agreement algorithms from
<A HREF = "#byz">[46]</A> did generalize to clock-synchronization algorithms.&nbsp; In
addition, Melliar-Smith had devised the interactive convergence
algorithm, which is also included in the paper.&nbsp; (As I recall, that
algorithm was his major contribution to the paper, and I wrote all the
proofs.)&nbsp;

<p>
Writing the proofs turned out to be much more difficult than I had
expected (see <A HREF = "#time-clocks">[27]</A>).&nbsp; I worked very hard to make them
as short and easy to understand as I could.&nbsp; So, I was rather annoyed
when a referee said that the proofs seemed to have been written
quickly and could be simplified with a little effort.&nbsp; In my replies
to the reviews, I referred to that referee as a "supercilious
bastard".&nbsp; Some time later, Nancy Lynch confessed to being that
referee.&nbsp; She had by then written her own proofs of clock
synchronization and realized how hard they were.&nbsp;

<p>
Years later, John Rushby and his colleagues at SRI wrote mechanically
verified versions of my proofs.&nbsp; They found only a couple of minor
errors.&nbsp; I'm rather proud that, even before I knew how to write
reliable, structured proofs (see <A HREF = "#lamport-how-to-write">[102]</A>), I was
sufficiently careful and disciplined to have gotten those proofs
essentially correct.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "priority"><B>What It Means for a Concurrent Program to Satisfy a
   Specification: Why No One Has Specified Priority</B><BR><i>Proceedings of
   the Twelfth ACM Symposium on Principles of Programming Languages</i>, ACM
   SIGACT-SIGPLAN (January 1985), 78-83.<BR>
<A HREF = "priority.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "priority.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "priority.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1985 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


I must have spent a lot of time at SRI arguing with Schwartz and
Melliar-Smith about the relative merits of temporal logic and
transition axioms.&nbsp; (See the discussion of <A HREF = "#spec">[50]</A>.)&nbsp; I don't
remember exactly what happened, but this paper's acknowledgment
section says that "they kept poking holes in my attempts to specify
FCFS [first-come, first-served] until we all finally recognized the
fundamental problem [that it can't be done]."


<BR>&nbsp;<P><LI> <A NAME = "alias"><B>Constraints: A Uniform Approach to Aliasing and
   Typing</B>&nbsp; (with Fred Schneider)<BR><i>Proceedings of the Twelfth ACM
   Symposium on Principles of Programming Languages</i>, ACM
   SIGACT-SIGPLAN (January 1985), 205-216.<BR>
<A HREF = "alias.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "alias.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "alias.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1985 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


My generalized Hoare logic requires reasoning about control
predicates, using the <i>at</i>, <i>in</i>, and <i>after</i>
predicates introduced in <A HREF = "#liveness">[47]</A>.&nbsp; These are not independent
predicates--for example, being after one statement is synonymous with
being at the following statement.&nbsp; At some point, Schneider and I
realized that the relations between control predicates could be viewed
as a generalized form of aliasing.&nbsp; Our method of dealing with control
predicates led to a general approach for handling aliasing in ordinary
Hoare logic, which is described in this paper.&nbsp; In addition to
handling the usual aliasing in programs, our method allowed one to
declare that the variables <i>r</i> and <i>theta</i>
were aliases of the variables <i>x</i> and <i>y</i> according to the
relations <i>x</i>&nbsp;=&nbsp;<i>r</i>&nbsp;*&nbsp;cos(<i>theta</i>) and
 <i>y</i>&nbsp;=&nbsp;<i>r</i>&nbsp;*&nbsp;sin(<i>theta</i>).&nbsp;

<p>
We generalized this work to handle arrays and pointers, and even cited
a later paper about this generalization.&nbsp; But, as has happened so often
when I write a paper that mentions a future one, the future paper
was never written.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "recursive-compiling"><B>Recursive Compiling and Programming
Environments (Summary)</B><BR>Rejected from the 1985 POPL Conference..<BR>
<A HREF = "recursive-compiling.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "recursive-compiling.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "recursive-compiling.pdf">PDF</A><HR>


This is an extended abstract I submitted to the 1995 POPL conference.&nbsp;
(I never wrote the complete version.)&nbsp; It proposes the idea of
recursive compiling, in which a program constructs a text string and
calls the compiler to compile it in the context of the current program
environment.&nbsp; Thus, a variable <i>foo</i> in the string is interpreted
by the compiler to mean whatever <i>foo</i> means at the current point
in the calling program.&nbsp; No one found the idea very compelling.&nbsp; When
I discussed it with Eric Roberts, he argued that run-time linking
would be a simpler way to provide the same functionality.&nbsp; I don't
know if Java's reflection mechanism constitutes recursive compiling or
just run-time linking.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "chandy"><B>Distributed Snapshots: Determining Global States of a
   Distributed System</B>&nbsp; (with Mani Chandy)<BR><i>ACM Transactions on Computer
   Systems 3</i>, 1 (February, 1985), 63-75.<BR>
<A HREF = "chandy.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1985 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


The distributed snapshot algorithm described here came about when I
visited Chandy, who was then at the University of Texas in Austin.&nbsp; He
posed the problem to me over dinner, but we had both had too much wine
to think about it right then.&nbsp; The next morning, in the shower, I came
up with the solution.&nbsp; When I arrived at Chandy's office, he was
waiting for me with the same solution.&nbsp; I consider the algorithm to
be a straightforward application of the basic ideas from
<A HREF = "#time-clocks">[27]</A>.&nbsp;

<p> 

In 2012, a reader noticed that the paper's reference list includes a
paper by Chandy and me titled <i>On Partially-Ordered Event Models
of Distributed Computations</i>, claiming it had been submitted for
publication.&nbsp; Several times I have made the mistake of referencing a
paper of mine "to appear" that never appeared.&nbsp; But I can't imagine
that I would have claimed that a nonexistent paper had been submitted for
publication.&nbsp; However, neither Chandy nor I have any memory of that
paper or the reference.&nbsp; My guess is that we inserted the reference
in a preliminary version when we expected to write and submit the
other paper, and then we forgot to remove it.&nbsp;



<BR>&nbsp;<P><LI> <A NAME = "munich"><B>Formal Foundation for Specification and
   Verification</B>&nbsp; (with Fred Schneider)<BR>Chapter 5 in <i>Distributed Systems:
   Methods and Tools for Specification</i>, Alford et al.&nbsp; Lecture Notes in
   Computer Science, Number 190.&nbsp; Springer-Verlag, Berlin (1985).<BR>Electronic version available from publisher.<HR>


This volume contains the notes for a two-week course given in Munich
in April of 1984 and again in April of 1985.&nbsp; Fred Schneider and I
lectured on the contents of <A HREF = "#ghl">[56]</A> and <A HREF = "#liveness">[47]</A>.&nbsp; This
chapter is of historical interest because it's the first place where I
published the precise definition of a safety property.&nbsp; (The concepts
of safety and liveness were introduced informally in
<A HREF = "#proving">[23]</A>.)&nbsp; This inspired Schneider to think about what the
precise definition of liveness might be.&nbsp; Shortly thereafter, he and
Bowen Alpern came up with the formal definition.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "semantics"><B>An Axiomatic Semantics of Concurrent Programming
   Languages</B><BR>In <i>Logics and Models of Concurrent Systems</i>,
   Krzysztof Apt, editor.&nbsp; Springer-Verlag, Berlin (1985), 77-122.<BR>
<A HREF = "semantics.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "semantics.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "semantics.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1985 by Springer-Verlag.</FONT><HR>


This paper appeared in a workshop held in Colle-sur-Loup, in the south
of France, in October, 1984.&nbsp; There is a long history of work on the
semantics of programming languages.&nbsp; When people began studying
concurrency in the 70s, they naturally wrote about the semantics of
concurrent languages.&nbsp; It always seemed to me that defining the
semantics of a concurrent language shouldn't be very hard.&nbsp; Once you
know how to specify a concurrent system, it's a straightforward task
to give a semantics to a concurrent programming language by specifying
the programs written in it.&nbsp; Writing this paper allowed me to
demonstrate that writing a semantics is as easy as I thought it was.&nbsp;
What I did discover from writing the paper is that the semantics of
programming languages is a very boring subject.&nbsp; I found this paper
boring to write; I find it boring to read.&nbsp; I have never worked on the
semantics of programming languages again.&nbsp;



<BR>&nbsp;<P><LI> <A NAME = "latex"><B>LaTeX: A Document Preparation
   System</B><BR><i>Addison-Wesley, Reading, Mass. (1986)</i>.<BR>
No electronic version available.<HR>


In the early 80s, I was planning to write the Great American
Concurrency Book.&nbsp; I was a TeX user, so I would need a set
of macros.&nbsp; I thought that, with a little extra effort, I could make
my macros usable by others.&nbsp; Don Knuth had begun issuing early
releases of the current version of TeX, and I figured I
could write what would become its standard macro package.&nbsp; That was
the beginning of LaTeX.&nbsp; I was planning to write a user
manual, but it never occurred to me that anyone would actually pay
money for it.&nbsp; In 1983, Peter Gordon, an Addison-Wesley editor, and
his colleagues visited me at SRI.&nbsp; Here is his account of
what happened.&nbsp;

<blockquote><P>
Our primary mission was to gather information for Addison-Wesley "to
publish a computer-based document processing system specifically
designed for scientists and engineers, in both academic and
professional environments." This system was to be part of a series of
related products (software, manuals, books) and services (database,
production).&nbsp; (La)TeX was a candidate to be at the core of
that system.&nbsp; (I am quoting from the original business plan.)&nbsp;
Fortunately, I did not listen to your doubt that anyone would buy the
LaTeX manual, because more than a few hundred thousand
people actually did.&nbsp; The exact number, of course, cannot accurately
be determined, inasmuch as many people (not all friends and relatives)
bought the book more than once, so heavily was it used.&nbsp;
</P></blockquote> 
Meanwhile, I still haven't written the Great American Concurrency
Book.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "interprocess"><B>On Interprocess Communication--Part I: Basic 
   Formalism, Part II: Algorithms</B><BR><i>Distributed Computing 1</i>, 2 (1986),
   77-101.&nbsp; Also appeared as SRC Research Report&nbsp;8.<BR>
<A HREF = "interprocess.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "interprocess.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "interprocess.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1986 by Springer-Verlag.</FONT><HR>


Most computer scientists regard synchronization problems, such as the
mutual exclusion problem, to be problems of mathematics.&nbsp; How can you
use one class of mathematical objects, like atomic reads and writes,
to implement some other mathematical object, like a mutual exclusion
algorithm?  I have always regarded synchronization problems to be
problems of physics.&nbsp; How do you use physical objects, like registers,
to achieve physical goals, like not having two processes active at the
same time?

<p>
With the discovery of the bakery algorithm (see <A HREF = "#bakery">[12]</A>), I
began considering the question of how two processes communicate.&nbsp; I
came to the conclusion that asynchronous communication requires some
object whose state can be changed by one process and observed by the
other.&nbsp; We call such an object a register.&nbsp; This paper introduced
three classes of registers.&nbsp; The weakest class with which arbitrary
synchronization is possible is called <i>safe</i>.&nbsp; The next strongest
is called <i>regular</i> and the strongest, generally assumed by
algorithm writers, is called <i>atomic</i>.&nbsp; 

<p>
I had obtained all the results presented here in the late 70s and had
described them to a number of computer scientists.&nbsp; Nobody found them
interesting, so I never wrote them up.&nbsp; Around 1984, I saw a paper by
Jay Misra, motivated by VLSI, that was heading in the general
direction of my results.&nbsp; It made me realize that, because VLSI had
started people thinking about synchronization from a more physical
perspective, they might now be interested in my results about
registers.&nbsp; So, I wrote this paper.&nbsp; As with <A HREF = "#mutual">[61]</A>, the
first part describes my formalism for describing systems with
nonatomic operations.&nbsp; This time, people were interested--perhaps
because it raised the enticing unsolved problem of implementing
multi-reader and multi-writer atomic registers.&nbsp; It led to a brief
flurry of atomic register papers.&nbsp;

<p>
Fred Schneider was the editor who processed this paper.&nbsp; He kept
having trouble understanding the proof of my atomic register
construction.&nbsp; After a couple of rounds of filling in the details of
the steps that Schneider couldn't follow, I discovered that the
algorithm was incorrect.&nbsp; Fortunately, I was able to fix the algorithm
and write a proof that he, I, and, as far as I know, all subsequent
readers did believe.&nbsp;

<p> Some fifteen years later, Jerry James, a member of the
EECS department at the University of Kansas, discovered a small error
in Proposition 1 when formalizing the proofs with the PVS mechanical
verification system.&nbsp; He proved a corrected version of the proposition
and showed how that version could be used in place of the original
one.&nbsp; A pdf file containing a note by James describing the
error and its correction can be obtained by <A href =
"james-erratum.pdf">clicking here</A>.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "the-byz-generals"><B>The Byzantine Generals</B>&nbsp; (with Danny Dolev,
  Marshall Pease, and Robert Shostak)<BR>In <i>Concurrency Control and 
  Reliability in Distributed Systems</i>, Bharat K. Bhargava, editor,
  Van Nostrand Reinhold (1987) 348-369.<BR>
<A HREF = "the-byz-generals.pdf">PDF</A>
<BR><FONT SIZE = -2>All copyrights reserved by Van Nostrand Reinhold 1987.</FONT><HR>


I have only a vague memory of this paper.&nbsp; I believe Bhargava asked me
to write a chapter about the results in <A HREF = "#reaching">[41]</A> and
<A HREF = "#byz">[46]</A>.&nbsp; I was probably too lazy and asked Dolev to write a
chapter that combined his more recent results on connectivity
requirements with our original results.&nbsp; I would guess that he did all
the work, though I must have at least read and approved of what he
wrote.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "formal-basis"><B>A Formal Basis for the Specification of 
   Concurrent Systems</B><BR>In <i>Distributed Operating Systems: Theory and
   Practice</i>, Paker, Banatre and Bozyigit, editors,
   Springer-Verlag (1987), 1-46.<BR>
<A HREF = "formal-basis.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "formal-basis.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "formal-basis.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1987 by Springer-Verlag.</FONT><HR>


This paper describes the transition axiom method I introduced in
<A HREF = "#spec">[50]</A>.&nbsp; It was written for a NATO Advanced Study Institute
that took place in Turkey in August, 1986, and contains little that
was new.&nbsp;



<BR>&nbsp;<P><LI> <A NAME = "fast-mutex"><B>A Fast Mutual Exclusion Algorithm</B><BR><i>ACM
   Transactions on Computer Systems 5</i>, 1 (February 1987), 1-11.&nbsp; Also
   appeared as SRC Research Report&nbsp;7.<BR>
<A HREF = "fast-mutex.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "fast-mutex.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "fast-mutex.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1987 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


Soon after I arrived at SRC, I was approached by some people at WRL
(Digital's Western Research Laboratory) who were building a
multiprocessor computer.&nbsp; They wanted to avoid having to add
synchronization instructions, so they wanted to know how efficiently
mutual exclusion could be implemented with just read and write
instructions.&nbsp; They figured that, with properly designed programs,
contention for a critical section should be rare, so they were
interested in efficiency in the absence of contention.&nbsp; I deduced the
lower bound on the number of operations required and the optimal
algorithm described in this paper.&nbsp; They decided that it was too slow,
so they implemented a test-and-set instruction.&nbsp;

<p>
I find it remarkable that, 20 years after Dijkstra first posed the
mutual exclusion problem, no one had thought of trying to find
solutions that were fast in the absence of contention.&nbsp; This
illustrates why I like working in industry: the most interesting
theoretical problems come from implementing real systems.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "derivation-simple"><B>Derivation of a Simple Synchronization
   Algorithm</B><BR>Rejected by <i>Information Processing Letters</i> 
              (February 1987).<BR>
<A HREF = "derivation-simple.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "derivation-simple.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "derivation-simple.pdf">PDF</A><HR>


Chuck Thacker posed a little synchronization problem to me, which I
solved with Jim Saxe's help.&nbsp; At that time, deriving concurrent
algorithms was the fashion--the idea that you discover the algorithm
by some form of black magic and then verify it was considered
pass&#233;.&nbsp; So, I decided to see if I could have derived
the algorithm from approved general principles.&nbsp; I discovered that I
could--at least, informally--and that this informal derivation
seemed to capture the thought process that led me to the solution in
the first place.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "distributed-system"><B>Distribution</B><BR>Email message sent to 
a DEC SRC bulletin board at 12:23:29 PDT on 28 May 87.<BR>
<A HREF = "distributed-system.txt">Text File</A><HR>


This message is the source of the following observation, which has 
been quoted (and misquoted) rather widely:
 <blockquote><P> 
A distributed system is one in which the failure of a computer you
didn't even know existed can render your own computer unusable.&nbsp;
 </P></blockquote>

<BR>&nbsp;<P><LI> <A NAME = "document-production"><B>Document Production: Visual or
   Logical?</B><BR><i>Notices of the American Mathematical Society</i> (June
   1987), 621-624.<BR>
<A HREF = "document-production.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "document-production.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "document-production.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1987 by the American Mathematical Society.</FONT><HR>


Richard Palais ran a column on mathematical typesetting in the
<i>AMS Notices</i>, and he invited me to be guest columnist.&nbsp; This is
what I wrote--a short exposition of my ideas about producing
mathematical documents.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "synchronizing-time-servers"><B>Synchronizing Time Servers</B><BR>SRC
   Research Report&nbsp;18 (June 1987).<BR>
<A HREF = "synchronizing-time-servers.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "synchronizing-time-servers.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "synchronizing-time-servers.pdf">PDF</A><HR>
  

When I joined DEC in 1985, they were the world leader in networking.&nbsp;
Using their VMS operating system, I could type a simple <i>copy</i>
command to a computer in California, specifying a file and machine
name, to copy a file from a computer in Massachusetts.&nbsp; Even today, I
can't copy a file from Massachusetts to California nearly as easily
with Unix or Windows.&nbsp;

<p>
The people responsible for DEC's network systems were the Network and
Communications group (NAC).&nbsp; Around 1987, NAC asked for my help in
designing a network time service.&nbsp; I decided that there were two
somewhat conflicting requirements for a time service: delivering the
correct time, and keeping the clocks on different computers closely
synchronized.&nbsp; This paper describes the algorithms I devised for doing
both.&nbsp; 

<p> 

I withdrew the paper because Tim Mann observed that the properties I
proved about the algorithms were weaker than the ones needed to make
them interesting.&nbsp; The major problem is that the algorithms were
designed to guarantee both a bound epsilon on the
synchronization of each clock with a source of correct time and an
independent bound delta on the synchronization between
any two clocks that could be made much smaller than
epsilon.&nbsp; Mann observed that the bound I proved on
delta was not the strong one independent of
epsilon that I had intended to prove.&nbsp; We believe
that the algorithms do satisfy the necessary stronger properties, and
Mann and I began rewriting the paper with the stronger results.&nbsp; But
that paper is still only partly written and is unlikely ever to see
the light of day.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "control"><B>Control Predicates Are Better than Dummy Variables for
   Representing Program Control</B><BR><i>ACM Transactions on Programming
   Languages and Systems 10</i>, 2 (April 1988), 267-281.&nbsp; Also appeared
   as SRC Research Report&nbsp;11.<BR>
<A HREF = "control.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "control.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "control.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1988 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


This paper describes an example I came across in which the explicit
control predicates introduced in <A HREF = "#liveness">[47]</A> lead to a simpler
proof than do dummy variables.&nbsp; This example served as an excuse.&nbsp; The
real reason for publishing it was to lobby for the use of control
predicates.&nbsp; There used to be an incredible reluctance by theoretical
computer scientists to mention the control state of a program.&nbsp; When I
first described the work in <A HREF = "#hoare">[40]</A> to Albert Meyer, he
immediately got hung up on the control predicates.&nbsp; We spent an hour
arguing about them--I saying that they were necessary (as was first
proved by Susan in her thesis), and he saying that I must be doing
something wrong.&nbsp; I had the feeling that I was arguing logical
necessity against religious belief, and there's no way logic can
overcome religion.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "lamport-ewd1013"><B>"EWD 1013"</B><BR>Unpublished.&nbsp; Probably written
   around April, 1988.<BR>
<A HREF = "lamport-ewd1013.txt">Text File</A><HR>


Dijkstra's EWD 1013, <i>Position Paper on "Fairness"</i>, argues
that fairness is a meaningless requirement because it can't be
verified by observing a system for a finite length of time.&nbsp; The
weakness in this argument is revealed by observing that it applies
just as well to termination.&nbsp; To make the point, I wrote this note,
which claims to be an early draft of EWD 1013 titled <i>Position
Paper on "Termination"</i>.&nbsp; It is, of course, essentially the same as
EWD 1013 with <i>fairness</i> replaced by <i>termination</i>.&nbsp; Because
of other things that happened at that time, I was afraid that Dijkstra
might not take it in the spirit of good fun in which it was intended,
and that he might find it offensive.&nbsp; So, I never showed it to anyone
but a couple of friends.&nbsp; I think the passage of time has had enough
of a mellowing effect that no one will be offended any more by it.&nbsp; It
is now of more interest for the form than for the content.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "position"><B>Another Position Paper on Fairness</B>&nbsp; (with Fred 
   Schneider)<BR><i>Software Engineering Notes 13</i>, 3 (July, 1988) 1-2.<BR>
<A HREF = "position.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "position.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "position.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1988 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


This is a more traditional response to Dijkstra's EWD 1013 (see
<A HREF = "#lamport-ewd1013">[79]</A>).&nbsp; We point out that Dijkstra's same argument
can be applied to show that termination is a meaningless requirement
because it can't be refuted by looking at a program for a finite
length of time.&nbsp; The real argument in favor of fairness, which we
didn't mention, is that it is a useful concept when reasoning about
concurrent systems.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "welch-lattice"><B>A Lattice-Structured Proof of a Minimum Spanning 
   Tree Algorithm</B>&nbsp; (with Jennifer Welch and Nancy Lynch)<BR><i>Proceedings 
   of the Seventh Annual ACM Symposium on Principles of 
   Distributed Computing</i> (August, 1988).<BR>
<A HREF = "welch-lattice.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1988 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


In 1983, Gallager, Humblet, and Spira published a distributed
algorithm for computing a minimum spanning tree.&nbsp; For several years, I
regarded it as a benchmark problem for verifying concurrent
algorithms.&nbsp; A couple of times, I attempted to write an invariance
proof, but the invariant became so complicated that I gave up.&nbsp; On a
visit to M.I.T., I described the problem to Nancy Lynch, and she
became interested in it too.&nbsp; I don't remember exactly how it
happened, but we came up with the idea of decomposing the proof not as
a simple hierarchy of refinements, but as a lattice of refinements.&nbsp;
Being an efficient academic, Lynch got Jennifer Welch to do the work
of actually writing the proof as part of her Ph. D. thesis.&nbsp;
This paper is the conference version, written mostly by her.&nbsp;

<p> 

There were three proofs of the minimum spanning-tree algorithm
presented at PODC that year: ours, one by Willem-Paul de Roever and
his student Frank Stomp, and the third by Eli Gafni and his student
Ching-Tsun Chou.&nbsp; Each paper used a different proof method.&nbsp; I
thought that the best of the three was the one by Gafni and
Chou--not because their proof method was better, but because they
understood the algorithm better and used their understanding to
simplify the proof.&nbsp; If they had tried to formalize their proof, it
would have turned into a standard invariance proof.&nbsp; Indeed, Chou
eventually wrote such a formal invariance proof in his doctoral
thesis.&nbsp;

<p>
The Gallager, Humblet, and Spira algorithm is complicated and its
correctness is quite subtle.&nbsp; (Lynch tells me that, when she lectured
on its proof, Gallager had to ask her why it works in a certain case.)&nbsp;
There doesn't seem to be any substitute for a standard invariance
proof for this kind of algorithm.&nbsp; Decomposing the proof the way we
did seemed like a good idea at the time, but in fact, it just added
extra work.&nbsp; (See <A HREF = "#lamport-composition">[125]</A> for a further
discussion of this.)&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "simple-approach"><B>A Simple Approach to Specifying Concurrent
   Systems</B><BR><i>Communications of the ACM 32</i>, 1 (January 1989),
   32-45.&nbsp; Also appeared as SRC Research Report&nbsp;15.<BR>
<A HREF = "simple-approach.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "simple-approach.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "simple-approach.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1989 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


This is a "popular" account of the transition-axiom method that I
introduced in <A HREF = "#spec">[50]</A>.&nbsp; To make the ideas more accessible, I
wrote it in a question-answer style that I copied from the dialogues
of Galileo.&nbsp; The writing in this paper may be the best I've ever done.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "pretending"><B>Pretending Atomicity</B>&nbsp; (with Fred Schneider)<BR>SRC 
   Research Report&nbsp;44 (May 1989).<BR>
<A HREF = "pretending.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "pretending.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "pretending.pdf">PDF</A><HR>
  

Reasoning about concurrent systems is simpler if they have fewer
separate atomic actions.&nbsp; To simplify reasoning about systems, we'd
like to be able to combine multiple small atomic actions into a single
large one.&nbsp; This process is called <i>reduction</i>.&nbsp; This paper
contains a reduction theorem for multiprocess programs.&nbsp; It was
accepted for publication, subject to minor revisions, in <i>ACM
Transactions on Programming Languages and Systems</i>.&nbsp; However, after
writing it, I invented TLA, which enabled me to devise a stronger and
more elegant reduction theorem.&nbsp; Schneider and I began to revise the
paper in terms of TLA.&nbsp; We were planning to present a
weaker, simpler version of the TLA reduction theorem that essentially
covered the situations considered in this report.&nbsp; However, we never
finished that paper.&nbsp; A more general TLA reduction theorem was finally
published in <A HREF = "#cohen-tlareduction">[124]</A>.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "abadi-realizable"><B>Realizable and Unrealizable Specifications of
   Reactive Systems</B>&nbsp; (with Mart&#237;n Abadi and
   Pierre Wolper)<BR><i>Automata, Languages and Programming</i>,
   Springer-Verlag (July 1989) 1-17.<BR>
<A HREF = "abadi-realizable.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "abadi-realizable.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "abadi-realizable.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1989 by Springer-Verlag.</FONT><HR>


Abadi and I came upon the concept of realizability in
<A HREF = "#abadi-composing">[98]</A>.&nbsp; Several other people independently came up
with the idea at around the same time, including Wolper.&nbsp; Abadi and
Wolper worked together to combine our results and his into a single
paper.&nbsp; Abadi recollects that the section of the paper dealing with
the general case was mostly ours, and Wolper mostly developed the
finite case, including the algorithms.&nbsp; He remembers adopting the term
"realizability" from realizability in intuitionistic logic, and
thinking of the relation with infinite games after seeing an article
about such games in descriptive set theory in the Journal of Symbolic
Logic.&nbsp; As I recall, I wasn't very involved in the writing of this
paper.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "old-tla-src"><B>A Temporal Logic of Actions</B><BR>SRC Research 
    Report&nbsp;57 (April 1990).<BR>
<A HREF = "old-tla-src.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "old-tla-src.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "old-tla-src.pdf">PDF</A><HR>


This was my first attempt at TLA, and I didn't get it quite right.&nbsp;
It is superseded by <A HREF = "#lamport-actions">[103]</A>.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-win"><B><i>win</i> and <i>sin</i>: Predicate Transformers
   for Concurrency</B><BR><i>ACM Transactions on Programming Languages and Systems 12</i>, 3 (July 1990) 396-428. Also
  appeared as SRC Research Report&nbsp;17 (May 1987).<BR>
<A HREF = "lamport-win.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-win.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-win.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1990 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


I had long been fascinated with algorithms that, like the bakery
algorithm of <A HREF = "#bakery">[12]</A>, do not assume atomicity of their
individual operations.&nbsp; I devised the formalism first published in
<A HREF = "#new-approach">[33]</A> for writing behavioral proofs of such
algorithms.&nbsp; I had also long been a believer in invariance proofs,
which required that the algorithm be represented in terms of atomic
actions.&nbsp; An assertional proof of the bakery algorithm required
modeling its nonatomic operations in terms of multiple atomic
actions--as I did in <A HREF = "#bakery">[12]</A>.&nbsp; However, it's easy to
introduce tacit assumptions with such modeling.&nbsp; Indeed, sometime
during the early 80s I realized that the bakery algorithm required an
assumption about how a certain operation is implemented that I had
never explicitly stated, and that was not apparent in any of the
proofs I had written.&nbsp; This paper introduces a method of writing
formal assertional proofs of algorithms directly in terms of their
nonatomic operations.&nbsp; It gives a proof of the bakery algorithm that
explicitly reveals the needed assumption.&nbsp; However, I find the method
very difficult to use.&nbsp; With practice, perhaps one could become facile
enough with it to make it practical.&nbsp; However, there don't seem to be
enough algorithms requiring reasoning about nonatomic actions for
anyone to acquire that facility.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-theorem"><B>A Theorem on Atomicity in Distributed
   Algorithms</B><BR><i>Distributed Computing 4</i>, 2 (1990), 59-68.&nbsp;
   Also appeared as SRC Research Report&nbsp;28.<BR>
<A HREF = "lamport-theorem.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-theorem.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-theorem.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1990 by Springer-Verlag.</FONT><HR>
 

This paper gives a reduction theorem for distributed algorithms (see
the discussion of <A HREF = "#pretending">[83]</A>).&nbsp; It includes what I believe to
be the first reduction result for liveness properties.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-chapter"><B>Distributed Computing: Models and
   Methods</B>&nbsp; (with Nancy Lynch)<BR><i>Handbook of Theoretical Computer
   Science, Volume&nbsp;B: Formal Models and Semantics</i>, 
  Jan van Leeuwen,
   editor, Elsevier (1990), 1157-1199.<BR>
<A HREF = "lamport-chapter.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-chapter.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-chapter.pdf">PDF</A>
<BR><FONT SIZE = -2>All copyrights reserved by Elsevier Science 1990.</FONT><HR>


Jan van Leeuwen asked me to write a chapter on distributed systems for
this handbook.&nbsp; I realized that I wasn't familiar enough with the
literature on distributed algorithms to write it by myself, so I asked
Nancy Lynch to help.&nbsp; I also observed that there was no chapter on
assertional verification of concurrent algorithms.&nbsp; (That was probably
due to the handbook's geographical origins, since process algebra
rules in Europe.)&nbsp; So I included a major section on proof methods.&nbsp; As
I recall, I wrote most of the first three sections and Lynch wrote the
fourth section on algorithms pretty much by herself.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "lamport-completion"><B>A Completeness Theorem for TLA</B><BR>Unpublished
(October, 1990).<BR>
<A HREF = "lamport-completion.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-completion.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-completion.pdf">PDF</A><HR>


This is the beginning of  a note that states and proves a
relative completeness result for the axioms of TLA in the absence of
temporal existential quantification (variable hiding).&nbsp; (The
ancient LaTeX macros used to format the note only work on the first
part. A text version of the complete note and of all other TLA notes
are available  <A
 HREF="/tla/notes.html">here</A>.&nbsp;
There are undoubtedly errors in the proof, but I think they're minor.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-concurrent-clocks"><B>The Concurrent Reading and
   Writing of Clocks</B><BR><i>ACM Transactions on Computer Systems 8</i>, 4
   (November 1990), 305-310.&nbsp; Also appeared as SRC Research 
   Report&nbsp;27.<BR>
<A HREF = "lamport-concurrent-clocks.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-concurrent-clocks.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-concurrent-clocks.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1990 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


This paper uses the results from <A HREF = "#rd-wr">[25]</A> to derive a couple of
algorithms for reading and writing multi-word clocks.&nbsp; These
algorithms are in the same vein as the ones in <A HREF = "#rd-wr">[25]</A>,
involving reading and writing multi-digit numbers in opposite
directions.&nbsp; In fact, I think I knew the algorithms when I wrote
<A HREF = "#rd-wr">[25]</A>.&nbsp; When the problem of reading and writing a two-word
clock arose in a system being built at SRC, I was surprised to
discover that the solution wasn't in <A HREF = "#rd-wr">[25]</A>.&nbsp; I don't know why
it wasn't, but I welcomed the opportunity to publish a paper reminding
people of the earlier results.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "lamport-mutual-solved"><B>The Mutual Exclusion Problem Has Been
   Solved</B><BR><i>Communications of the ACM 34</i>, 1 (January 1991), 110.<BR>
<A HREF = "lamport-mutual-solved.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-mutual-solved.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-mutual-solved.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1991 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


In 1990, <i>CACM</i> published one of their self-assessment
procedures, this time on concurrent programming.&nbsp; The "correct"
answer to one of the questions implied that mutual exclusion can be
implemented only using atomic operations that are themselves
implemented with lower-level mutual exclusion.&nbsp; It seemed appropriate
to point out that this was wrong, and that I had actually solved the
mutual exclusion problem 16 years earlier in
<A HREF = "#bakery">[12]</A>--ironically, an article in <i>CACM</i>.&nbsp;
So, I submitted this short note to that effect.&nbsp; The quotation from
Samuel Johnson at the end is one that Bob Taylor likes very much and
taught to everyone at SRC when he was lab director.&nbsp;

<p>
The original version, which I no longer have, quoted all the sources
cited in support of the "correct" answer, showing how all those
experts in the field had no idea that the mutual exclusion problem had
been solved.&nbsp; However, the editor of <i>CACM</i> took it upon
himself to handle my submission personally.&nbsp; He insisted that all this
material be deleted, along with the accompanying sarcasm.&nbsp; Although I
didn't like this editorial bowdlerization, I didn't feel like
fighting.&nbsp;

<p> 
I was appalled at how this note finally appeared.&nbsp; I have never seen a
published article so effectively hidden from the reader.&nbsp; I defy
anyone to take that issue of <i>CACM</i> and find the note without
knowing the page number.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "abadi-existence"><B>The Existence of Refinement 
  Mappings</B>&nbsp; (with Mart&#237;n 
  Abadi)<BR><i>Theoretical Computer Science 82</i>, 2 (May
  1991), 253-284.&nbsp; (An abridged version appeared in <i>Proceedings of
   the Third Annual Logic In Computer Science Conference</i> (July 1988).)&nbsp;
  Also appeared as SRC Research Report&nbsp;29.<BR>
<A HREF = "abadi-existence.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "abadi-existence.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "abadi-existence.pdf">PDF</A>
<BR><FONT SIZE = -2>All copyrights reserved by Elsevier Science 1991.</FONT><HR>


The method of proving that one specification implements another by
using a refinement mapping was well-established by the mid-80s.&nbsp; (It
is clearly described in <A HREF = "#what-good">[54]</A>, and it also appears in
<A HREF = "#spec">[50]</A>.)&nbsp; It was known that constructing the refinement mapping
might require adding history variables to the implementation.&nbsp; Indeed,
Lam and Shankar essentially constructed all their refinement mappings
with history variables.&nbsp; Jim Saxe discovered a simple example showing
that history variables weren't enough.&nbsp; To handle that example, he
devised a more complicated refinement-mapping rule.&nbsp; I realized that I
could eliminate that complicated rule, and use ordinary refinement, by
introducing a new kind of dummy variable that I called a prophecy
variable.&nbsp; A prophecy variable is very much like a history variable,
except it predicts the future instead of remembering the past.&nbsp; (Nancy
Lynch later rediscovered Saxe's rule and used it to "simplify"
refinement proofs by eliminating prophecy variables.)&nbsp; I then
remembered a problematic example by Herlihy and Wing in their classic
paper <i>Linearizability: A Correctness Condition for Concurrent
Objects</i> that could be handled with prophecy variables.&nbsp;

<p> This paper was my first collaboration with Abadi.&nbsp; Here's
my recollection of how it was written.&nbsp; I had a hunch that history and
prophecy variables were all one needed.&nbsp; Abadi had recently joined
SRC, and this seemed like a fine opportunity to interest him
in the things I was working on.&nbsp; So I described my hunch to him and
suggested that he look into proving it.&nbsp; He came back in a few weeks
with the results described in the paper.&nbsp; My hunch was right, except
that there were hypotheses needed that I hadn't suspected.&nbsp; Abadi,
however, recalls my having had a clearer picture of the final theorem,
and that we worked out some of the details together when writing the
final proof.&nbsp;

<p> 
I had just developed the structured proof style described in
<A HREF = "#lamport-how-to-write">[102]</A>, so I insisted that we write our proofs
in this style, which meant rewriting Abadi's original proofs.&nbsp; In the
process, we discovered a number of minor errors in the proofs, but no
errors in the results.&nbsp;

<p>
This paper won the LICS 1988 Test of Time Award (awarded in 2008).&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "abadi-preserving"><B>Preserving Liveness: Comments on `Safety and
   Liveness from a Methodological Point of
   View'</B>&nbsp; (with Mart&#237;n Abadi et
   al.)<BR><i>Information Processing Letters 40</i>, 3 (November 1991),
   141-142.<BR>
<A HREF = "abadi-preserving.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "abadi-preserving.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "abadi-preserving.pdf">PDF</A>
<BR><FONT SIZE = -2>All copyrights reserved by Elsevier Science 1991.</FONT><HR>


This is a very short article--the list of authors takes up almost as
much space as the text.&nbsp; In a note published in <i>IPL</i>,
Dederichs and Weber rediscovered the concept of non-machine-closed
specifications.&nbsp; We observed here that their reaction to those
specifications was naive.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-critique"><B>Critique of the Lake Arrowhead
   Three</B><BR><i>Distributed Computing 6</i>, 1 (1992), 65-71.<BR>
<A HREF = "lamport-critique.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-critique.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-critique.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1992 by Springer-Verlag.</FONT><HR>


For a number of years, I was a member of a committee that planned an
annual workshop at Lake Arrowhead, in southern California.&nbsp; I was
finally pressured into organizing a workshop myself.&nbsp; I got Brent
Hailpern to be chairman of a workshop on specification and
verification of concurrent systems.&nbsp; A large part of the conference
was devoted to a challenge problem of specifying sequential
consistency.&nbsp; This was a problem that, at the time, I found difficult.&nbsp;
(I later learned how to write the simple, elegant specification that
appears in <A HREF = "#ladkin-gerth">[127]</A>.)&nbsp;

<p> Among the presentations at the workshop session on the
challenge problem, there were only two serious attempts at solving the
problem.&nbsp; (As an organizer, I felt that I shouldn't present my own
solution.)&nbsp; After a long period of review and revision, these two and
a third, subsequently-written solution, appeared in a special issue of
<i>Distributed Computing</i>.&nbsp; This note is a critique of the three
solutions that I wrote for the special issue.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "lamport-reduction-tlanote"><B>The Reduction 
   Theorem</B><BR>Unpublished (April, 1992).<BR>
<A HREF = "lamport-reduction-tlanote.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-reduction-tlanote.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-reduction-tlanote.pdf">PDF</A><HR>


This note states and proves a TLA reduction theorem.&nbsp; See the
discussion of <A HREF = "#cohen-tlareduction">[124]</A>.&nbsp;
Text versions of this and all other TLA notes are available 
<A
HREF="/tla/notes.html">here</A>.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "engberg-mechanical"><B>Mechanical Verification of Concurrent Systems
   with TLA</B>&nbsp; (with Urban Engberg and Peter
   Gr&#248;nning)<BR><i>Computer-Aided Verification</i>,
   G.&nbsp;v.&nbsp;Bochmann and D. K. Probst editors.&nbsp; (Proceedings of the Fourth
   International Conference, CAV'92.)&nbsp; Lecture Notes in Computer Science,
   number 663, Springer-Verlag, (June, 1992) 44-55.<BR>
<A HREF = "engberg-mechanical.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "engberg-mechanical.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "engberg-mechanical.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1992 by Springer-Verlag.</FONT><HR>


When I developed TLA, I realized that, for the first time, I had a
formalism that really was completely formal--so formal that
mechanically checking TLA proofs should be straightforward.&nbsp; Working
out a tiny example (the specification and trivial implementation of
mutual exclusion) using the LP theorem prover, I confirmed that this
was the case.&nbsp; I used LP mainly because we had LP experts at
SRC--namely, Jim Horning and Jim Saxe.&nbsp;

<p> 
My tiny example convinced me that we want to reason in
TLA, not in LP.&nbsp; To do this, we need to translate a TLA
proof into the language of the theorem prover.&nbsp; The user should write
the proof in the hierarchical style of <A HREF = "#lamport-how-to-write">[102]</A>,
and the prover should check each step.&nbsp; One of the advantages of this
approach turned out to be that it allows separate translations for the
action reasoning and temporal reasoning.&nbsp; This is important because
 about 95%
 of a proof consists of action reasoning, and these proofs are much
simpler if done with a special translation than in the same
translation that handles temporal formulas.&nbsp; (In action reasoning,
<i>x</i> and <i>x</i>' are two completely separate
variables; <i>x</i>' is handled no differently than the
variable <i>y</i>.)&nbsp; So, I invited Urban Engberg and Peter
Gr&#248;nning, who were then graduate students in
Denmark, to SRC for a year to design and implement such a system.&nbsp; The
result of that effort is described in this paper.&nbsp; For his doctoral
research, Engberg later developed the system into one he called TLP.&nbsp;

<p>
Georges Gonthier demonstrated how successful this system was in his
mechanical verification of the concurrent garbage collector developed
by Damien Doligez and hand-proved in his thesis.&nbsp; Gonthier estimated
that using TLP instead of working directly in LP reduced the amount of
time it took him to do the proof by about a factor of five.&nbsp;
His proof is reported in:
<blockquote><P>
 Georges Gonthier, <i>Verifying the Safety of a
Practical Concurrent Garbage Collector</i>, in Rajeev Alur, Thomas A.&nbsp;
Henzinger (Ed.): <i>Computer Aided Verification, 8th International
Conference, CAV '96</i>.&nbsp; Lecture Notes in Computer Science, 
 Vol. 1102, Springer, 1996, 462-465.&nbsp;
 </P></blockquote> 
TLP's input language was essentially a very restricted subset of
TLA+ (described in
<A HREF = "#lamport-spec-tla-plus">[128]</A>)--a language that did not
exist when TLP was implemented.&nbsp; 
I regarded TLP as a proof of
concept and I did not use it after it was built.&nbsp;
Around 2006, 
work began on a theorem prover for real TLA+
specifications.&nbsp; It led to
  <a href="httpss://tla.msr-inria.inria.fr/tlaps/content/Home.html">
TLAPS, the TLA+ proof system</a>.&nbsp; TLAPS uses
several back-end theorem provers, including Isabelle, Zenon, and SMT
solvers.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "reconfiguration-patent"><B>Reconfiguration system and method for high-speed mesh
connected local area network</B><BR>United State Patent number 5,138,615
(August 11, 1992).<BR><A HREF="https://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&d=PALL&p=1&u=%2Fnetahtml%2FPTO%2Fsrchnum.htm&r=1&f=G&l=50&s1=5,138,615.PN.&OS=PN/5,138,615&RS=PN/5,138,615">Available 
On-Line</A><HR>


This is one of 19 patents for which I was an inventor.&nbsp; I am including
it here because it contains an interesting algorithm that I never
published.&nbsp; It's an enhancement of an algorithm at the heart of the
algorithm proved correct in <A HREF = "#dist">[48]</A> for maintaining routing
tables in a distributed message-passing system.&nbsp; That algorithm
eventually converges to correct routing tables.&nbsp; However, it does not
inform processes when it has converged.&nbsp; The computation of the parts
of the tables that say how to send messages to one particular node is
a distributed version of a classic algorithm for computing a
minimal-depth spanning tree.&nbsp; The patent covers an enhancement of that
spanning tree algorithm that informs processes when the computation
has terminated.&nbsp; I don't know if anyone else has since discovered and
published that algorithm.&nbsp; The inventors listed on the patent are Tom
Rodeheffer, Mani Chandy, and me.&nbsp; However, I believe that Mani and I
were the ones who invented the algorithm.&nbsp; Tom was probably the sole
inventor of the other ideas contained in the patent.&nbsp;



<BR>&nbsp;<P><LI> <A NAME = "abadi-composing"><B>Composing
   Specifications</B>&nbsp; (with Mart&#237;n
   Abadi)<BR><i>ACM Transactions on Programming Languages and Systems 15</i>, 1 (January 1993), 73-132.&nbsp; Also appeared
  as SRC Research Report&nbsp;66.&nbsp; A preliminary version 
  appeared in <i>Stepwise Refinement of Distributed Systems</i>,
 J. W. de Bakker, W.-P. de Roever, and G. Rozenberg editors,
  Springer-Verlag Lecture Notes in Computer Science 
   Volume 430 (1989), 1-41..<BR>
<A HREF = "abadi-composing.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "abadi-composing.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "abadi-composing.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1993 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


Since the late 80s, I had vague concerns about separating the
specification of a system from requirements on the environment.&nbsp; The
ability to write specifications as mathematical formulas (first with
temporal logic and then, for practical specifications, with TLA)
provides an answer.&nbsp; The specification is simply <i>E</i> implies
<i>M</i>, where <i>E</i> specifies what we assume of the environment
and <i>M</i> specifies what the system guarantees.&nbsp; This specification
allows behaviors in which the system violates its guarantee and the
environment later violates its assumption--behaviors that no correct
implementation could allow.&nbsp; So, we defined the notion of the
realizable part of a specification and took as the system
specification the realizable part of <i>E</i> implies
<i>M</i>.&nbsp; We later decided that introducing an explicit
<i>realizable-part</i> operator was a mistake, and that it was better
to replace implication with a temporal <i>while</i> operator that made
the specifications realizable.&nbsp; That's the approach we took in
<A HREF = "#abadi-conjoining">[113]</A>, which supersedes this paper.&nbsp;


<p>
This is the second paper in which we used structured proofs, the first
being <A HREF = "#abadi-existence">[92]</A>.&nbsp; In this case, structured proofs were
essential to getting the results right.&nbsp; We found reasoning about
realizability to be quite tricky, and on several occasions we
convinced ourselves of incorrect results, finding the errors only by
trying to write structured proofs.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "kurshan-multiplier"><B>Verification of a Multiplier: 64 Bits and
   Beyond</B>&nbsp; (with R. P. Kurshan)<BR><i>Computer-Aided Verification</i>, Costas
   Courcoubetis, editor.&nbsp; (Proceedings of the Fifth International
   Conference, CAV'93.)&nbsp; Lecture Notes in Computer Science, number 697,
   Springer-Verlag (June, 1993), 166-179.<BR>
<A HREF = "kurshan-multiplier.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "kurshan-multiplier.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "kurshan-multiplier.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1993 by Springer-Verlag.</FONT><HR>


As I observed in <A HREF = "#lamport-composition">[125]</A>, verifying a system by
first decomposing it into separate subsystems can't reduce the size of
a proof and usually increases it.&nbsp; However, such a decomposition can
reduce the amount of effort if it allows much of the resulting proof
to be done automatically by a model checker.&nbsp; This paper shows how the
decomposition theorem of <A HREF = "#abadi-conjoining">[113]</A> can be used to
decompose a hardware component (in this case, a multiplier) that is
too large to be verified by model checking alone.&nbsp; Here is Kurshan's
recollection of how this paper came to be.&nbsp; My CAV'92 talk was mostly
about <A HREF = "#lamport-how-to-write">[102]</A>, and the "Larch support"
refers to <A HREF = "#engberg-mechanical">[96]</A>

<P><BLOCKQUOTE>

I cornered you after your invited address at CAV92.&nbsp; At CAV, you
talked about indenting (and TLA, and its Larch support).&nbsp; I challenged
you with a matter I had been thinking about since at least 1990, the
year of the first CAV.&nbsp; In the preface to the CAV90
proceedings, I stated as a paramount challenge to the CAV community,
to create a beneficial interface between automated theorem proving,
and model checking.&nbsp;

<P>

I asked you if you thought that linking TLA/Larch with S/R (which
should be simple to do on account of their very close syntax and
semantics for finite-state models), could be useful.&nbsp; I suggested the
(artificial) problem of verifying a multiplier constructed from
iterated stages of a very complex 8x8 multiplier.&nbsp;
The 8x8 multiplier would be too complex to verify
handily with a theorem prover.&nbsp; A (say) 64x64
multiplier could be built from the 8x8 one.&nbsp; We'd
use the model checker (cospan) to verify the 8x8,
and Larch/TLA to verify the induction step.&nbsp; You liked the idea, and
we did it, you working with Urban and I working with Mark
[Foissoitte].&nbsp;

<P>
Sadly, with the interface in place, I was unable to come up with a
non-artificial feasible application.&nbsp; To this day, although there have
been a succession of such interfaces built (I believe ours was the
first), none has really demonstrated a benefit on a real application.&nbsp;
The (revised) challenge is to find an application in which the
combination finds a bug faster than either one could by itself.&nbsp;
</P></BLOCKQUOTE>


<BR>&nbsp;<P><LI> <A NAME = "lamport-verification"><B>Verification and Specification of
   Concurrent Programs</B><BR><i>A Decade of Concurrency: Reflections and
   Perspectives</i>, J. W. de Bakker, W.-P. de Roever, and G. Rozenberg
   editors.&nbsp; Lecture Notes in Computer Science, number 803,
   Springer-Verlag, (June, 1993) 347-374.<BR>
<A HREF = "lamport-verification.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-verification.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-verification.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1993 by Springer-Verlag.</FONT><HR>


In keeping with the theme of the workshop, this paper provides a
brief, biased overview of 18 years of verifying and specifying
concurrent systems, along with an introduction to TLA.&nbsp;
Looking at it almost 10 years later, I find it a rather nice read.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-hybrid"><B>Hybrid Systems in
   TLA+</B><BR><i>Hybrid Systems</i>, Robert L. Grossman, Anil
   Nerode, Hans Rischel, and Anders P. Ravn, editors.&nbsp; Lecture Notes in
   Computer Science, number 736, Springer-Verlag (1993), 77-102.<BR>
<A HREF = "lamport-hybrid.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-hybrid.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-hybrid.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1993 by Springer-Verlag.</FONT><HR>


In the early 90s, hybrid systems became a fashionable topic in formal
methods.&nbsp; Theoreticians typically respond to a new problem domain by
inventing new formalisms.&nbsp; Physicists don't have to revise the theory
of differential equations every time they study a new kind of system,
and computer scientists shouldn't have to change their formalisms when
they encounter a new kind of system.&nbsp; Abadi and I showed in
<A HREF = "#lamport-old-fashioned">[107]</A> that TLA can handle real-time
specifications by simply introducing a variable to represent the
current time.&nbsp; It's just as obvious that it can handle hybrid systems
by introducing variables to represent other physical quantities.&nbsp; It
is often necessary to demonstrate the obvious to people, and the
opportunity to do this arose when there was a workshop in Denmark
devoted to a toy problem of specifying a simple gas burner and proving
the correctness of a simple implementation.&nbsp; I was unable to attend
the workshop, but I did write this TLA+ solution.&nbsp; (The
version of TLA+ used here is slightly different from the
more current version that is described in
<A HREF = "#lamport-spec-tla-plus">[128]</A>.)&nbsp;

<p>

The correctness conditions given in the problem statement included an
<i>ad hoc</i> set of rules about how long the gas could be on if the
flame was off.&nbsp; The purpose of those conditions was obviously to
prevent a dangerous build-up of unburned gas.&nbsp; To demonstrate the
power of TLA+, and because it made the problem more fun,
I wrote a higher-level requirement stating that the concentration of
gas should be less than a certain value.&nbsp; Assuming that the
dissipation of unburned gas satisfied a simple differential equation,
I proved that their conditions implied my higher-level
specification--under suitable assumptions about the rate of diffusion
of the gas.&nbsp; This required, among other things, formally specifying
the Riemann integral, which took about 15 lines.&nbsp; I also sketched a
proof of the correctness of the next implementation level.&nbsp; All of
this was done completely in TLA+.&nbsp; The introduction to the
volume says that I
  "extend[ed] ... TLA+ with explicit variables
  that denote continuous states and clocks."
That, of course, is nonsense.&nbsp; Apparently, by their logic, you have
extended C if you write a C program with variables named <i>time</i>
and <i>flame</i> instead of <i>t</i> and <i>f</i>.&nbsp;



<BR>&nbsp;<P><LI> <A NAME = "lamport-how-to-write"><B>How to Write a Proof</B><BR><i>American
   Mathematical Monthly</i> 102, 7 (August-September 1995) 600-608.&nbsp; Also
   appeared in <i>Global Analysis in Modern Mathematics</i>, Karen
   Uhlenbeck, editor.&nbsp; Publish or Perish Press, Houston.&nbsp; Also appeared
   as SRC Research Report&nbsp;94.<BR>
<A HREF = "lamport-how-to-write.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-how-to-write.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-how-to-write.pdf">PDF</A><HR>


TLA gave me, for the first time, a formalism in which it was 
possible to write completely formal proofs without first having to add
an additional layer of formal semantics.&nbsp; I began writing proofs the
way I and all mathematicians and computer scientists had learned to
write them, using a sequence of lemmas whose proofs were a mixture of
prose and formulas.&nbsp; I quickly discovered that this approach collapsed
under the weight of the complexity of any nontrivial proof.&nbsp; I became
lost in a maze of details, and couldn't keep track of what had and had
not been proved at any point.&nbsp; Programmers learned long ago that the
way to handle complexity is with hierarchical structuring.&nbsp; So, it was
quite natural to start structuring the proofs hierarchically, and I
soon developed a simple hierarchical proof style.&nbsp; It then occurred to
me that this structured proof style should be good for ordinary
mathematical proofs, not just for formal verification of systems.&nbsp;
Trying it out, I found that it was great.&nbsp; I now never write
old-fashioned unstructured proofs for myself, and use them only in
some papers for short proof sketches that are not meant to be
rigorous.&nbsp;

<p> 

I first presented these ideas in a talk at a celebration of the 60th
birthday of Richard Palais, my <i>de jure</i> thesis advisor,
collaborator, and friend.&nbsp; I was invited along with all of Palais'
former doctoral students, and I was the only non-mathematician who
gave a talk.&nbsp; (I believe all the other talks presented that day appear
among the articles in the volume edited by Uhlenbeck.)&nbsp; Lots of people
jumped on me for trying to take the fun out of mathematics.&nbsp; The
strength of their reaction indicates that I hit a nerve.&nbsp; Perhaps they
really do think it's fun having to recreate the proofs themselves if
they want to know whether a theorem in a published paper is actually
correct, and to have to struggle to figure out why a particular step
in the proof is supposed to hold.&nbsp; I republished the paper in the
<i>AMM Monthly</i> so it would reach a larger audience of
mathematicians.&nbsp; Maybe I should republish it again for computer
scientists.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-actions"><B>The Temporal Logic of Actions</B><BR><i>ACM Transactions on Programming Languages and Systems 16</i>, 3
   (May 1994), 872-923. Also appeared
   as SRC Research Report&nbsp;79.<BR>
<A HREF = "lamport-actions.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-actions.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-actions.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1994 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


This paper introduces TLA, which I now believe is the best general
formalism for describing and reasoning about concurrent systems.&nbsp; The
new idea in TLA is that one can use <i>actions</i>--formulas with
primed and unprimed variables--in temporal formulas.&nbsp; An action
describes a state-transition relation.&nbsp; For example, the action
<tt>x'=x+1</tt> means approximately the same thing as
the programming-language statement <tt>x := x+1</tt>.&nbsp;
However, the action is much simpler because it talks only about
<i>x</i> and says nothing about another variable <i>y</i>, while the
assignment statement may (or may not) assert that <i>y</i> doesn't
change.&nbsp; TLA allows you to write specifications essentially the same
way advocated in <A HREF = "#simple-approach">[82]</A>.&nbsp; However, the specification
becomes a single mathematical formula.&nbsp; This opens up a whole new
realm of possibilities.&nbsp; Among other things, it provides an elegant
way to formalize and systematize all the reasoning used in concurrent
system verification.&nbsp;

<p>
The moral of TLA is: if you're not writing a program, don't use a
programming language.&nbsp; Programming languages are complicated and have
many ugly properties because a program is input to a compiler that
must generate reasonably efficient code.&nbsp; If you're describing an
algorithm, not writing an actual program, you shouldn't burden
yourselves with those complications and ugly properties.&nbsp; The toy
concurrent programming languages with which computer scientists have
traditionally described algorithms are not as bad as real programming
languages, but they are still uglier and more complicated than they
need to be.&nbsp; Such a toy program is no closer to a real C or Java
program than is a TLA formula.&nbsp; And the TLA formula is a lot easier to
deal with mathematically than is a toy program.&nbsp; (Everything I say
about programming languages applies just as well to hardware
description languages.&nbsp; However, hardware designers are generally more
sensible than to try to use low-level hardware languages for
higher-level system descriptions.)&nbsp; Had I only realized this 20 years
ago!

<p> 

The first major step in getting beyond traditional programming
languages to describe concurrent algorithms was Misra and Chandy's
Unity.&nbsp; Unity simply eliminated the control state, so you just had a
single global state that you reasoned about with a single invariant.&nbsp;
You can structure the invariant any way you want; you're not
restricted by the particular programming constructs with which the
algorithm is described.&nbsp; The next step was TLA, which eliminated the
programming language and allowed you to write your algorithm directly
in mathematics.&nbsp; This provides a much more powerful and flexible way
of describing the next-state relation.&nbsp;

<p> 

An amusing footnote to this paper is that, after reading an earlier
draft, Simon Lam claimed that he deserved credit for the idea of
describing actions as formulas with primed and unprimed variables.&nbsp; A
similar notation for writing postconditions dates from the 70s, but
that's not the same as actually specifying the action in this way.&nbsp; I
had credited Rick Hehner's 1984 <i>CACM</i> article, but I figured
there were probably earlier instances.&nbsp; After a modest amount of
investigation, I found one earlier published use--in <A HREF = "#spec">[50]</A>.&nbsp;



<BR>&nbsp;<P><LI> <A NAME = "abadi-decomposing"><B>Decomposing Specifications of Concurrent
   Systems</B>&nbsp; (with Mart&#237;n
   Abadi)<BR><i>Programming Concepts, Methods and Calculi</i>,
   Ernst-R&#252;diger Olderog editor.&nbsp; (Proceedings of the
   IFIP TC2/WG2.1/WG2.2/WG2.3 Working Conference, Procomet&nbsp;'94, 
   San Miniato, Italy.)&nbsp; North-Holland, (1994) 327-340.<BR>
<A HREF = "abadi-decomposing.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "abadi-decomposing.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "abadi-decomposing.pdf">PDF</A>
<BR><FONT SIZE = -2>All copyrights reserved by Elsevier Science 1994.</FONT><HR>


See the discussion of <A HREF = "#abadi-conjoining">[113]</A>.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "abadi-open"><B>Open Systems in
   TLA</B>&nbsp; (with Mart&#237;n
   Abadi)<BR><i>Proceedings of the Thirteenth Annual ACM Symposium on
   Principles of Distributed Computing,</i> (August 1994) 81-90.<BR>
<A HREF = "abadi-open.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "abadi-open.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "abadi-open.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1994 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


See the discussion of <A HREF = "#abadi-conjoining">[113]</A>.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "tlz"><B>TLZ (Abstract)</B><BR><i>Z User's Workshop, Cambridge 1994</i>.&nbsp;
   J.P. Bowen and J.A. Hall (Eds.) 267-268.<BR>
<A HREF = "tlz.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "tlz.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "tlz.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1994 by Springer-Verlag.</FONT><HR>


Z is a formal specification language that describes a system by
writing actions--essentially the same kinds of actions that appear in
a TLA specification.&nbsp; It was developed by Mike Spivey and others at
Oxford for specifying sequential programs.&nbsp; When someone develops a
method for sequential programs, they usually think that it
will also handle concurrent programs--perhaps by adding an extra
feature or two.&nbsp; I had heard that this wasn't true of the Z
developers, and that they were smart enough to realize that Z did not
handle concurrency.&nbsp; Moreover, Z is based on mathematics not
programming languages, so it is a fairly nice language.&nbsp;

<p>
TLA assumes an underlying logic for writing actions.&nbsp; The next step
was obvious: devise a language for specifying concurrent systems that
extends Z with the operators of TLA.&nbsp; Equally obvious was
the name of such a language: TLZ.&nbsp;

<p>
In the Spring of 1991, I visited Oxford and gave a talk on TLA,
pointing out how naturally it could be combined with Z.&nbsp; The
idea was as popular as bacon and eggs at Yeshiva University.&nbsp; Tony
Hoare was at Oxford, and concurrency at Oxford meant CSP.&nbsp;
The Z community was interested only in combining Z with CSP--which is
about as natural as combining predicate logic with C++.&nbsp;

<p> 
A couple of years later, I was invited to give a talk at
the Z User's Meeting.&nbsp; I dusted off the TLZ idea and presented it at
the meeting.&nbsp; Again, I encountered a resounding lack of interest.&nbsp;

<p>
Had TLA been adopted by the Z community, it might have become a lot
more popular.&nbsp; On the other hand, not being attached to Z meant that I
didn't have to live with Z's drawbacks and was free to design a more
elegant language for specifying actions.&nbsp; The result was
TLA+, described in <A HREF = "#lamport-spec-tla-plus">[128]</A>.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-old-fashioned"><B>An Old-Fashioned Recipe for Real
   Time</B>&nbsp; (with Mart&#237;n Abadi)<BR><i>ACM Transactions on Programming Languages and Systems 16</i>, 5
   (September 1994) 1543-1571.&nbsp; Also appeared as SRC Research 
  Report&nbsp;91.&nbsp;
   A preliminary version appeared in
   <i>Real-Time: Theory in Practice</i>, J. W. de Bakker, C. Huizing, W.&nbsp;
   P. de Roever, and G. Rozenberg, editors (1992), Springer-Verlag,
   1-27.<BR>
<A HREF = "lamport-old-fashioned.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-old-fashioned.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-old-fashioned.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1994 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


As explained in the discussion of <A HREF = "#spec-and-proof">[51]</A>, it's been
clear for a long time that assertional methods for reasoning about
concurrent algorithms can easily handle real-time algorithms.&nbsp; Time
just becomes another variable.&nbsp; That hasn't stopped academics from
inventing new formalisms for handling time.&nbsp; (Model checking real-time
algorithms does raise new problems, since they are inherently not
finite-state.)&nbsp; So, when de Roever held a workshop on formalisms for
real-time systems, it was a good opportunity to show off how easily
TLA deals with real-time algorithms.&nbsp; We also proved some new results
about nonZeno specifications.&nbsp; I believe this paper introduced the
terms <i>Zeno</i> and <i>nonZeno</i>, though the notion of Zeno
behaviors had certainly occurred to others.&nbsp; It does seem to have been
the first to observe the relation between nonZenoness and machine
closure.&nbsp; Abadi has the following to say about this paper:

 <blockquote><P>
For me, this paper was partly a vindication of some work I had done
with [Gordon] Plotkin 
 [<B>A 
         Logical View of Composition</B>,
   <i>Theoretical Computer Science 114</i>, 1 (June 1993), 3-30], 
where we explored the definition and properties of the "while"
operator (<tt>-|></tt>).&nbsp; I believe that you thought
that the work was a formal game, so I was pleased to find that we
could use it in this paper.&nbsp;
 </P></blockquote> 


The paper uses as an example a mutual exclusion protocol due to
Michael Fischer.&nbsp; This example has an amusing history.&nbsp; When I was
working on <A HREF = "#fast-mutex">[73]</A>, I sent Fischer email describing my
algorithm and asking if he knew of a better solution.&nbsp; He responded

 <blockquote><P>
No, I don't, but the practical version of the problem sounds very
similar to the problem of bus allocation in contention networks.&nbsp;
I wonder if similar solutions couldn't be used?  For example, how
about...&nbsp;
 </P></blockquote> 

He followed this with a simple, elegant protocol based on real-time
delays.&nbsp; Because of its real-time assumptions, his protocol didn't
solve the problem that motivated <A HREF = "#fast-mutex">[73]</A>.&nbsp; I mentioned
this algorithm in <A HREF = "#fast-mutex">[73]</A>, but had forgotten about it by
the time of de Roever's workshop.&nbsp; Fred Schneider reminded me of it
when he used it as an example in his talk at the workshop.&nbsp; We
then incorporated the example in our paper.&nbsp; I later mentioned this to
Fischer, who had no memory of the protocol and even claimed that it
wasn't his.&nbsp; Fortunately, I happen to have saved his original email,
so I had the proof.&nbsp; The message, complete with message number, is
cited in the paper--the only instance of such a citation that I know
of.&nbsp;

<P>

The proofs in this paper are in an appendix that was published
by ACM only online.&nbsp; It is on their web site, but in a separate file
from that containing the paper.&nbsp; The link above is to a version of the
paper containing the appendix.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-ftrtft94"><B>Specifying and Verifying Fault-Tolerant
   Systems</B>&nbsp; (with Stephan Merz)<BR><i>Formal Techniques in Real-Time and
   Fault-Tolerant Systems</i>, H. Langmaack, W.-P. de Roever, J. Vytopil
   editors.&nbsp; Lecture Notes in Computer Science, number 863,
   Springer-Verlag, (September 1994) 41-76.<BR>
<A HREF = "lamport-ftrtft94.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-ftrtft94.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-ftrtft94.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1994 by Springer-Verlag.</FONT><HR>


Willem-Paul de Roever invited me to give a talk at this symposium.&nbsp; I
was happy to have a podium to explain why verifying fault-tolerant,
real-time systems should not be a new or especially difficult problem.&nbsp;
This was already explained in <A HREF = "#lamport-old-fashioned">[107]</A> for
real-time systems, but I knew that there would be people who thought
that fault-tolerance made a difference.&nbsp; Moreover, de Roever assured
me that L&#252;beck, where the symposium was held, is a
beautiful town.&nbsp; (He was right.)&nbsp; So, I decided to redo in TLA the
proof from <A HREF = "#spec-and-proof">[51]</A>.&nbsp; However, when the time
came to write the paper, I realized that I had overextended myself and
needed help.&nbsp; The abstract states
<blockquote><P>
  We formally specify a well known solution to the Byzantine generals
problem and give a rigorous, hierarchically structured proof of its
correctness.&nbsp; We demonstrate that this is an engineering exercise,
requiring no new scientific ideas.&nbsp; 
 </P></blockquote> 
However, there were few computer scientists capable of doing this
straightforward engineering exercise.&nbsp; Stephan Merz was one of them.&nbsp;
So, I asked him to write the proof, which he did with
his usual elegance and attention to detail.&nbsp; I think I provided most
of the prose and the initial version of the TLA specification,
which Merz modified a bit.&nbsp; The proof was all Merz's.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "lamport-howtowrite"><B>How to Write a Long Formula</B><BR><i>FACJ 6</i>(5)
   (September/October 1994) 580-584.&nbsp; Also appeared as SRC Research
   Report&nbsp;119.<BR>
<A HREF = "lamport-howtowrite.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-howtowrite.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-howtowrite.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1994 by Springer-Verlag.</FONT><HR>


Specifications often contain formulas that are a page or two long.&nbsp;
Mathematicians almost never write formulas that long, so they haven't
developed the notations needed to cope with them.&nbsp; This article
describes my notation for using indentation to eliminate parentheses
in a formula consisting of nested conjunctions and disjunctions.&nbsp; I
find this notation very useful when writing specifications.&nbsp; The
notation is part of the formal syntax of TLA+ (see
<A HREF = "#lamport-spec-tla-plus">[128]</A>).&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "intro-to-tla"><B>Introduction to TLA</B><BR>SRC Technical Note 1994-001
 (December 1994).<BR>
<A HREF = "intro-to-tla.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "intro-to-tla.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "intro-to-tla.pdf">PDF</A><HR>


This is a very brief (7-page) introduction to what TLA formulas mean.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "adding-process-algebra"><B>Adding "Process Algebra" to 
    TLA</B><BR>Unpublished (January 1995).<BR>
<A HREF = "adding-process-algebra.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "adding-process-algebra.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "adding-process-algebra.pdf">PDF</A><HR>


At the Dagstuhl workshop described in the discussion of
<A HREF = "#broy-specification-problem">[115]</A>, I was impressed by the elegance
of the process-algebraic specification presented by Rob van Glabbeek.&nbsp;
The ability to encode control state in the process structure permits
one to express some specifications quite nicely in CCS.&nbsp;
However, pure CCS forces you to encode the entire state in the process
structure, which is impractical for real specifications.&nbsp; I had the
idea of trying to get the best of both worlds by combining CCS and
TLA, and wrote this preliminary note about it.&nbsp; I hoped to work on
this with van Glabbeek but, although he was interested, he was busy
with other things and we never discussed it, and I never did anything
more with the idea.&nbsp; When I wrote this note, I wasn't sure if it was a
good idea.&nbsp; I now think that examples in which adding CCS to TLA would
significantly simplify the specification are unlikely to arise in
practice.&nbsp; So, I don't see any reason to complicate TLA in this way.&nbsp;
But, someone else may feel otherwise.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "ccs-proofs"><B>What Process Algebra Proofs Use Instead of 
   Invariance</B><BR>Unpublished (January 1995).<BR>
<A HREF = "ccs-proofs.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "ccs-proofs.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "ccs-proofs.pdf">PDF</A><HR>


Working on <A HREF = "#adding-process-algebra">[111]</A> got me thinking about how
process-algebraic proofs work.&nbsp; This draft note describes my
preliminary thoughts about what those proofs use instead of
invariance.&nbsp; I never developed this far enough to know if
it's right.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "abadi-conjoining"><B>Conjoining
   Specifications</B>&nbsp; (with Mart&#237;n
   Abadi)<BR><i>ACM Transactions on Programming Languages and Systems 17</i>, 3 (May 1995), 507-534.&nbsp; Also appeared as SRC
   Research Report&nbsp;118.<BR>
<A HREF = "abadi-conjoining.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "abadi-conjoining.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "abadi-conjoining.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1995 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


The obvious way to write an assume/guarantee specification is in the
form <i>E</i> implies <i>M</i>, where <i>E</i> specifies what we
assume of the environment and <i>M</i> specifies what the system
guarantees.&nbsp; That is what we did in <A HREF = "#abadi-composing">[98]</A>.&nbsp;
However, such a specification allows behaviors in which the system
violates the guarantee and the environment later violates its
assumption.&nbsp; This paper presents a better way to write the
specification that we discovered later.&nbsp; Instead of <i>E</i> implies
<i>M</i>, we take as the specification the stronger condition that
<i>M</i> must remain true at least one step longer than <i>E</i> is.&nbsp;
This enabled us to simplify and strengthen our results.&nbsp;

<p>
This paper contains two major theorems, one for decomposing
closed-system specifications and another for composing open-system
specifications.&nbsp; A preliminary conference version of the result for
closed systems appeared in <A HREF = "#abadi-decomposing">[104]</A>.&nbsp; A preliminary
conference version of the second appeared in <A HREF = "#abadi-open">[105]</A>.&nbsp;

<P>

Although the propositions and theorems in this paper are not in
principle difficult, it was rather hard to get the details right.&nbsp; We
couldn't have done it without writing careful, structured proofs.&nbsp; So,
I wanted those proofs published.&nbsp; But rigorous structured proofs, in
which all the details are worked out, are long and boring, and the
referees didn't read them.&nbsp; Since the referees hadn't read the proofs,
the editor didn't want to publish them.&nbsp; Instead, she wanted simply to
publish the paper without proofs.&nbsp; I was appalled that she was willing
to publish theorems whose proofs hadn't been checked, but was
unwilling to publish the unchecked proofs.&nbsp; But, I sympathized with
her reluctance to kill all those trees, so we agreed that she would
find someone to referee the proof and we would publish the appendix
electronically.&nbsp; The referee read the proofs carefully and
found three minor errors, which were easily corrected.&nbsp; Two of the
errors occurred when we made changes to one part of the proof without
making corresponding changes to another.&nbsp; The third was a careless
mistake in a low-level statement.&nbsp; When asked, the referee said that
the hierarchical structure, with all the low-level details worked out,
made the proofs quite clear and easy to check.&nbsp;

<P>

Almost all the proofs in this paper are in an appendix that was published
by ACM only online.&nbsp; It is on their web site, but in a separate file
from that containing the paper.&nbsp; The link above is to a version of the
paper containing the appendix.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "lamport-pictures"><B>TLA in Pictures</B><BR><i>IEEE Transactions on
   Software Engineering SE-21</i>, 9 September 1995), 768-775.&nbsp; Also
   appeared as SRC Research Report&nbsp;127.<BR>
<A HREF = "lamport-pictures.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-pictures.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-pictures.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1995 Personal use of this material is permitted.
However, permission to reprint/republish this
material for advertising or promotional purposes or
for creating new collective works for resale or
redistribution to servers or lists, or to reuse any
copyrighted component of this work in other works
must be obtained from the IEEE.
</FONT><HR>


Back in the 50s and 60s, programmers used flowcharts.&nbsp; Eventually,
guided by people like Dijkstra and Hoare, we learned that pictures
were a bad way to describe programs because they collapsed under the
weight of complexity, producing an incomprehensible spaghetti of boxes
and arrows.&nbsp; In the great tradition of learning from our mistakes how
to make the same mistake again, many people decided that drawing
pictures was a wonderful way to specify systems.&nbsp; So, they devised
graphical specification languages.&nbsp;

<p>
Not wanting to be outdone, I wrote this paper to show that you can
write TLA specifications by drawing pictures.&nbsp; It describes how to
interpret as TLA formulas the typical circles and arrows with which
people describe state transitions.&nbsp; These diagrams represent safety
properties.&nbsp; I could also have added some baroque conventions for
adding liveness properties to the pictures, but there's a limit to how
silly I will get.&nbsp; When I wrote the paper, I actually did think that
pictures might be useful for explaining parts of specifications.&nbsp; But
I have yet to encounter any real example where they would have helped.&nbsp;

<p>
This paper contains, to my knowledge, the only incorrect "theorem" I
have ever published.&nbsp; It illustrates that I can be as lazy as anyone
in not bothering to check "obvious" assertions.&nbsp; I didn't published
a correction because the theorem, which requires an additional
hypothesis, was essentially a footnote and didn't affect the main
point of the paper.&nbsp; Also, I was curious to see if anyone would notice
the error.&nbsp; Apparently, no one did.&nbsp; I discovered the error in
writing <A HREF = "#abadi-dagstuhl">[116]</A>


<BR>&nbsp;<P><LI> <A NAME = "broy-specification-problem"><B>The RPC-Memory Specification Problem:
   Problem Statement</B>&nbsp; (with Manfred Broy)<BR><i>Formal Systems Specification:
   The RPC-Memory Specification Case Study</i>, Manfred Broy, Stephan Merz,
   and Katharina Spies editors.&nbsp; Lecture Notes in Computer Science,
   number 1169, (1996), 1-4.<BR>
<A HREF = "broy-specification-problem.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "broy-specification-problem.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "broy-specification-problem.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1996 by Springer-Verlag.</FONT><HR>


I don't remember how this came about, but Manfred Broy and I organized
a Dagstuhl workshop on the specification and verification of
concurrent systems.&nbsp; (I'm sure I agreed to this knowing that Broy and
his associates would do all the real organizing.)&nbsp; We decided to pose
a problem that all participants were expected to solve.&nbsp; This is the
problem statement.&nbsp;

<p>
There is an interesting footnote to this workshop.&nbsp; As explained in
the discussion of <A HREF = "#spec">[50]</A>, I don't believe in writing
specifications as a conjunction of the properties that the system
should satisfy.&nbsp; Several participants used this approach.&nbsp; I thought
that the high-level specification was sufficiently trivial that by
now, people would be able to specify it in this way.&nbsp; However, Reino
Kurki-Suonio noticed an error that was present in all the "purely
axiomatic" specifications--that is, ones that mentioned only the
interface, without introducing internal variables.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "abadi-dagstuhl"><B>A TLA Solution to the RPC-Memory Specification
   Problem</B>&nbsp; (with Mart&#237;n Abadi and Stephan
   Merz)<BR><i>Formal Systems Specification: The RPC-Memory Specification
   Case Study</i>, Manfred Broy, Stephan Merz, and Katharina Spies editors.&nbsp;
   Lecture Notes in Computer Science, number 1169, (1996), 21-66.<BR>
<A HREF = "abadi-dagstuhl.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "abadi-dagstuhl.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "abadi-dagstuhl.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1996 by Springer-Verlag.</FONT><HR>


Since the problem posed in <A HREF = "#broy-specification-problem">[115]</A> was
devised by both Broy and me, I felt it was reasonable for me to
present a TLA solution.&nbsp; Mart&#237;n
Abadi, Stephan Merz, and I worked out a solution that Merz and I
presented at the workshop.&nbsp; Afterwards, we worked some more on it and
finally came up with a more elegant approach that is described in this
paper.&nbsp; I believe that Abadi and I wrote most of the prose.&nbsp; Merz
wrote the actual proofs, which he later checked using his <A
HREF =
"https://www.pst.informatik.uni-muenchen.de/~merz/isabelle/index.html">embedding
of TLA in Isabelle</A>.&nbsp; We all contributed to the writing
of the specifications.&nbsp;

<p> 

This is the only example I've encountered in which the pictures of TLA
formulas described in <A HREF = "#lamport-pictures">[114]</A> were of some use.&nbsp; In
fact, I discovered the error in <A HREF = "#lamport-pictures">[114]</A> when I
realized that one of the pictures in this paper provided a
counterexample to its incorrect theorem.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "automobile"><B>How to Tell a Program from an Automobile</B><BR>In <i>A
   Dynamic and Quick Intellect</i>, John Tromp editor (1996)--a <i>Liber
   Amicorum</i> issued by the CWI in honor of Paul Vitanyi's 25-year
   jubilee.<BR>
<A HREF = "automobile.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "automobile.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "automobile.pdf">PDF</A><HR>


I wrote this brief note in January, 1977.&nbsp; It came about because I was
struck by the use of the term <i>program maintenance</i>, which
conjured up in my mind images of lubricating the branch statements and
cleaning the pointers.&nbsp; So, I wrote this to make the observation that
programs are mathematical objects that can be analyzed logically.&nbsp; I
was unprepared for the strong emotions this stirred up among my
colleagues at Massachusetts Computer Associates, who objected
vehemently to my thesis.&nbsp; So, I let the whole thing drop.&nbsp; Years
later, when I was invited to submit a short paper for the volume
honoring Vitanyi, I decided that this paper would be appropriate
because he had learned to drive only a few years earlier.&nbsp; The paper
retains its relevance, since programmers still don't seem to
understand the difference between a program and an automobile.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "refinement"><B>Refinement in State-Based Formalisms</B><BR>SRC Technical 
      Note 1996-001 (December 1996).<BR>
<A HREF = "refinement.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "refinement.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "refinement.pdf">PDF</A><HR>


A brief (7-page) note explaining what refinement and dummy variables
are all about.&nbsp; It also sneaks in an introduction to TLA.&nbsp;
In September 2004, Tommaso Bolognesi pointed out an error in the
formula on the bottom of page 4 and suggested a correction.&nbsp; Instead
of modifying the note, I've decided to leave the problem of finding
and correcting the error as an exercise for the reader.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-drummers"><B>Marching to Many Distant 
   Drummers</B>&nbsp; (with Tim Mann)<BR>Unpublished (May 1997).<BR>
<A HREF = "lamport-drummers.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-drummers.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-drummers.pdf">PDF</A><HR>


In 1990, there were two competing proposals for a time service for the
Internet.&nbsp; One was from the DEC networking group and the other was in
an RFC by David Mills.&nbsp; The people in DEC asked me for theoretical
support for their belief that their proposal was better than that of
Mills.&nbsp; I asked Tim Mann to help me.&nbsp; We decided that we didn't like
either proposal very much, and instead we wrote a note with our own
idea for an algorithm to obtain the correct time in an Internet-like
environment.&nbsp; We sat on the idea for a few years, and eventually Tim
presented it at a Dagstuhl workshop on time synchronization.&nbsp; We then
began writing a more rigorous paper on the subject.&nbsp; This is as far as
we got.&nbsp; The paper is mostly finished, but it contains some minor
errors in the statements of the theorems and the proofs are not
completed.&nbsp; We are unlikely ever to work on this paper again.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-eye-of-beholder"><B>Processes are in the Eye of the
   Beholder</B><BR><i>Theoretical Computer Science, 179</i>, (1997), 333-351.&nbsp;
    Also appeared as SRC Research Report&nbsp;132.<BR>
<A HREF = "lamport-eye-of-beholder.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-eye-of-beholder.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-eye-of-beholder.pdf">PDF</A>
<BR><FONT SIZE = -2>All copyrights reserved by Elsevier Science 1997.</FONT><HR>

   
The notion of a process has permeated much of the work on concurrency.&nbsp;
Back in the late 70s, I was struck by the fact that a uniprocessor
computer could implement a multiprocess program, and that I had no
idea how to prove the correctness of this implementation.&nbsp; Once I had
realized that a system was specified simply as a set of sequences of
states, the problem disappeared.&nbsp; Processes are just a particular way
of viewing the state, and different views of the same system can have
different numbers of processors.&nbsp; 

<p> A nice example of this is an <i>N</i>-buffer
producer/consumer system, which is usually viewed as consisting of a
producer and a consumer process.&nbsp; But we can also view it as an
<i>N</i>-process system, with each buffer being a process.&nbsp;
Translating the views into concrete programs yields two programs that
look quite different.&nbsp; It's not hard to demonstrate their equivalence
with a lot of hand waving.&nbsp; With TLA, it's easy to replace the
hand waving by a completely formal proof.&nbsp; This paper sketches how.&nbsp;


<p>

I suspected that it would be quite difficult and perhaps impossible to
prove the equivalence of the two programs with process algebra.&nbsp; So,
at the end of the paper, I wrote "it would be interesting to compare
a process-algebraic proof ... with our TLA proof." As
far as I know, no process algebraist has taken up the challenge.&nbsp; I
figured that a proof similar to mine could be done in any trace-based
method, such as I/O automata.&nbsp; But, I expected that trying to make it
completely formal would be hard with other methods.&nbsp; Yuri Gurevich and
Jim Huggins decided to tackle the problem using Gurevich's evolving
algebra formalism (now called <i>abstract state machines</i>).&nbsp; The
editor processing my paper told me that they had submitted their
solution and suggested that their paper and mine be published in the
same issue, and that I write some comments on their paper.&nbsp; I agreed,
but said that I wanted to comment on the final version.&nbsp; I heard
nothing more about their paper, so I assumed that it had been
rejected.&nbsp; I was surprised to learn, three years later, that the
Gurevich and Huggins paper, <i>Equivalence is in the Eye of the
Beholder</i>, appeared right after mine in the same issue of
<i>Theoretical Computer Science</i>.&nbsp; They chose to write a
"human-oriented" proof rather than a formal one.&nbsp; Readers can judge
for themselves how easy it would be to formalize their proof.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-how-to-make"><B>How to Make a Correct Multiprocess Program
   Execute Correctly on a Multiprocessor</B><BR><i>IEEE Transactions on
   Computers 46</i>, 7 (July 1997), 779-782.&nbsp; Also appeared as
   SRC Research Report&nbsp;96.<BR>
<A HREF = "lamport-how-to-make.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-how-to-make.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-how-to-make.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1997 Personal use of this material is permitted.
However, permission to reprint/republish this
material for advertising or promotional purposes or
for creating new collective works for resale or
redistribution to servers or lists, or to reuse any
copyrighted component of this work in other works
must be obtained from the IEEE.
</FONT><HR>


This paper was inspired by Kourosh Gharachorloo's
thesis.&nbsp; The problem he addressed was how to execute a multiprocess
program on a computer whose memory did not provide sequential
consistency (see <A HREF = "#multi">[35]</A>), but instead required explicit
synchronization operations (such as Alpha's memory barrier
instruction).&nbsp; He presented a method for deducing what synchronization
operations had to be added to a program.&nbsp; I realized that, if one
proved the correctness of an algorithm using the two-arrow formalism of 
<A HREF = "#new-approach">[33]</A>, the proof would tell you what synchronization
operations were necessary.&nbsp; This paper explains how.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "substitution"><B>Substitution: Syntactic versus Semantic</B><BR>SRC Technical
Note 1998-004 (March 1998).&nbsp; Rejected by <i>Information
Processing Letters</i>.<BR>
<A HREF = "substitution.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "substitution.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "substitution.pdf">PDF</A><HR>


What I find to be the one subtle and somewhat ugly part of TLA
involves substitution in <i>Enabled</i> predicates.&nbsp; In the predicate
<i>Enabled&nbsp;A</i>, there is an implicit quantification
over the primed variables in <i>A</i>.&nbsp; Hence, mathematical
substitution does not distribute over the <i>Enabled</i> operator.&nbsp;
This four-page note explains that the same problem arises in most
program logics because there is also an implicit quantification in the
sequential-composition (semicolon) operator, so substitution does not
distribute over semicolon.&nbsp; Apparently, no one had noticed this before
because they hadn't tried using programming logics to do the sort of
things that are easy to do in TLA.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-paxos"><B>The Part-Time Parliament</B><BR><i>ACM Transactions
   on Computer Systems 16</i>, 2 (May 1998), 133-169.&nbsp; Also appeared as
   SRC Research Report&nbsp;49.&nbsp; This paper was first 
   submitted in 1990, setting a personal record for publication delay
   that has since been broken by <A HREF = "#buridan">[60]</A>.<BR>
<A HREF = "lamport-paxos.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-paxos.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-paxos.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1998 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


A fault-tolerant file system called <i>Echo</i> was built at SRC in
the late 80s.&nbsp; The builders claimed that it would maintain consistency
despite any number of non-Byzantine faults, and would make progress if
any majority of the processors were working.&nbsp; As with most such
systems, it was quite simple when nothing went wrong, but had a
complicated algorithm for handling failures based on taking care of
all the cases that the implementers could think of.&nbsp; I decided that
what they were trying to do was impossible, and set out to prove it.&nbsp;
Instead, I discovered the Paxos algorithm, described in this paper.&nbsp;
At the heart of the algorithm is a three-phase consensus protocol.&nbsp;
Dale Skeen seems to have been the first to have recognized the need
for a three-phase protocol to avoid blocking in the presence of an
arbitrary single failure.&nbsp; However, to my knowledge, Paxos contains
the first three-phase commit algorithm that is a real algorithm, with
a clearly stated correctness condition and a proof of correctness.&nbsp;

<p> 

I thought, and still think, that Paxos is an important algorithm.&nbsp;
Inspired by my success at popularizing the consensus problem by
describing it with Byzantine generals, I decided to cast the algorithm
in terms of a parliament on an ancient Greek island.&nbsp; Leo Guibas
suggested the name <i>Paxos</i> for the island.&nbsp; I gave the Greek
legislators the names of computer scientists working in the field,
transliterated with Guibas's help into a bogus Greek dialect.&nbsp; (Peter
Ladkin suggested the title.)&nbsp; Writing about a lost civilization
allowed me to eliminate uninteresting details and indicate
generalizations by saying that some details of the parliamentary
protocol had been lost.&nbsp; To carry the image further, I gave a few
lectures in the persona of an Indiana-Jones-style archaeologist,
replete with Stetson hat and hip flask.&nbsp;

<p> 

My attempt at inserting some humor into the subject was a dismal
failure.&nbsp; People who attended my lecture remembered Indiana Jones, but
not the algorithm.&nbsp; People reading the paper apparently got so
distracted by the Greek parable that they didn't understand the
algorithm.&nbsp; Among the people I sent the paper to, and who claimed to
have read it, were Nancy Lynch, Vassos Hadzilacos, and Phil Bernstein.&nbsp;
A couple of months later I emailed them the following question:
 <blockquote><P> 
Can you implement a distributed database that can tolerate the failure
of any number of its processes (possibly all of them) without losing
consistency, and that will resume normal behavior when more than half
the processes are again working properly?
 </P></blockquote>
None of them noticed any connection between this question and the
Paxos algorithm.&nbsp;

<p> 

I submitted the paper to <i>TOCS</i> in 1990.&nbsp; All three referees said
that the paper was mildly interesting, though not very important, but
that all the Paxos stuff had to be removed.&nbsp; I was quite annoyed at
how humorless everyone working in the field seemed to be, so I did
nothing with the paper.&nbsp; A number of years later, a couple of people
at SRC needed algorithms for distributed systems they were building,
and Paxos provided just what they needed.&nbsp; I gave them the paper to
read and they had no problem with it.&nbsp; Here is Chandu Thekkath's
account of the history of Paxos at SRC.&nbsp;
 <blockquote><P> 
When Ed Lee and I were working on 
 <A href = 
  "https://dl.acm.org/doi/10.1145/237090.237157">Petal</A>
we needed some sort of commit protocol to make sure global operations
in the distributed system completed correctly in the presence of
server failures.&nbsp; We knew about 3PC and studied a description of it in
Bernstein, Hadzilacos, and Goodman's book <i>Concurrency Control and
Recovery in Database Systems</i>.&nbsp; We found the protocol a bit difficult
to understand and therefore abandoned our attempts at implementing it.&nbsp;
At around this time, Mike Schroeder told us about a protocol for
consensus that Leslie Lamport had invented and suggested we ask him
about it.&nbsp; Leslie gave Ed a copy of the <i>Part-Time Parliament</i> tech
report, which we both enjoyed reading.&nbsp; I particularly liked its
humour and to this day, cannot understand why people don't like that
tech report.&nbsp; Paxos had all the necessary properties we wanted for our
system and we figured we could implement it.&nbsp; Leslie provided
essential consulting help as well, which resulted in the first
implementation of the Paxos algorithm (including dynamic
reconfiguration) as far as I am aware.&nbsp; A year later, when we needed a
distributed lock server for the 
 <A href = 
  "https://dl.acm.org/doi/10.1145/268998.266694">Frangipani file system</A>
we used Paxos again.&nbsp;
 </P></blockquote>
So, I thought that maybe the
time had come to try publishing it again.&nbsp;

<p> Meanwhile, the one exception in this dismal tale was
Butler Lampson, who immediately understood the algorithm's
significance.&nbsp; He mentioned it in lectures and in a paper, and he
interested Nancy Lynch in it.&nbsp; De Prisco, Lynch, and Lampson published
their version of a specification and proof.&nbsp; Their papers made it more
obvious that it was time for me to publish my paper.&nbsp; So, I proposed
to Ken Birman, who was then the editor of <i>TOCS</i>, that he publish
it.&nbsp; He suggested revising it, perhaps adding a TLA specification of
the algorithm.&nbsp; But rereading the paper convinced me that the
description and proof of the algorithm, while not what I would write
today, was precise and rigorous enough.&nbsp; Admittedly, the paper needed
revision to take into account the work that had been published in the
intervening years.&nbsp; As a way of both carrying on the joke and saving
myself work, I suggested that instead of my writing a revision, it be
published as a recently rediscovered manuscript, with annotations by
Keith Marzullo.&nbsp; Marzullo was willing, Birman agreed, and the paper
finally appeared.&nbsp;

<p> 

There was an amusing typesetting footnote to this.&nbsp; To set off
Marzullo's annotations, I decided that they should be printed on a
gray background.&nbsp; ACM had recently acquired some wonderful new
typesetting software, and TOCS was not accepting camera-ready copy.&nbsp;
Unfortunately, their wonderful new software could not do shading.&nbsp; So,
I had to provide camera-ready copy for the shaded text.&nbsp; Moreover, their
brilliant software could accept this copy only in floating figures, so
Marzullo's annotations don't appear quite where they should.&nbsp;
Furthermore, their undoubtedly expensive software wasn't up to
typesetting serious math.&nbsp; (After all, it's a computing journal, so
why should they have to typeset formulas?)  Therefore, I had to provide
the camera-ready copy for the definitions of the invariants in section
A2, which they inserted as Figure 3 in the published version.&nbsp; So, the
fonts in that figure don't match those in the rest of the paper.&nbsp;


<p>
This paper won an ACM SIGOPS Hall of Fame Award in 2012.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "cohen-tlareduction"><B>Reduction in TLA</B>&nbsp; (with Ernie
   Cohen)<BR><i>CONCUR'98 Concurrency Theory</i>, David Sangiorgi and Robert
   de Simone editors.&nbsp; Lecture Notes in Computer Science, number 1466,
   (1998), 317-331.<BR>
<A HREF = "cohen-tlareduction.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "cohen-tlareduction.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "cohen-tlareduction.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1998 by Springer-Verlag.</FONT><HR>


Reduction is a method of deducing properties of a system by reasoning
about a coarser-grained model--that is, one having larger atomic
actions.&nbsp; Reduction was probably first used informally for reasoning
about multiprocess programs to justify using the coarsest model in
which each atomic operation accesses only a single shared variable.&nbsp;
The term <i>reduction</i> was coined by Richard Lipton, who published
the first paper on the topic.&nbsp; Reduction results have traditionally
been based on an argument that the reduced (coarser-grained) model is
in some sense equivalent to the original.&nbsp; For terminating programs
that simply produce a result, equivalence just means producing the
same result.&nbsp; But for reactive programs, it has been difficult to pin
down exactly what equivalence means.&nbsp; TLA allowed me for the first
time to understand the precise relation between the original and the
reduced systems.&nbsp; In <A HREF = "#lamport-reduction-tlanote">[95]</A>, I proved a
result for safety specifications that generalized the standard
reduction theorems.&nbsp; This result formulated reduction as a temporal
theorem relating the original and reduced specifications--that is, as
a property of individual behaviors.&nbsp; This formulation made it
straightforward to extend the result to handle liveness, but I didn't
get around to working on the extension until late in 1996.&nbsp;

<p> Meanwhile, Ernie Cohen had been working on reduction using
Kleene algebra, obtaining elegant proofs of nice, general results for
safety properties.&nbsp; I showed him the TLA version and my preliminary
results on liveness, and we decided to collaborate.&nbsp; This paper is the
result.&nbsp; We translated his more general results for safety into TLA
and obtained new results for liveness properties.&nbsp; The paper promises
a complete proof and a more general result in a later paper.&nbsp; The
result exists, but the later paper is unlikely ever to appear.&nbsp;
A draft of the complete proof is available
as a 
 <A HREF = "reduction-proof.ps">Postscript</A>,
 <A HREF = "reduction-proof.ps.Z">compressed Postscript</A>, or
 <A HREF = "reduction-proof.pdf">pdf</A> file.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "lamport-composition"><B>Composition: A Way to Make Proofs
   Harder</B><BR><i>Compositionality: The Significant Difference
   (Proceedings of the COMPOS'97 Symposium)</i>, Willem-Paul de Roever, Hans
   Langmaack, and Amir Pnueli editors.&nbsp; Lecture Notes in Computer
   Science, number 1536, (1998), 402-423.<BR>
<A HREF = "lamport-composition.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-composition.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-composition.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1998 by Springer-Verlag.</FONT><HR>


Systems are complicated.&nbsp; We master their complexity by building them
from simpler components.&nbsp; This suggests that to master the complexity
of reasoning about systems, we should prove properties of the separate
components and then combine those properties to deduce properties of
the entire system.&nbsp; In concurrent systems, the obvious choice of
component is the process.&nbsp; So, compositional reasoning has come to
mean deducing properties of a system from properties of its processes.&nbsp;

<p> I have long felt that this whole approach is rather silly.&nbsp;
You don't design a mutual exclusion algorithm by first designing the
individual processes and then hoping that putting them together
guarantees mutual exclusion.&nbsp; Similarly, anyone who has tried to deduce
mutual exclusion from properties proved by considering the processes
in isolation knows that it's the wrong way to approach the problem.&nbsp;
You prove mutual exclusion by finding a global invariant, and then
showing that each process's actions maintains the invariant.&nbsp; TLA
makes the entire reasoning process completely mathematical--the
specifications about which one reasons are mathematical formulas, and
proving correctness means proving a single mathematical formula.&nbsp; A
mathematical proof is essentially decompositional: you apply a
deduction rule to reduce the problem of proving a formula to that of
proving one or more simpler formulas.&nbsp; 

<p> This paper explains why traditional compositional
reasoning is just one particular, highly constrained way of
decomposing the proof.&nbsp; In most cases, it's not a very natural way and
results in extra work.&nbsp; This extra work is justified if it can
be done by a computer.&nbsp; In particular, decomposition along processes
makes sense if the individual processes are simple enough to be
verified by model checking.&nbsp; TLA is particularly good for doing
this because, as illustrated by <A HREF = "#lamport-eye-of-beholder">[120]</A>,
it allows a great deal of flexibility in choosing what constitutes
a process.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-possibility"><B>Proving Possibility
   Properties</B><BR><i>Theoretical Computer Science 206</i>, 1-2, (October
   1998), 341-352.&nbsp; Also appeared as SRC Research
   Report&nbsp;137.<BR>
<A HREF = "lamport-possibility.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-possibility.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-possibility.pdf">PDF</A>
<BR><FONT SIZE = -2>All copyrights reserved by Elsevier Science 1998.</FONT><HR>


One never wants to assert possibility properties as correctness
properties of a system.&nbsp; It's not interesting to know that a system
<i>might</i> produce the correct answer.&nbsp; You want to know that it
will never produce the wrong answer (safety) and that it eventually
will produce an answer (liveness).&nbsp; Typically, possibility properties
are used in branching-time logics that cannot express liveness.&nbsp; If
you can't express the liveness property that the system must do
something, you can at least show that the system might do it.&nbsp; In
particular, process algebras typically can express safety but not
liveness.&nbsp; But the trivial system that does nothing implements any
safety property, so process algebraists usually rule out such trivial
implementations by requiring bisimulation--meaning that the
implementation allows all the same possible behaviors as the
specification.&nbsp;

<p>

People sometimes argue that possibility properties are important by
using the ambiguities of natural language to try to turn a liveness
property into a possibility property.&nbsp; For example, they may say that
it should be possible for the user of a bank's ATM to withdraw money
from his account.&nbsp; However, upon closer examination, you don't just
want this to be possible.&nbsp; (It's possible for me to withdraw money
from an ATM, even without having an account, if a medium-sized
meteorite hits it.)&nbsp; The real condition is that, if the user presses
the right sequence of buttons, then he must receive the money.&nbsp;


<p> 

Since there is no reason to prove possibility properties of a system,
I was surprised to learn from Bob Kurshan--a very reasonable
person--that he regularly uses his model checker to verify
possibility properties.&nbsp; Talking to him, I realized that although
verifying possibility properties tells you nothing interesting about a
system, it can tell you something interesting about a specification,
which is a mathematical model of the system.&nbsp; For example, you don't
need to specify that a user can hit a button on the ATM, because
you're specifying the ATM, not the user.&nbsp; However, we don't reason
about a user interacting with the ATM; we reason about a mathematical
model of the user and the ATM.&nbsp; If, in that mathematical
model, it were impossible for the button to be pushed, then the model
would be wrong.&nbsp; Proving possibility properties can provide sanity
checks on the specification.&nbsp; So, I wrote this paper explaining how
you can use TLA to prove possibility properties of a
specification--even though a linear-time temporal logic like TLA
cannot express the notion of possibility.&nbsp;

<p> I originally submitted this paper to a different journal.&nbsp;
However, the editor insisted that, to publish the paper, I had to add
a discussion about the merits of branching-time versus linear-time
logic.&nbsp; I strongly believe that it's the job of an editor to judge the
paper that the author wrote, not to get him to write the paper that
the editor wants him to.&nbsp; So, I appealed to the editor-in-chief.&nbsp;
After receiving no reply for several months, I withdrew the paper and
submitted it to <i>TCS</i>.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "ladkin-gerth"><B>A Lazy Caching Proof in TLA</B>&nbsp; (with Peter Ladkin, Bryan
   Olivier, and Denis Roegel)<BR><i>Distributed Computing 12</i>, 2/3,
   (1999), 151-174.<BR>
<A HREF = "ladkin-gerth.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "ladkin-gerth.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "ladkin-gerth.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1999 by Springer-Verlag.</FONT><HR>


At some gathering (I believe it was the workshop where I presented
<A HREF = "#abadi-decomposing">[104]</A>), Rob Gerth told me that he was compiling a
collection of proofs of the lazy caching algorithm of Afek, Brown, and
Merritt.&nbsp; I decided that this was a good opportunity to demonstrate
the virtues of TLA, so there should be a TLA solution.&nbsp; In particular,
I wanted to show that the proof is a straightforward engineering task,
not requiring any new theory.&nbsp; I wanted to write a completely formal,
highly detailed structured proof, but I didn't want to do all that
dull work myself.&nbsp; So, I enlisted Ladkin, Olivier (who was then a
graduate student of Paul Vitanyi in Amsterdam), and Roegel (who was
then a graduate student of Dominique Mery in Nancy), and divided the
task among them.&nbsp; However, writing a specification and proof is a
process of continual refinement until everything fits together right.&nbsp;
Getting this done in a long-distance collaboration is not easy, and we
got tired of the whole business before a complete proof was written.&nbsp;
However, we had done enough to write this paper, which contains
specifications and a high-level overview of the proof.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-spec-tla-plus"><B>Specifying Concurrent Systems with
   TLA+</B><BR><i>Calculational System Design</i>.&nbsp; M. Broy and R.&nbsp;
   Steinbr&#252;ggen, editors.&nbsp; IOS Press, Amsterdam,
   (1999), 183-247.<BR>
<A HREF = "lamport-spec-tla-plus.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-spec-tla-plus.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-spec-tla-plus.pdf">PDF</A><HR>


I was invited to lecture at the 1998 Marktoberdorf summer school.&nbsp; One
reason I accepted was that I was in the process of writing a book on
concurrency, and I could use the early chapters of the book as my
lecture notes.&nbsp; However, I decided to put aside (perhaps forever) that
book and instead write a book on TLA+.&nbsp; I was able to
recycle much material from my original notes for the purpose.&nbsp; For the
official volume of published notes for the course, I decided to
provide this, which is a preliminary draft of the first several
chapters of <A HREF = "#tla+-book">[145]</A>.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "fm99"><B>TLA+ Verification of Cache-Coherence
  Protocols</B>&nbsp; (with Homayoon Akhiani et al.)<BR>Rejected from 
 <i>Formal Methods&nbsp;'99</i> 
 (February 1999).<BR>
<A HREF = "fm99.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "fm99.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "fm99.pdf">PDF</A><HR>


Mark Tuttle, Yuan Yu, and I formed a small group applying TLA to
verification problems at Compaq.&nbsp; Our two major projects, in which we
have had other collaborators, have been verifications of protocols for
two multiprocessor Alpha architectures.&nbsp; We thought it would be a good
idea to write a paper describing our experience doing verification in
industry.&nbsp; The FM'99 conference had an Industrial Applications track,
to include "Experience reports [that] might describe a case study or
industrial project where a formal method was applied in practice."
So, we wrote this paper and submitted it.&nbsp; It was rejected.&nbsp; One of
the referees wrote, "The paper is rather an experience report than a
scientific paper." Our paper is indeed short on details, since
neither system had been released at that time and almost all
information about it was still company confidential.&nbsp; However, I think
it still is worth reading if you're interested in what goes on in the
industrial world.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-types"><B>Should Your Specification Language Be
   Typed?</B>&nbsp; (with Larry Paulson)<BR><i>ACM Transactions on Programming
   Languages and Systems 21</i>, 3 (May 1999) 502-526.&nbsp; Also
   appeared as SRC Research Report&nbsp;147.<BR>
<A HREF = "lamport-types.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-types.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-types.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 1999 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


In 1995, I wrote a diatribe titled <i>Types Considered Harmful</i>.&nbsp;
It argued that, although types are good for programming languages,
they are a bad way to formalize mathematics.&nbsp; This implies that they
are bad for specification and verification, which should be
mathematics rather than programming.&nbsp; My note apparently provoked some
discussion, mostly disagreeing with it.&nbsp; I thought it might be fun to
promote a wider discussion by publishing it, and <i>TOPLAS</i> was,
for me, the obvious place.&nbsp; Andrew Appel, the editor-in-chief at the
time, was favorably disposed to the idea, so I submitted it.&nbsp; Some of
my arguments were not terribly sound, since I know almost nothing
about type theory.&nbsp; The referees were suitably harsh, but Appel felt
it would still be a good idea to publish a revised version along with
rebuttals.&nbsp; I suggested that it would be better if I and the referees
cooperated on a single balanced article presenting both sides of the
issue.&nbsp; The two referees agreed to shed their anonymity and
participate.&nbsp; Larry Paulson was one of the referees.&nbsp; It soon became
apparent that Paulson and I could not work with the other referee, who
was rabidly pro-types.&nbsp; (At one point, he likened his situation to
someone being asked by a neo-Nazi to put his name on a "balanced"
paper on racism.)&nbsp; So, Paulson and I wrote the paper by ourselves.&nbsp; We
expected that the other referee would write a rebuttal, but he
apparently never did.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "yuanyu-model-checking"><B>Model Checking TLA+
   Specifications</B>&nbsp; (with Yuan Yu and Panagiotis Manolios)<BR>In
   <i>Correct Hardware Design and Verification Methods (CHARME '99)</i>,
   Laurence Pierre and Thomas Kropf editors.&nbsp; Lecture Notes in Computer
   Science, number 1703, Springer-Verlag, (September 1999) 54-66.<BR>
<A HREF = "yuanyu-model-checking.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "yuanyu-model-checking.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "yuanyu-model-checking.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
1999 by Springer-Verlag.</FONT><HR>


Despite my warning him that it would be impossible, Yuan Yu wrote a
model checker for TLA+ specifications.&nbsp; He succeeded
beyond my most optimistic hopes.&nbsp; This paper is a preliminary report
on the model checker.&nbsp; I was an author, at Yu's insistence, because I
gave him some advice on the design of the model checker (more useful
advice than just don't do it).&nbsp; Manolios worked at SRC as a summer
intern and contributed the state-compression algorithm that is
described in the paper, but which ultimately was not used in the model
checker.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-latex-interview"><B>How (La)TeX changed the face of
   Mathematics</B><BR><i>Mitteilungen der Deutschen Mathematiker-Vereinigung
   1/2000</i> (Jan 2000) 49-51.<BR>
<A HREF = "lamport-latex-interview.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-latex-interview.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-latex-interview.pdf">PDF</A><HR>


G&#252;nther Ziegler interviewed me by email for this note, which
delves into the history of LaTeX.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lamport-fairness"><B>Fairness and Hyperfairness</B><BR><i>Distributed 
Computing 13</i>, 4 (2000), 239-245.&nbsp; Also appeared as
SRC Research 
  Report&nbsp;152.&nbsp; 
    A preliminary version of this paper 
   was rejected by <i>Concur</i>&nbsp;99.<BR>
<A HREF = "lamport-fairness.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-fairness.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "lamport-fairness.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
2000 by Springer-Verlag.</FONT><HR>


In 1993, Attie, Francez, and Grumberg published a paper titled
<i>Fairness and Hyperfairness in Multi-Party Interactions</i>.&nbsp; This
was a follow-up to the 1988 paper <i>Appraising Fairness in
Languages for Distributed Programming</i> by Apt, Francez, and Katz,
which attempted to define fairness.&nbsp; I have long believed that the
only sensible formal definition of fairness is machine closure, which
is essentially one of the conditions mentioned by Apt, Francez, and
Katz.&nbsp; (They called it <i>feasibility</i> and phrased it as a
condition on a language rather than on an individual specification.)&nbsp;
I refereed the Attie, Francez, and Grumberg paper and found it rather
uninteresting because it seemed to be completely language-dependent.&nbsp;
They apparently belonged to the school, popular in the late 70s and
early 80s, that equated concurrency with the CSP programming
constructs.&nbsp; I wrote a rather unkind review of that paper, which
obviously didn't prevent its publication.&nbsp; Years later, it suddenly
struck me that there was a language-independent definition of
hyperfairness--or more precisely, a language-independent notion that
seemed to coincide with their definition on a certain class of CSP
programs.&nbsp; I published this paper for three reasons: to explain the
new definition of hyperfairness; to explain once again that fairness
is machine closure and put to rest the other two fairness criteria
conditions of Apt, Francez, and Katz; and, in some small way, to make
up for my unkind review of the Attie, Francez, and Grumberg paper.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "www9"><B>Archival References to Web Pages</B><BR>Ninth 
 International World Wide Web Conference: Poster Proceedings (May 2000), 
  page 74..<BR><A HREF="https://www9.org/final-posters/poster52.html">Web 
              Page</A><HR>


On several occasions, I've had to refer to a web page in a published
article.&nbsp; The problem is that articles remain on library shelves for
many decades, while URLs are notoriously transitory.&nbsp; This short
poster at WWW9 describes a little trick of mine for referring to a web
page by something more permanent than a URL.&nbsp; It involved
embedding a unique string in the page, along with an explanation of
how a document could tell people to search for that string without
actually putting the string in the document.&nbsp; Unfortunately, the
version posted by the conference is missing a gif file.&nbsp; A
version with the gif is available
 <A HREF="../tla/www9.html">here</A>.&nbsp;

<p>It was a lovely idea that required no infrastructure and
anyone could just use it.&nbsp; Unfortunately, vandals soon invaded the Web
and, with the intellectual sophistication of people who deface
street signs, they put such a string on lots of other web pages.&nbsp;
Fortunately, web search engines have gotten better and web pages can
simply be given descriptive titles.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "disk-paxos"><B>Disk Paxos</B>&nbsp; (with Eli Gafni)<BR><i>Distributed 
Computing 16</i>, 1 (2003) 1-20.<BR>
<A HREF = "disk-paxos.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "disk-paxos.ps.gz">gzipped Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "disk-paxos.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
2003 by Springer-Verlag.</FONT><HR>


In 1998, Jim Reuter of DEC's storage group asked me for a
leader-election algorithm for a network of processors and disks that
they were designing.&nbsp; The new wrinkle to the problem was that they
wanted a system with only two processors to continue to operate if
either processor failed.&nbsp; We could assume that the system had at least
three disks, so the idea was to find an algorithm that achieved fault
tolerance by replicating disks rather than processors.&nbsp; I convinced
them that they didn't want a leader-election protocol, but rather a
distributed state-machine implementation (see <A HREF = "#time-clocks">[27]</A>).&nbsp;
At the time, Eli Gafni was on sabbatical from UCLA and was consulting
at SRC.&nbsp; Together, we came up with the algorithm described
in this paper, which is a disk-based version of the Paxos algorithm of
<A HREF = "#lamport-paxos">[123]</A>.&nbsp;

<p> 

Gafni devised the initial version of the algorithm, which didn't look
much like Paxos.&nbsp; As we worked out the details, it evolved into its
current form.&nbsp; Gafni wanted a paper on the algorithm to follow the
path with which the algorithm had been developed, starting from his
basic idea and deriving the final version by a series of
transformations.&nbsp; We wrote the first version of the paper in this way.&nbsp;
However, when trying to make it rigorous, I found that the
transformation steps weren't as simple as they had appeared.&nbsp; I found
the resulting paper unsatisfactory, but we submitted it anyway to
PODC'99, where it was rejected.&nbsp; Gafni was then willing to let me do
it my way, and I turned the paper into its current form.&nbsp;

<p>

A couple of years after the paper was published, Mauro J. Jaskelioff
encoded the proof in Isabelle/HOL and mechanically checked it.&nbsp; He
found about a dozen small errors.&nbsp; Since I have been proposing Disk
Paxos as a test example for mechanical verification of concurrent
algorithms, I have decided not to update the paper to correct the
errors he found.&nbsp; Anyone who writes a rigorous mechanically-checked
proof will find them.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "disk-paxos-disc"><B>Disk Paxos (Conference Version)</B>&nbsp; (with Eli
Gafni)<BR><i>Distributed Computing: 14th International Conference,
DISC 2000</i>, Maurice Herlihy, editor. Lecture Notes in Computer Science
number 1914, Springer-Verlag, (2000) 330-344.<BR>
<A HREF = "disk-paxos-disc.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "disk-paxos-disc.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "disk-paxos-disc.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
2000 by Springer-Verlag.</FONT><HR>


This is the abridged conference version of <A HREF = "#disk-paxos">[135]</A>.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "when-mutex"><B>When Does a Correct Mutual Exclusion Algorithm
  Guarantee Mutual Exclusion</B>&nbsp; (with Sharon Perl and William Weihl)<BR>
  <i>Information Processing Letters 76</i>, 3 (March 2000),
   131-134.<BR>
<A HREF = "when-mutex.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "when-mutex.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "when-mutex.pdf">PDF</A>
<BR><FONT SIZE = -2>All copyrights reserved by Elsevier Science 2000.</FONT><HR>


Mutual exclusion is usually defined to mean that two processes are not
in their critical section <i>at the same time</i>.&nbsp; Something Dan
Scales said during a conversation made me suddenly realize that
conventional mutual exclusion algorithms do not satisfy that property.&nbsp;
I then conjectured how that property could be satisfied, and Perl and
Weihl proved that my conjecture was correct.&nbsp; This paper explains why
mutual exclusion had not previously been achieved, and how to achieve
it--all in less than five pages.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "consensus-bounds"><B>Lower Bounds on Consensus</B><BR>unpublished note
(March 2000).<BR>
<A HREF = "consensus-bounds.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "consensus-bounds.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "consensus-bounds.pdf">PDF</A><HR>


This short note is described by its abstract:
   <blockquote><P> 
   We derive lower bounds on the number of messages and the number of
   message delays required by a nonblocking fault-tolerant consensus
   algorithm, and we show that variants of the Paxos algorithm achieve
   those bounds.&nbsp;
   </P></blockquote>
I sent it to Idit Keidar who told me that the bounds I derived were
already known, so I forgot about it.&nbsp; About a year later, she mentioned
that she had cited the note in:
   <blockquote><P> 
Idit Keidar and Sergio Rajsbaum.&nbsp;
 On the Cost of Fault-Tolerant Consensus When There Are No Faults - A Tutorial.&nbsp;
 MIT Technical Report MIT-LCS-TR-821, May 24 2001.&nbsp;
 Preliminary version in SIGACT News 32(2), Distributed Computing
 column, pages 45-63, June 2001 (published in May 15th).&nbsp;
   </P></blockquote>
I then asked why they had cited my note if the results were already known.&nbsp;
She replied, 
   <blockquote><P> 
There are a few results in the literature that are similar, but not
identical, because they consider slightly different models or
problems.&nbsp; This is a source of confusion for many people. Sergio and I
wrote this tutorial in order to pull the different known results
together. Hopefully, it can help clarify things up.&nbsp;
   </P></blockquote>


<BR>&nbsp;<P><LI> <A NAME = "wildfire-challenge"><B>The Wildfire Challenge Problem</B>&nbsp; (with Madhu 
   Sharma, Mark Tuttle, and Yuan Yu)<BR>
   Rejected from CAV 2001 (January 2001).<BR>
<A HREF = "wildfire-challenge.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "wildfire-challenge.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "wildfire-challenge.pdf">PDF</A><HR>


 From late fall 1996 through early summer 1997, Mark Tuttle, Yuan Yu,
and I worked on the specification and verification of the
cache-coherence protocol for a computer code-named Wildfire.&nbsp; We
worked closely with Madhu Sharma, one of Wildfire's designers.&nbsp; We
wrote a detailed specification of the protocol as well as a
specification of the memory model that it was supposed to implement.&nbsp;
We then proved various properties, but did not attempt a complete
proof.&nbsp; In early 2000, Madhu, Mark, and I wrote a specification of a
higher-level abstract version of the protocol.&nbsp; 

<p> 

There was one detail of the protocol that struck me as particularly
subtle.&nbsp; I had the idea of publishing an incorrect version of the
specification with that detail omitted as a challenge problem for the
verification community.&nbsp; I did that and put it  
<A HREF="/tla/wildfire-challenge.html">on
the Web</A> in June, 2000.&nbsp; To further disseminate the problem, we
wrote this description of it for the CAV (Computer Aided Verification)
conference.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "paxos-simple"><B>Paxos Made Simple</B><BR><i>ACM SIGACT News
(Distributed Computing Column) 32</i>, 4 (Whole Number 121, December
2001) 51-58.<BR>
<A HREF = "paxos-simple.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "paxos-simple.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "paxos-simple.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 2001 by the Association for Computing Machinery, Inc.Permission to make digital or hard copies of part 
or all of this work for personal or classroom use
is granted without fee provided that copies are
not made or distributed for profit or commercial
advantage and that copies bear this notice and
the full citation on the first page.  Copyrights
for components of this work owned by others than
ACM must be honored.  Abstracting with credit is
permitted.  To copy otherwise, to republish, to
post on servers, or to redistribute to lists,
requires prior specific permission and/or a fee.
Request permissions from Publications Dept, ACM
Inc., fax +1 (212) 869-0481, or
permissions@acm.org.  The definitive version of
this paper can be found at ACM's Digital Library
--http://www.acm.org/dl/.
</FONT><HR>


At the PODC 2001 conference, I got tired of everyone saying how
difficult it was to understand the Paxos algorithm, published in
<A HREF = "#lamport-paxos">[123]</A>.&nbsp; Although people got so hung up in the
pseudo-Greek names that they found the paper hard to understand, the
algorithm itself is very simple.&nbsp; So, I cornered a couple of people at
the conference and explained the algorithm to them orally, with no
paper.&nbsp; When I got home, I wrote down the explanation as a short note,
which I later revised based on comments from Fred Schneider and Butler
Lampson.&nbsp; The current version is 13 pages long, and contains no
formula more complicated than <i>n</i>1 > <i>n</i>2.&nbsp;

<p> 
In 2015, Michael Deardeuff of Amazon informed me that one
sentence in this paper is ambiguous, and interpreting it the wrong way
leads to an incorrect algorithm.&nbsp; Deardeuff found
that a number of Paxos implementations on Github implemented
this incorrect algorithm.&nbsp; Apparently, the implementors did not bother
to read the precise description of the algorithm in 
 <A HREF = "#lamport-paxos">[123]</A>.&nbsp;
I am not going to remove this ambiguity or reveal where it is.&nbsp; Prose
is not the way to precisely describe algorithms.&nbsp;
  <b>Do not try to implement the algorithm
from this paper.&nbsp; Use <A HREF = "#lamport-paxos">[123]</A> instead.</b>



<BR>&nbsp;<P><LI> <A NAME = "spec-and-verifying"><B>Specifying and Verifying Systems with 
  TLA+</B>&nbsp; (with John Matthews, Mark Tuttle, and Yuan Yu)<BR>Proceedings 
   of the Tenth ACM SIGOPS European Workshop (2002), 45-48.<BR>
<A HREF = "spec-and-verifying.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "spec-and-verifying.ps.gz">gzipped Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "spec-and-verifying.pdf">PDF</A><HR>


This describes our experience at DEC/Compaq using TLA+
and the TLC model checker on several systems, mainly cache-coherence
protocols.&nbsp; It is shorter than, and more up-to-date
than <A HREF = "#fm99">[129]</A>.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "arbiter-free"><B>Arbiter-Free Synchronization</B><BR>
<i>Distributed Computing 16</i>, 2/3, (2003) 219-237.<BR>
<A HREF = "arbiter-free.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "arbiter-free.ps.gz">gzipped Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "arbiter-free.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
2003 by Springer-Verlag.</FONT><HR>


With the bakery algorithm of <A HREF = "#bakery">[12]</A>, I discovered that mutual
exclusion, and hence all conventional synchronization problems, could
be solved with simple read/write registers.&nbsp; However, as recounted in
the description of <A HREF = "#glitch">[22]</A>, such a register requires an
arbiter.&nbsp; This leads to the question: what synchronization problems
can be solved without an arbiter?  Early on, I devised a more
primitive kind of shared register that can be implemented without an
arbiter, and I figured out how to solve the producer/consumer problem
with such registers.&nbsp; I think that hardware designers working on
self-timed circuits probably already knew that producer/consumer
synchronization could be implemented without an arbiter.&nbsp; (If not,
they must have figured it out at about the same time I did.)&nbsp;
Hardware people used Muller C-elements instead of my shared registers,
but it would have been obvious to them what I was doing.&nbsp;

<p> 

In Petri nets, arbitration appears explicitly as conflict.&nbsp; A class of
Petri nets called marked graphs, which were studied in the early 70s
by Anatol Holt and Fred Commoner, are the largest class of Petri nets
that are syntactically conflict-free.&nbsp; Marked-graph synchronization is
a natural generalization of producer/consumer synchronization.&nbsp; It was
clear to me that marked-graph synchronization can be implemented
without an arbiter, though I never bothered writing down the precise
algorithm.&nbsp; I assumed that marked graphs describe precisely the class
of synchronization problems that could be solved without an arbiter.&nbsp;

<p> 

That marked-graph synchronization can be implemented without an
arbiter is undoubtedly obvious to people like Anatol Holt and Chuck
Seitz, who are familiar with multiprocess synchronization, Petri nets,
and the arbiter problem.&nbsp; However, such people are a dying breed.&nbsp; So,
I thought I should write up this result before it was lost.&nbsp; I had
been procrastinating on this for years when I was invited to submit an
article for a special issue of <i>Distributed Computing</i> celebrating
the 20th anniversary of the PODC conference.&nbsp; The editors wanted me to
pontificate for a few pages on the past and future of distributed
computing--something I had no desire to do.&nbsp; However, it occurred to
me that it would be fitting to contribute some unpublished 25-year-old
work.&nbsp; So, I decided to write about arbiter-free synchronization.&nbsp;

<p> 

Writing the paper required me to figure out the precise arbiter-free
implementation of marked graphs, which wasn't hard.&nbsp; It also required
me to prove my assumption that marked graphs were all one could
implement without an arbiter.&nbsp; When I tried, I realized that my
assumption was wrong.&nbsp; There are multiprocess synchronization problems
not describable by marked graphs that can be solved without an
arbiter.&nbsp; The problem was more complicated than I had realized.&nbsp;

<p> 

I wish I knew exactly what can be done without an arbiter, but I
don't.&nbsp; It turns out that I don't really understand arbiter-free
synchronization.&nbsp; Lack of understanding leads to messy exposition.&nbsp; I
understand the results about the equivalence of registers, and I have
nice, crisp ways of formalizing and proving these results.&nbsp; But the
other results in the paper are a mess.&nbsp; They're complicated and I
don't have a good way of formalizing them.&nbsp; Had I written this
twenty-five years ago, I would probably have kept working on the
problem before publishing anything.&nbsp; But I don't have the time I once
did for mulling over hard problems.&nbsp; I decided it was better to
publish the results I have, even though they're messy, and hope that
someone else will figure out how to do a better job.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "ds-interview"><B>A 
Discussion With Leslie Lamport</B><BR>An interview in <i>IEEE
Distributed Systems Online 3</i>, 8 
<A HREF=
"https://dsonline.computer.org/portal/site/dsonline/menuitem.9ed3d9924aeb0dcd82ccc6716bbe36ec/index.jsp?&pName=dso_level1&path=dsonline/past_issues/0208/f&file=lam_print_int.xml&xsl=article.xsl&">(Web Page)</A>.<BR>
<A HREF = "ds-interview.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 2002 Personal use of this material is permitted.
However, permission to reprint/republish this
material for advertising or promotional purposes or
for creating new collective works for resale or
redistribution to servers or lists, or to reuse any
copyrighted component of this work in other works
must be obtained from the IEEE.
</FONT><HR>


In the spring of 2002, Dejan Milojicic proposed interviewing me for an
IEEE on-line publication.&nbsp; He volunteered to send me the questions in
advance, and to send me the transcript afterwards for correction.&nbsp;
This seemed pretty silly, so I just wrote my answers.&nbsp; The
"interview" was conducted by a few email exchanges.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "bertinoro"><B>Lower Bounds for Asynchronous
Consensus</B><BR><i>Future Directions in Distributed Computing</i>,
Andr&#233; Schiper, Alex A. Shvartsman, Hakim Weatherspoon,
and Ben Y. Zhao, editors.&nbsp; Lecture Notes in Computer Science
number 2584, Springer, (2003) 22-23.<BR>
<A HREF = "bertinoro.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "bertinoro.ps.gz">gzipped Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "bertinoro.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
2003 by Springer-Verlag.</FONT><HR>


The FuDiCo (Future Directions in Distributed Computing) workshop was
held in a lovely converted monastery outside Bologna.&nbsp; I was supposed
to talk about future directions, but restrained my natural inclination
to pontificate and instead presented some new lower-bound results.&nbsp;
The organizers wanted to produce a volume telling the world about the
future of distributed computing research, so everyone was supposed to
write a five-page summary of their presentations.&nbsp; I used only two
pages.&nbsp; Since I didn't have rigorous proofs of my results, and I
expected to discover special-case exceptions, I called them
approximate theorems.&nbsp; This paper promises that future papers will
give precise statements and proofs of the theorems, and algorithms
showing that the bounds are tight.&nbsp; Despite my almost perfect record
of never writing promised future papers, I actually wrote up the case
of non-Byzantine failures in <A HREF = "#lower-bound">[154]</A>.&nbsp; I intend some day
to write another paper with the general results for the Byzantine case.&nbsp;
Really.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "tla+-book"><B>Specifying Systems: The TLA+
Language and Tools for Hardware and Software Engineers</B><BR><i>Addison-Wesley
(2002)</i>.<BR><A HREF="https://lamport.azurewebsites.net/tla/book.html">Available On-Line</A><HR>


The complete book of TLA+.&nbsp; The first seven chapters (83
pages) are a rewritten version of <A HREF = "#lamport-spec-tla-plus">[128]</A>.&nbsp;
That and the chapter on the TLC model checker are about as much of the
book as I expect people to read.&nbsp; The web page contains errata and
some exercises and examples.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "fmsd"><B>Checking Cache-Coherence Protocols with
TLA+</B>&nbsp; (with Rajeev Joshi, John Matthews, Serdar Tasiran, Mark
Tuttle, and Yuan Yu)<BR><i>Formal Methods in System
Design 22</i>, 2 (March 2003) 125-131.<BR>
<A HREF = "fmsd.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "fmsd.ps.Z">Compressed Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "fmsd.pdf">PDF</A>
<BR><FONT SIZE = -2>All copyrights reserved by Kluwer Academic 2003.</FONT><HR>


Yet another report on the TLA+ verification activity at
Compaq.&nbsp; It mentions some work that's been done since we wrote
<A HREF = "#spec-and-verifying">[141]</A>.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "high-level"><B>High-Level Specifications: Lessons from
Industry</B>&nbsp; (with Brannon Batson)<BR><i>Formal Methods for Components and Objects</i>,
Frank S. de Boer, Marcello M. Bonsangue,
Susanne Graf, and Willem-Paul de Roever, editors.&nbsp; Lecture Notes in Computer Science
number 2852, Springer, (2003) 242-262.<BR>
<A HREF = "high-level.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "high-level.ps.gz">gzipped Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "high-level.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
2003 by Springer-Verlag.</FONT><HR>


I was invited to speak about TLA at the FMCO symposium.&nbsp; I didn't feel
that I had anything new to say, so I asked Brannon Batson who was
then at Intel to
help me prepare the talk and the paper.&nbsp; Brannon is a hardware
designer who started using TLA+ while at Compaq and has
continued using it at Intel.&nbsp; The most interesting part of this paper
is Section 4, which is mainly devoted to Brannon's description of how
his group is using TLA+ in their design process.&nbsp; 

Section 5 was inspired by the symposium's call for papers, whose
list of topics included such fashionable buzzwords as
"object-oriented", "component-based", and "information hiding".&nbsp;
It explains why those concepts are either irrelevant to or are a bad
idea for high-level specification.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "future-of-computing"><B>The Future of Computing: Logic or 
Biology</B><BR>Text of a talk given at Christian Albrechts University, Kiel on 
11 July 2003.<BR>
<A HREF = "future-of-computing.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "future-of-computing.ps.gz">gzipped Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "future-of-computing.pdf">PDF</A><HR>


I was invited to give a talk to a general university audience at Kiel.&nbsp;
Since I'm not used to giving this kind of non-technical talk, I wrote
it out in advance and read it.&nbsp; Afterwards, I received several
requests for a copy to be posted on the Web.&nbsp; So, here it is.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "paxos-commit"><B>Consensus on Transaction Commit</B>&nbsp; (with Jim Gray)<BR>
 <i>ACM Transactions on Database Systems 31</i>, 1 (2006), 133-160.&nbsp;
   Also appeared as Microsoft Research 
  Technical Report MSR-TR-2003-96 (February 2005).<BR><A HRef = 
 "https://www.microsoft.com/en-us/research/publication/consensus-on-transaction-commit/">Available 
  On-Line</A><HR>


In <A HREF = "#bertinoro">[144]</A>, I announced some lower-bound results for the
consensus problem.&nbsp; One result states that two message delays are
required to choose a value, and a relatively large number of
processors are needed to achieve that bound.&nbsp; When writing a careful
proof of this result, I realized that it required the hypothesis that
values proposed by two different processors could be chosen in two
message delays.&nbsp; This led me to realize that fewer processors were
needed if there were only one processor whose proposed value could be
chosen in two message delays, and values proposed by other processors
took longer to be chosen.&nbsp; In fact, a simple modification to the Paxos
algorithm of <A HREF = "#lamport-paxos">[123]</A> accomplished this.&nbsp;


<p>

I then looked for applications of consensus in which there is a single
special proposer whose proposed value needs to be chosen quickly.&nbsp; I
realized there is a "killer app"--namely, distributed transaction
commit.&nbsp; Instead of regarding transaction commit as one consensus
problem that chooses the single value <i>commit</i> or <i>abort</i>,
it could be presented as a set of separate consensus problems, each
choosing the commit/abort desire of a single participant.&nbsp; Each
participant then becomes the special proposer for one of the consensus
problems.&nbsp; This led to what I call the Paxos Commit algorithm.&nbsp; It is
a fault-tolerant (non-blocking) commit algorithm that I believed had
fewer message delays in the normal (failure-free) case than any
previous algorithm.&nbsp; I later learned that an algorithm published by
Guerraoui, Larrea, and Schiper in 1996 had the same normal-case
behavior.&nbsp;

<p>

Several months later, Jim Gray and I got together to try to understand
the relation between Paxos and the traditional Two-Phase Commit
protocol.&nbsp; After a couple of hours of head scratching, we figured out
that Two-Phase Commit is the trivial version of Paxos Commit that
tolerates zero faults.&nbsp; That realization and several months of
procrastination led to this paper, which describes the Two-Phase
Commit and Paxos Commit algorithms and compares them.&nbsp; It also
includes an appendix with TLA+ specifications of the
transaction-commit problem and of the two algorithms.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "hair"><B>On Hair Color in France</B>&nbsp; (with Ellen Gilkerson)<BR><i>Annals
of Improbable Research</i>, Jan/Feb 2004, 18-19.<BR>
<A HREF = "hair.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "hair.ps.gz">gzipped Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "hair.pdf">PDF</A><HR>


While traveling in France, Gilkerson and I observed many blonde women,
but almost no blonde men.&nbsp; Suspecting that we had stumbled upon a
remarkable scientific discovery, we endured several weeks of hardship
visiting the auberges and restaurants of France to gather data.&nbsp; After
several years of analysis and rigorous procrastination, we wrote this
paper.&nbsp; Much of our magnificent prose was ruthlessly eliminated by the
editor to leave space for less important research.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "wsfm-web"><B>Formal Specification of a Web Services
Protocol</B>&nbsp; (with James E. Johnson, David E. Langworthy, and Friedrich H.&nbsp;
Vogt)<BR><i>Electronic Notes in Theoretical Computer Science 105</i>,
M. Bravetti and G. Zavattaro editors. 
(December 2004) 147-158.<BR>
<A HREF = "wsfm-web.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "wsfm-web.ps.gz">gzipped Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "wsfm-web.pdf">PDF</A><HR>


Fritz Vogt spent part of a sabbatical at our lab during the summer and
fall of 2003.&nbsp; I was interested in getting TLA+ used in
the product groups at Microsoft, and Fritz was looking for an
interesting project involving distributed protocols.&nbsp; Through his
contacts, we got together with Jim Johnson and Dave Langworthy, who
work on Web protocols at Microsoft in Redmond.&nbsp; Jim and Dave were
interested in the idea of formally specifying protocols, and Jim
suggested that we look at the Web Services Atomic Transaction protocol
as a simple example.&nbsp; Fritz and I spent part of our time for a couple
of months writing it, with a lot of help from Jim and Dave in
understanding the protocol.&nbsp; This paper describes the specification
and our experience writing it.&nbsp; The specification itself
can
be found by <A
HREF="/tla/ws-at.html">clicking
here</A>.&nbsp;

This was a routine exercise for me, as it would have been for anyone
with a moderate amount of experience specifying concurrent systems.&nbsp;
Using TLA+ for the first time was a learning experience
for Fritz.&nbsp; It was a brand new world for Jim and Dave, who had never
been exposed to formal methods before.&nbsp; They were happy with the
results.&nbsp; Dave began writing specifications by himself, and has become
something of a TLA+ guru for the Microsoft networking
group.&nbsp; We submitted this paper to WS-FM 2004 as a way of introducing
the Web services community to formal methods and TLA+.&nbsp;

<p>
References [4] and [5] are no longer at the URLs given in the reference
list.&nbsp; Reference [5] no longer seems to be on the Web.&nbsp; Reference [4]
seems to be the document now available
<a 
  href="https://web.archive.org/web/20070315194916/http://msdn2.microsoft.com/en-us/library/ms951228.aspx">here</a>.&nbsp;
However, that paper seems to be a revised version of the one on which
we based our formal spec, and it could have been influenced by our spec.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "web-dsn-submission"><B>Cheap Paxos</B>&nbsp; (with Mike
Massa)<BR><i>Proceedings of the International Conference on Dependable
Systems and Networks (DSN 2004)</i> held in Florence in June-July 2004.<BR>
<A HREF = "web-dsn-submission.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "web-dsn-submission.ps.gz">gzipped Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "web-dsn-submission.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright � 2004 Personal use of this material is permitted.
However, permission to reprint/republish this
material for advertising or promotional purposes or
for creating new collective works for resale or
redistribution to servers or lists, or to reuse any
copyrighted component of this work in other works
must be obtained from the IEEE.
</FONT><HR>


A system that can tolerate a single non-Byzantine failure requires
three processors.&nbsp; It has long been realized that only two of those
processors need to maintain the system state, but the third processor
must take part in every decision to maintain consistency.&nbsp; Mike Massa,
at the time an engineer at Microsoft, observed that if we weaken the
fault-tolerance guarantee, then the third processor needs to be used
only in the case of failure or repair of one of the other two
processors.&nbsp; The third processor can then be a less powerful machine
or a process run occasionally on a computer devoted to other tasks.&nbsp; I
generalized his idea to a variation of the Paxos algorithm of
<A HREF = "#lamport-paxos">[123]</A> called Cheap Paxos that tolerates up to
<i>f</i> failures with <i>f</i>+1 main processors and
<i>f</i> auxiliary ones.&nbsp; A paper on this algorithm was rejected from
the PODC and DISC conferences.&nbsp; Most of the referees thought that it
just presented the old idea that only the main processors need to
maintain the system state, not realizing that it differed from the old
approach because the remaining <i>f</i> processors need not take part
in every decision.&nbsp;

One review contained a silly assertion that it was easy to solve the
problem in a certain way.&nbsp; When trying to prove his or her assertion
false, I discovered a somewhat simpler version of Cheap Paxos that
achieved the same result as the original version.&nbsp; (The new algorithm
wasn't at all what the referee said should be done, which was
impossible.)&nbsp; This paper describes the simpler algorithm.&nbsp; The
original algorithm actually has some advantage over the new one, but
it remains unpublished.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "combining"><B>Implementing and Combining Specifications</B><BR>Unpublished
note (September 2004).<BR>
<A HREF = "combining.pdf">PDF</A><HR>


I wrote this note to help some Microsoft developers understand how
they could write TLA+ specifications of the software they
were designing.&nbsp; Their biggest problem was figuring out how to specify
an API (Application Programming Interface) in TLA+, since
there were no published examples of such specifications.&nbsp; The note
also explains two other things they didn't understand: what it means
to implement an API specification and how to use an API specification
in specifying a system that calls the API.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "lower-bound">
<B>Lower Bounds for Asynchronous Consensus</B><BR><i>Distributed Computing 19</i>, 2 (2006), 104-125.&nbsp; Also appeared as
Microsoft Research Technical
Report MSR-TR-2004-72 (July 2004, revised August 2005).<BR>
<A HREF = "lower-bound.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
2006 by Springer-Verlag.</FONT><HR>


This paper contains the precise statements and proofs of the results
announced in <A HREF = "#bertinoro">[144]</A> for the non-Byzantine case.&nbsp; It also
includes another result showing that a completely general consensus
algorithm cannot be faster than the Paxos algorithm of
<A HREF = "#lamport-paxos">[123]</A> in the presence of conflicting requests.&nbsp;
However, there are two exceptional cases in which this result does not
hold, and the paper presents potentially useful optimal algorithms for
both cases.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "generalized"><B>Generalized Consensus and Paxos</B><BR>Microsoft Research Technical
Report MSR-TR-2005-33 (15 March 2005).<BR><A HRef =
"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2005-33.pdf">Available
On-Line</A><HR>


In <A HREF = "#lower-bound">[154]</A>, I proved lower bounds for the number of
message delays required to reach consensus.&nbsp; I showed that the best
algorithms can reach consensus in the normal case in 2 message delays.&nbsp;
This result in turn led me to a new version of the Paxos algorithm of
<A HREF = "#lamport-paxos">[123]</A> called Fast Paxos, described in
<A HREF = "#fast-paxos">[159]</A>,  that achieves this bound.&nbsp;
However, Fast Paxos can take 3 message delays in the event of
conflict, when two values are proposed concurrently.&nbsp; I showed in
<A HREF = "#lower-bound">[154]</A> that this was unavoidable in a general algorithm,
so this seemed to be the last word.&nbsp;

<P>

It then occurred to me that, in the state-machine approach (introduced
in <A HREF = "#time-clocks">[27]</A>), such conflicting proposals arise because two
different commands are issued concurrently by two clients, and both
are proposed as command number <i>i</i>.&nbsp; This conflict is necessary
only if the two proposed commands do not commute.&nbsp; If they do, then
there is no need to order them.&nbsp; This led me to a new kind of
agreement problem that requires dynamically changing agreement on a
growing partially ordered set of commands.&nbsp; I realized that
generalizing from partially ordered sets of commands to a new
mathematical structure I call a <i>c-struct</i> leads to a generalized
consensus problem that covers both ordinary consensus and this new
dynamic agreement problem.&nbsp; I also realized that Fast Paxos can be
generalized to solve this new problem.&nbsp; I wrote up these results in
March 2004.&nbsp; However, I was in the embarrassing position of having
written a paper generalizing Fast Paxos without having written a paper
about Fast Paxos.&nbsp; So, I just let the paper sit on my disk.&nbsp;

<p>

I was invited to give a keynote address at the 2004 DSN conference,
and I decided to talk about fast and generalized Paxos.&nbsp; Fernando
Pedone came up after my talk and introduced himself.&nbsp; He said that he
and Andr&#233; Schiper had already published a paper with
the same generalization from the command sequences of the
state-machine approach to partially ordered sets of commands, together
with an algorithm that achieved the same optimal number of message
delays in the absence of conflict.&nbsp; It turns out that their algorithm
is different from the generalized Paxos algorithm.&nbsp; There are cases in
which generalized Paxos takes only 2 message delays while their
algorithm takes 3.&nbsp; But the difference in efficiency between the two
algorithms is insignificant.&nbsp; The important difference is that
generalized Paxos is more elegant.&nbsp;

<p>

I've been sitting on this paper for so long because it doesn't seem
right to publish a paper on a generalization of Fast Paxos before
publishing something about Fast Paxos itself.&nbsp; Since generalized Paxos
is a generalization, this paper also explains Fast Paxos.&nbsp; But
people's minds don't work that way.&nbsp; They need to understand Fast
Paxos before they can really understand its generalization.&nbsp; So, I
figured I would turn this paper into the second part of a long paper
or monograph whose first part explains Fast Paxos.&nbsp; However, in recent
years I've been discovering new Paxonian results faster than I can
write them up.&nbsp; It therefore seems silly not to release a paper that
I've already written about one of those results.&nbsp; So, I added a brief
discussion of the Pedone-Schiper result and a citation to
<A HREF = "#lower-bound">[154]</A> and am posting the paper here.&nbsp; Now that I have
written the Fast Paxos paper and submitted it for publication, I may
rewrite this paper as part two of that one.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "real-simple"><B>Real Time is Really Simple</B><BR>Microsoft Research Technical
Report MSR-TR-2005-30 (4 March 2005).&nbsp; Rejected by <i>Formal
Methods in Systems Design</i>.<BR><A HRef =
"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2005-30.pdf">Available
On-Line</A><HR>


It should be quite obvious that no special logic or language is needed
to write or reason about real-time specifications.&nbsp; There's a simple
way to do it: just use a variable to represent time.&nbsp;
Mart&#237;n Abadi and I showed in
<A HREF = "#lamport-old-fashioned">[107]</A> that this can be done very elegantly in
TLA.&nbsp; A simpler, more straightforward approach works with
any sensible formal method, but it's too simple and obvious to
publish.&nbsp; So instead, hundreds of papers and theses have been written
about special real-time logics and languages--even though, for most
purposes, there's no reason to use them instead of the simple, obvious
approach.&nbsp; And since no one writes papers about the simple way of
handling real time, people seem to assume that they need to use a
real-time logic.&nbsp; Naturally, I find this rather annoying.&nbsp; So when I
heard that a computer scientist was planning to write a book about one
of these real-time logics, I decided it was time to write another
paper explaining the simple approach.&nbsp;

<P>

Since you can't publish a new paper about an old idea, no matter how
good the idea may be, I needed to find something new to add.&nbsp; The TLC
model checker provided the opportunity I needed.&nbsp; The method described
in <A HREF = "#lamport-old-fashioned">[107]</A> and <A HREF = "#tla+-book">[145]</A> is good for
specifying and reasoning about real-time systems, but it produces
specifications that TLC can't handle.&nbsp; TLC works only with the simpler
approach, so I had an excuse for a new paper.&nbsp;

<P>

There's a naive approach for checking real-time specifications with
TLC that I had thought for a while about trying.&nbsp; It involves checking
the specification for all runs up to some maximum time value that one
hopes is large enough to find any bugs.&nbsp; So I did that using as
examples two versions of Fischer's mutual exclusion protocol, which is
mentioned in the discussion of <A HREF = "#lamport-old-fashioned">[107]</A>.&nbsp;

<P>

One possible reason to use a special real-time approach is for model
checking.&nbsp; I figured that model checkers using special algorithms for
real time should do much better than this naive approach, so I wanted
some justification for using TLA+ and TLC instead.&nbsp;
Looking through the literature, I found that all the real-time model
checkers seemed to use low-level languages that could describe only
simple controllers.&nbsp; So I added the example of a distributed algorithm
that they couldn't represent.&nbsp; Then I discovered that, since the
papers describing it had been published, the Uppaal model checker had
been enhanced with new language features that enabled it to model this
algorithm.&nbsp; This left me no choice but to compare TLC with Uppaal on
the example.&nbsp;

<P>

I asked Kim Larsen of Aalborg University, the developer of Uppaal, for
help writing an Uppaal spec of the algorithm.&nbsp; Although I really did
this because I'm lazy, I could justify my request because I had never
used Uppaal and couldn't see how to write a nice model with it.&nbsp;
Larsen got his colleague Arne Skou to write a model that was quite
nice, though it did require a bit of a "hack" to encode the
high-level constructs of TLA+ in Uppaal's lower-level
language.&nbsp; Skou was helped by Larsen and his colleague, Gerd Behrmann.&nbsp;
As I expected, Uppaal was much faster than TLC--except in one
puzzling case in which Uppaal ran out of memory.&nbsp;

 <P>

I put the paper aside for a while.&nbsp; When I got back to it, I realized
that there's a simple way of using TLC to do complete checking of
these real-time specifications that is much faster than what I had
been doing.&nbsp; The idea is so simple that I figured it was well known,
and I kicked myself for not seeing it right away.&nbsp; I checked with Tom
Henzinger, who informed me that the method was known, but it had
apparently not been published.&nbsp; It seems to be an idea that is obvious
to the experts and unknown to others.&nbsp; So this provided another
incentive for publishing my paper, with a new section on how to use an
explicit-time model checker like TLC to check real-time specs.&nbsp;
Henzinger also corrected a basic misunderstanding I had about
real-time model checkers.&nbsp; Rather than trying to be faster, most of
them try to be better by using continuous time.&nbsp; He wrote:
 <blockquote><P> 
If you are happy with discrete time, I doubt you can do any better
[than my naive approach].&nbsp; Uppaal, Kronos etc. deal with
real-numbered time, and therefore rely on elaborate and expensive
clock region constructions.&nbsp;
 </P></blockquote>
I was then inspired to do some more serious data gathering.&nbsp; I
discovered the explanation of that puzzling case: Uppaal runs out of
memory when the ratio of two parameters becomes too large.&nbsp; The
results reported in the paper show that neither TLC nor Uppaal comes
out clearly better on this example.&nbsp; 


 <P>

The Uppaal distribution comes with a model of a version of Fischer's
algorithm, and I decided to get some data for that example too.&nbsp; Uppaal
did clearly better than TLC on it.&nbsp; However, I suspected that the
reason was not because real-time model checkers are better, but
because TLC is less efficient for this kind of simple algorithm than a
model checker that uses a lower-level language.&nbsp; So I got data for two
ordinary model checkers that use lower-level languages, Spin and
SMV.&nbsp; I was again lazy and got the developers of those model
checkers, Gerard Holzmann and Ken McMillan, to do all the work of
writing and checking the models.&nbsp; 


 <P>

I submitted this paper to the journal <i>Formal Methods in Systems
Design</i>.&nbsp; I thought that the part about model checking was interesting
enough to be worth putting into a separate conference paper.&nbsp; I
therefore wrote <A HREF = "#charme2005">[158]</A>, which was accepted at the 2005
Charme conference.&nbsp; However, the journal submission was rejected
because it didn't contain enough new ideas.&nbsp; 


<BR>&nbsp;<P><LI> <A NAME = "PaxosGST"><B>How Fast Can Eventual Synchrony Lead to
Consensus?</B>&nbsp; (with Partha Dutta and Rachid Guerraoui)<BR><i>Proceedings of 
the International Conference on Dependable Systems and
Networks (DSN 2005)</i>.<BR>
<A HREF = "PaxosGST.ps">Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "PaxosGST.ps.gz">gzipped Postscript</A> &nbsp;&nbsp;-&nbsp;&nbsp; 
<A HREF = "PaxosGST.pdf">PDF</A><HR>


During a visit I made to the EPFL in March 2004, Dutta and Guerraoui
explained a problem they were working on.&nbsp; Asynchronous consensus
algorithms like Paxos&nbsp;<A HREF = "#lamport-paxos">[123]</A> maintain
safety despite asynchrony, but are guaranteed to make progress only
when the system becomes synchronous--meaning that messages are
delivered in a bounded length of time.&nbsp; Dutta and Guerraoui were
looking for an algorithm that always reaches agreement within a
constant number of message delays after the system becomes
synchronous.&nbsp; This is a hard problem only if messages sent before the
system becomes synchronous can be delivered arbitrarily far in the
future.&nbsp; I took the solution they had come up with and combined it
with Paxos to obtain the algorithm described is this paper.&nbsp; It's a
nice solution to a mildly interesting theoretical problem with no
apparent practical application.&nbsp; As I recall, I wanted to include a
sentence in the paper saying this, but my co-authors sensibly pointed
out that doing so would ensure the paper's rejection.&nbsp; (My co-authors
don't remember this.)&nbsp; Computer scientists in this field must keep up
the pretense that everything they do is practical.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "charme2005"><B>Real-Time Model Checking is Really
Simple</B><BR><i>Correct Hardware Design and Verification Methods</i>
(CHARME 2005), Dominique Borrione and Wolfgang J. Paul editors,
Springer-Verlag Lecture Notes in Computer Science Volume 3725 (2005),
162-175.<BR>
<A HREF = "charme2005.pdf">PDF</A><HR>


This is an abridged version of <A HREF = "#real-simple">[156]</A>, containing only
the material on model checking.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "fast-paxos"><B>Fast Paxos</B><BR><i>Distributed Computing 19</i>, 2 (October
2006) 79-103.&nbsp; Also appeared as Microsoft Research Technical Report
MSR-TR-2005-112 (14 July 2005).&nbsp; .<BR><A HRef =
"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2005-112.pdf">Available
On-Line</A><HR>


The Paxos consensus algorithm of <A HREF = "#lamport-paxos">[123]</A> requires two
message delays between when the leader proposes a value and when other
processes learn that the value has been chosen.&nbsp; Since inventing
Paxos, I had thought that this was the optimal message delay.&nbsp;
However, sometime in late 2001 I realized that in most systems that
use consensus, values aren't picked out of the air by the system
itself; instead, they come from clients.&nbsp; When one counts the message
from the client, Paxos requires three message delays.&nbsp; This led me to
wonder whether consensus in two message delays, including the client's
message, was in fact possible.&nbsp; I proved the lower-bound result
announced in <A HREF = "#bertinoro">[144]</A> that an algorithm that can make
progress despite <i>f</i> faults and can achieve consensus in two
message delays despite <i>e</i> faults requires more than
<tt>2e+f</tt> processes.&nbsp; The proof of that result led
me pretty quickly to the Fast Paxos algorithm described here.&nbsp; Fast
Paxos generalizes the classic Paxos consensus algorithm.&nbsp; It can
switch between learning in two or three message delays depending on
how many processes are working.&nbsp; More precisely, it can achieve
learning in two message delays only in the absence of concurrent
conflicting proposals, which <A HREF = "#lower-bound">[154]</A> shows is the best a
general algorithm can do.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "celebrity"><B>Measuring Celebrity</B><BR><i>Annals
of Improbable Research</i>, Jan/Feb 2006, 14-15.<BR>
<A HREF = "celebrity.pdf">PDF</A><HR>


In September 2005, I had dinner with Andreas Podelski, who was
visiting Microsoft's Cambridge Research Laboratory.&nbsp; He mentioned that
his home page was the fourth item returned by a Google search on his
first name.&nbsp; His casual remark inspired the scientific research
reported here.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "dcas"><B>Checking a Multithreaded Algorithm with +CAL</B><BR>In
<i>Distributed Computing: 20th International Conference, DISC 2006</i>,
Shlomi Dolev, editor.&nbsp; Springer-Verlag (2006) 11-163.<BR>
<A HREF = "dcas.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
2006 by Springer-Verlag.</FONT><HR>


Yuan Yu told me about a multithreaded algorithm that was later
reported to have a bug.&nbsp; I thought that writing the algorithm in 
PlusCal (formerly called +CAL)
<A HREF = "#pluscal">[162]</A> and checking it with the TLC model checker
<A HREF = "#lamport-spec-tla-plus">[128]</A> would be a good test of the PlusCal
language.&nbsp; This is the story of what I did.&nbsp; The PlusCal specification of
the algorithm and the error trace it found are available <A HREF="../tla/dcas-example.html">here</A>.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "pluscal"><B>The PlusCal Algorithm Language</B><BR><i>Theoretical Aspects
of Computing-ICTAC 2009</i>, Martin Leucker and Carroll Morgan editors.&nbsp;
Lecture Notes in Computer Science, number 5684, 36-60.<BR>
<A HREF = "pluscal.pdf">PDF</A><HR>


PlusCal (formerly called +CAL) is an algorithm language.&nbsp; It is meant
to replace pseudo-code for writing high-level descriptions of
algorithms.&nbsp; An algorithm written in PlusCal is translated into a TLA+
specification that can be checked with the TLC model
checker&nbsp;<A HREF = "#lamport-spec-tla-plus">[128]</A>.&nbsp; This paper
describes the language and the rationale for its design.&nbsp; A language
manual and further information are available <A HREF
= "../tla/pluscal.html">here</A>.&nbsp;

<P>

An earlier version was rejected from POPL 2007.&nbsp; Based on the reviews
I received and comments from Simon Peyton-Jones, I revised the paper
and submitted it to TOPLAS, but it was again rejected.&nbsp; It may be
possible to write a paper about PlusCal that would be considered
publishable by the programming-language community.&nbsp; However, such a
paper is not the one I want to write.&nbsp; For example, two of the three
TOPLAS reviewers wanted the paper to contain a formal
semantics--something that I would expect people interested in using
PlusCal to find quite boring.&nbsp; (A formal TLA+ specification of the
semantics is available on the Web.)&nbsp; I therefore decided to publish it
as an invited paper in the ICTAC conference proceedings.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "spec-book-chap"><B>TLA+</B><BR>Chapter in <i>Software
Specification Methods: An Overview Using a Case Study</i>, Henri Habrias
and Marc Frappier, editors.&nbsp; Hermes, April 2006.<BR>
<A HREF = "spec-book-chap.pdf">PDF</A><HR>


I was asked to write a chapter for this book, which consists of a
collection of formal specifications of the same example system written
in a multitude of different formalisms.&nbsp; The system is so simple that
the specification should be trivial in any sensible formalism.&nbsp; I
bothered writing the chapter because it seemed like a good idea to
have TLA+ represented in the book, and because it wasn't
much work since I was able to copy a lot from the Z specification 
in Jonathan Bowen's chapter and
simply explain how and why the Z and TLA+ specifications
differ.&nbsp; Bowen's chapter
is available 
   <A HREF = 
         "bowen-chapter.pdf">here</A>.&nbsp;

 <P>

Because the example is so simple and involves no concurrency, its
TLA+ specification is neither interesting nor
enlightening.&nbsp; However, my comments about the specification process
may be of some interest.&nbsp; 

<BR>&nbsp;<P><LI> <A NAME = "synchronizing"><B>Implementing Dataflow With Threads</B><BR><i>Distributed Computing 21</i>, 3 (2008), 163-181.&nbsp; Also appeared as
Microsoft Research Technical
Report MSR-TR-2006-181 (December 2006)..<BR>
<A HREF = "synchronizing.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
2008 by Springer-Verlag.</FONT><HR>


In the summer of 2005, I was writing an algorithm in PlusCal
<A HREF = "#pluscal">[162]</A> and essentially needed barrier synchronization as a
primitive.&nbsp; The easiest way to do this in PlusCal was to write a little
barrier synchronization algorithm.&nbsp; I used the simplest algorithm I
could think of, in which each process maintains a single 3-valued
variable--the <i>Barrier</i>1 algorithm of this paper.&nbsp; The algorithm
seemed quite nice, and I wondered if it was new.&nbsp; A Web search
revealed that it was.&nbsp; (In 2008, Wim Hesselink informed me that he had
discovered this algorithm in 2001, but he had "published" it only in
course notes.)&nbsp; I was curious about what barrier synchronization
algorithm was used inside the Windows operating system and how it
compared with mine, so I asked Neill Clift.&nbsp; He and John Rector found
that my algorithm outperformed the one inside Windows.&nbsp; Meanwhile, I
showed my algorithm to Dahlia Malkhi, who suggested some variants,
including the paper's <i>Barrier</i>2 algorithm.&nbsp;

<P>
By around 1980, I knew that the producer/consumer algorithm introduced
in <A HREF = "#proving">[23]</A> should generalize to an arbitrary marked graph,
but I never thought it important enough to bother working out the
details.&nbsp; (Marked graphs, which specify dataflow computation, are
described in the discussion of <A HREF = "#arbiter-free">[142]</A>.)&nbsp; I realized
that these new barrier synchronization algorithms should also be
instances of that generalization.&nbsp; The fact that the barrier
algorithms worked well on a real multiprocessor made the general
algorithm seem more interesting.&nbsp; Further thought revealed that the
good performance of these barrier algorithms was not an accident.&nbsp;
They have optimal caching behavior, and that optimal behavior can be
achieved in the general case.&nbsp; All this makes the general
synchronization algorithm relevant for the coming generation of
multicore processor chips.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "commentary-web"><B>Leslie Lamport: The Specification Language
TLA+</B><BR>In 
<A HREF ="https://www.springer.com/west/home?SGWID=4-102-22-173762603-0&changeHeader=true&SHORTCUT=www.springer.com/978-3-540-74106-0"><i>Logics 
of Specification Languages</i>, Dines
Bj&#248;rner and Martin C. Henson, editors.&nbsp;
Springer (2008)</A>, 616-620.<BR>
<A HREF = "commentary-web.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
2008 by Springer-Verlag.</FONT><HR>


This is a "review" of a chapter by Stephan Merz in the same book.&nbsp;
It is mainly a brief account of the history behind TLA and
TLA+.&nbsp; It includes an interesting quote from Brannon
Battson.&nbsp; (See <A HREF = "#high-level">[147]</A>.)&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "state-machine"><B>Computation and State Machines</B><BR>Unpublished 
  (February 2008).<BR>
<A HREF = "state-machine.pdf">PDF</A><HR>


I have long thought that computer science is about concepts, not
languages.&nbsp; On a visit to the University of Lugano in 2006, the
question arose of what that implied about how computer science should
be taught.&nbsp; This is a first, tentative attempt at an answer.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "keappa08-web"><B>A TLA+ Proof System</B>&nbsp; (with Kaustuv Chaudhuri, Damien Doligez, and Stephan Merz)<BR>
 <i>Proceedings of the LPAR Workshops</i>, CEUR Workshop Proceedings
No.~418, 17-37  (2008).<BR>
<A HREF = "keappa08-web.pdf">PDF</A><HR>


This is a description of the TLA+ constructs for
writing formal proofs, and a preliminary description of the TLA proof
system.&nbsp; It includes an appendix with a formal semantics of
TLA+ proofs.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "mailbox-web"><B>The Mailbox Problem</B>&nbsp; (with Marcos Aguilera and 
Eli Gafni)<BR><i>Distributed Computing 23</i>, 2 (2010), 113-134.&nbsp; 
(A shorter version appeared
in <i>Proceedings of the 22nd International Symposium
               on Distributed Computing, (DISC 2008)</i>, 1-15.).<BR>
<A HREF = "mailbox-web.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
2010 by Springer-Verlag.</FONT><HR>


This paper addresses a little synchronization problem that I first
thought about in the 1980s.&nbsp; When Gafni visited MSR Silicon valley in
2008, I proposed it to him and we began working on it.&nbsp; I thought the
problem was unsolvable, but we began to suspect that there was a
solution.&nbsp; Gafni had an idea for an algorithm, but instead of trying
to understand the idea, I asked for an actual algorithm.&nbsp; We then went
through a series of iterations in which Gafni would propose an
algorithm, I'd code it in PlusCal (see <A HREF = "#pluscal">[162]</A>) and let the
model checker find an error trace, which I would then give to him.&nbsp;
(At some point, he learned enough PlusCal to do the coding himself,
but he never installed the TLA+ tools and I continued to run the model
checker.)&nbsp; This process stopped when Aguilera joined MSR and began
collaborating with us.&nbsp; He turned Gafni's idea into an algorithm that
the model checker approved of.&nbsp; Gafni and Aguilera came up with the
impossibility results.&nbsp; Aguilera and I did most of the actual writing,
which included working out the details of the proofs.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "teaching-concurrency"><B>Teaching Concurrency</B><BR><i>ACM SIGACT News 
Volume 40</i>,  Issue 1  (March 2009), 58-62.<BR>
<A HREF = "teaching-concurrency.pdf">PDF</A><HR>


Idit Keidar invited me to submit a note to a distributed computing
column in SIGACT News devoted to teaching concurrency.&nbsp; In an
introduction, she wrote that my note "takes a step back from the
details of where, what, and how, and makes a case for the high level
goal of teaching students how to think clearly." What does it say
about the state of computer science education that one must make a
case for teaching how to think clearly?

<BR>&nbsp;<P><LI> <A NAME = "vertical-paxos"><B>Vertical Paxos and Primary-Backup
Replication</B>&nbsp; (with Dahlia Malkhi and Lidong Zhou)<BR><i>Proceedings of the
28th Annual ACM Symposium on Principles of Distributed Computing, PODC
2009</i>, Srikanta Tirthapura and Lorenzo Alvisi, editors.&nbsp; ACM (2009),
312-313.<BR>
<A HREF = "vertical-paxos.pdf">PDF</A><HR>


This paper came out of much discussion between Malkhi, Zhou, and
myself about reconfiguration.&nbsp; Some day, what we did may result in a
long paper about state-machine reconfiguration containing these
results and others that have not yet been published.&nbsp; The ideas here
are related to the original, unpublished version of
<A HREF = "#web-dsn-submission">[152]</A>.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "deroever-festschrift"><B>Computer Science and State
Machines</B><BR><i>Concurrency, Compositionality, and Correctness (Essays
in Honor of Willem-Paul de Roever).</i> Dennis Dams, Ulrich Hannemann,
and Martin Steffen editors.&nbsp; Lecture Notes in Computer Science, number
5930 (2010), 60-65.<BR>
<A HREF = "deroever-festschrift.pdf">PDF</A><HR>


This is the six-page version of <A HREF = "#state-machine">[166]</A>.&nbsp; I think it is
also the first place I have mentioned the Whorfian syndrome in print.&nbsp;
It is structured around a lovely simple example in which an important
hardware protocol is derived from a trivial specification by
substituting an expression for the specification's variable.&nbsp; This
example is supporting evidence for the thesis of
<A HREF = "#teaching-concurrency">[169]</A> that computation should be described
with mathematics.&nbsp; (Substitution of an expression for a variable is an
elementary operation of mathematics, but is meaningless in a
programming language.)&nbsp; 

<BR>&nbsp;<P><LI> <A NAME = "reconfiguration-tutorial"><B>Reconfiguring a State Machine</B>&nbsp; (with Dahlia Malkhi and Lidong Zhou)<BR><i>ACM SIGACT News 
Volume 41</i>,  Issue 1  (March 2010)..<BR>
<A HREF = "reconfiguration-tutorial.pdf">PDF</A><HR>



This paper describes several methods of reconfiguring a state machine.&nbsp;
All but one of them can be fairly easily derived from the basic
state-machine reconfiguration method presented in the Paxos paper
<A HREF = "#lamport-paxos">[123]</A>.&nbsp; We felt that it was worthwhile publishing
them because few people seemed to understand the basic method.&nbsp; (The
basic method has a parameter &alpha; that I took
to be 3 in <A HREF = "#lamport-paxos">[123]</A> because I stupidly thought that
everyone would realize that the 3 could be any positive integer.)&nbsp;
The one new algorithm, here called the "brick wall" method,
is just sketched.&nbsp; It is described in detail in <A HREF = "#stoppable">[173]</A>.&nbsp;

<p> 

This paper was rejected by the 2008 PODC conference.&nbsp; Idit Keidar
invited us to submit it as a tutorial to her distributed computing
column in SIGACT News.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "stoppable"><B>Stoppable Paxos</B>&nbsp; (with Dahlia Malkhi and Lidong Zhou)<BR>Unpublished
(April 2009).<BR>
<A HREF = "stoppable.pdf">PDF</A><HR>


This paper contains a complete description and proof of the "brick
wall" algorithm that was sketched in
<A HREF = "#reconfiguration-tutorial">[172]</A>.&nbsp; It was rejected from the 2008 DISC
conference.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "pnueli"><B>Temporal Logic: The Lesser of Three
  Evils</B><BR>Unpublished (April 2010).<BR>
<A HREF = "pnueli.pdf">PDF</A><HR>


This paper was written for a symposium in memory of Amir Pnueli held
at New York University in May of 2010.&nbsp; I liked and greatly respected
Amir, and I made the mistake of writing a paper he would have enjoyed.&nbsp;
This was a mistake because a memorial is not for the dead, but for the
people who loved him.&nbsp; Those people found the paper unsuitable for a
memorial, and I don't question their judgement.&nbsp; I hope enough time
has passed so that people can now read the paper and see in it a small
indication of my gratitude to Amir for what he did, as I would have
expressed it to him.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "verifying-safety"><B>Verifying Safety Properties With the 
  TLA+ Proof System</B>&nbsp; (with Kaustuv Chaudhuri et al.)<BR>Fifth International Joint 
  Conference on Automated Reasoning (IJCAR), Edinburgh, UK. (July 2010)
  142-148.<BR><A 
 href="https://arxiv.org/pdf/1011.2560.pdf">Available 
     from Math arXiv.</A><HR>


This was essentially a progress report on the development of the TLAPS
proof system.&nbsp; I believe it describes the state of the system, largely
implemented by Chaudhuri, at the end of his post-doc position on the
project. 

<BR>&nbsp;<P><LI> <A NAME = "web-byzpaxos"><B>Byzantizing Paxos by Refinement</B><BR><i>Distributed 
Computing: 25th International Symposium: DISC 2011</i>,
David Peleg, editor.&nbsp; Springer-Verlag (2011) 211-224.<BR>
<A HREF = "web-byzpaxos.pdf">PDF</A><HR>


The Castro-Liskov algorithm (Miguel Castro and Barbara Liskov,
<i>Practical Byzantine Fault Tolerance and Proactive Recovery</i>,
TOCS 20:4 [2002] 398-461) intuitively seems like a modification of
Paxos <A HREF = "#lamport-paxos">[123]</A> to handle Byzantine failures, using
3n+1 processes instead of 2n+1 to handle
n failures.&nbsp; In 2003 I realized that a nice way to think about
the algorithm is that 2n+1 non-faulty processes are
trying to implement ordinary Paxos in the presence of n
malicious processes--each good process not knowing which of the other
processes are malicious.&nbsp; Although I mentioned the idea in lectures, I
didn't work out the details.&nbsp;

<p>

The development 
  of<a href="https://tla.msr-inria.inria.fr/tlaps/content/Home.html
">
TLAPS, the TLA+ proof system</a>,
inspired me to write formal TLA+ specifications of the
two algorithms and a TLAPS-checked proof that the Castro-Liskov
algorithm refines ordinary Paxos.&nbsp; This paper describes the results.&nbsp;
The complete specifications and proof are
available
  <!-at
   \url{https://lamport.azurewebsites.net//tla/byzpaxos.html}%
   -><a href="../tla/byzpaxos.html">here</a>.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "disc-leaderless-web"><B>Leaderless Byzantine Paxos</B><BR><i>Distributed 
Computing: 25th International Symposium: DISC 2011</i>,
David Peleg, editor.&nbsp; Springer-Verlag (2011) 141-142.<BR>
<A HREF = "disc-leaderless-web.pdf">PDF</A><HR>


This two-page note describes a simple idea that I had in 2005.&nbsp; I have
found the Castro-Liskov algorithm and other "Byzantine Paxos"
algorithms unsatisfactory because they use a leader and, for progress,
they require detecting and removing a malicious leader.&nbsp; My idea was
to eliminate the leader by using a synchronous Byzantine agreement
algorithm to implement a virtual leader.&nbsp; The note is too short to
discuss the practical details, but they seem to be straightforward.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "euclid"><B>Euclid Writes an Algorithm: A
Fairytale</B><BR><i>International Journal of Software and Informatics 5</i>,
1-2 (2011) Part 1, 7-20.<BR>
<A HREF = "euclid.pdf">PDF</A><HR>


This was an invited paper for a festschrift in honor of Manfred Broy's
60th birthday.&nbsp; It's a whimsical introduction to TLA+,
including proofs.&nbsp; Judged as literature, it's probably the best thing
I have ever written.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "proof"><B>How to Write a 21st Century Proof</B><BR><i>Journal of 
Fixed Point Theory and Applications</i> doi:10.1007/s11784-012-0071-6 
(6 March 2012)..<BR>
<A HREF = "proof.pdf">PDF</A>
<BR><FONT SIZE = -2>Copyright 
2012 by Springer-Verlag.</FONT><HR>


I was invited to give a talk at a celebration of the 80th birthday of
Richard Palais.&nbsp; It was at a celebration of his 60th birthday that I
first gave a talk about how to write a proof--a talk that led to
<A HREF = "#lamport-how-to-write">[102]</A>.&nbsp; So, I thought it would be fun to give
the same talk, updated to reflect my 20 years of experience writing
structured proofs.&nbsp; The talk was received much more calmly than my
earlier one, and the mathematicians were open to considering that I
might have something interesting to say about writing proofs.&nbsp; Perhaps
in the last 20 years I have learned to be more persuasive, or perhaps
the mathematicians in the audience had just grown older and calmer.&nbsp;
In any case, they were still not ready to try changing how they write
their own proofs.&nbsp;

<p>

My experience preparing and giving the talk made me realize it was
time for a new paper on the subject.&nbsp; This paper is built around a
simple example--a lemma from Michael Spivak's calculus text.&nbsp; I tried
to show how a mathematician can easily transform the proofs she now
writes into structured proofs.&nbsp; The paper also briefly describes how
formal structured proofs are written in TLA+, and an
appendix contains a machine-checked proof of Spivak's lemma.&nbsp; While
mathematicians will not write formal proofs in the
foreseeable future, I argue that learning how to write them is a good
way to learn how to write rigorous informal proofs.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "tlaps"><B>TLA+ Proofs</B>&nbsp; (with Denis Cousineau et al.)<BR><i>Proceedings of
the 18th International Symposium on Formal Methods (FM 2012)</i>, 
Dimitra Giannakopoulou and Dominique Mery, editors.&nbsp;
 Springer-Verlag Lecture Notes in Computer Science,
   Volume 7436 (2012) 147-154.<BR>
<A HREF = "tlaps.pdf">PDF</A><HR>


This is a short paper describing TLAPS, the TLA+ proof system being
developed at the Microsoft Research-INRIA Joint Centre.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "wired"><B>Why We Should Build 
Software Like We Build Houses</B><BR>Published on-line by <i>Wired</i> 
in January 2013, but no longer available on their site.<BR>
<A HREF = "wired.pdf">PDF</A><HR>


I was approached by an editor at <i>Wired</i> to write an article for
them.&nbsp; After a great deal of discussion and rewriting, we finally came
up with this compromise between what <i>Wired</i> wanted and what I
was willing to sign my name to.&nbsp; The basic message of the piece is
that, when programming, it's a good idea to think before you code.&nbsp; I
was surprised when the posted comments revealed that this is a
controversial statement.&nbsp; The version contained here is the latest one
that I have and is probably what was published by <i>Wired</i>.&nbsp; I
later expanded it to paper number <A HREF = "#blueprints">[184]</A>.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "adaptive"><B>Adaptive Register Allocation with a Linear Number
of Registers</B>&nbsp; (with Delporte-Gallet et al.)<BR><i> Proceedings of the 27th
International Symposium on Distributed Computing (DISC 2013)</i> 269-283.<BR>
<A HREF = "adaptive.pdf">PDF</A><HR>


I had little to do with the algorithms in this paper.&nbsp; I was mainly
responsible for writing them in PlusCal and getting a 
 <A HREF ="/tla/snapshot.html">TLA+ proof 
 </A>written.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "coalescing"><B>Coalescing: Syntactic Abstraction for Reasoning in
 First-Order Modal Logics</B>&nbsp; (with Damien Doligez et al.)<BR><i>Proceedings of the Workshop
 on Automated Reasoning in Quantified Non-Classical Logics (ARNL 2014)</i>.<BR>
<A HREF = "coalescing.pdf">PDF</A><HR>


When using a theorem prover that reasons about a certain class of
mathematical operators to reason about expressions containing a larger
class of operators, we have to hide from the prover the operators it
can't handle.&nbsp; For example, a prover for ordinary mathematics would 
(incorrectly) deduce that the TLA+ action formula 
 
  <code>(x = y) => (x' = y')</code>
is a tautology if it were to treat priming 
 (&nbsp;<code>'</code>&nbsp;) as an ordinary
mathematical operator.&nbsp; We call this kind of hiding 
 <i>coalescing</i>.&nbsp; 
This paper shows that coalescing is somewhat more subtle than one
might think, and explains how to do it correctly in some important
cases.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "blueprints"><B>Who Builds a House without Drawing Blueprints?</B><BR><i>Communications of the ACM 58</i>, 4 (April 2015), 38-41.<BR><A Href = "https://cacm.acm.org/magazines/2015/4/184705-who-builds-a-house-without-drawing-blueprints/fulltext">Available on ACM
web site.</A><HR>


Discusses informal specification.&nbsp; It is an
expanded version of <A HREF = "#wired">[181]</A>.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "turing"><B>The Computer Science of Concurrency: The Early Years</B><BR>
<i>Communications of the ACM, June 2015</i>, Vol. 58 No. 6, Pages 71-76.<BR>
<A HREF = "turing.pdf">PDF</A><HR>


This is the written version of my Turing lecture, which I gave at
the PODC conference in Paris in July, 2014.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "auxiliary"><B>Auxiliary Variables in TLA+</B>&nbsp; (with Stephan
Merz)<BR>Unpublished, arXiv paper 1703.05121 
(May 2017).<BR>
<A HREF = "auxiliary.pdf">PDF</A><HR>


Although most of the ideas were already well-established at the time,
paper <A HREF = "#abadi-existence">[92]</A> has become the standard reference on
refinement mappings and auxiliary variables--variables added to a
specification to permit constructing a refinement mapping under which
it implements a higher-level specification.&nbsp; A major new idea that
paper introduced was prophecy variables, which are auxiliary variables
that predict the future.&nbsp; Prophecy variables seemed very elegant,
being history variables run backwards.&nbsp; They had one serious problem:
they were extremely difficult even for me to use in practice.&nbsp;

<p>


A 2015 paper by Mart&#237;n Abadi
introducing a method for making prophecy variables easier to use
inspired me to take a new look at them.&nbsp; I came up with a completely
new kind of prophecy variable--one that I find quite easy to use.&nbsp; I
believe that Abadi and I did not discover this kind of variable in
1988 because I had not yet invented TLA+, so we were
thinking only in terms of states and not in terms of actions
(predicates on pairs of states).&nbsp; After I had the initial idea, I
asked Stephan Merz to work with me to get the details right.&nbsp;

<p>

This paper is written for engineers.&nbsp; It contains a TLA+
module for each type of auxiliary variable and shows how to use
operators defined in that module to add such an auxiliary variable to
a specification.&nbsp; These modules, along with the modules for all the
examples in the paper, are <A HREF ="https://lamport.azurewebsites.net/tla/auxiliary/auxiliary.html">available on the
Web</A>.&nbsp;

<p>

There are three types of auxiliary variables, each with its own
module.&nbsp; In addition to history variables that record the past and
prophecy variables that predict the future, there are stuttering
variables that add stuttering steps (steps that do nothing).&nbsp; Paper
<A HREF = "#abadi-existence">[92]</A> used prophecy variables to add stuttering
steps, but we have long known that it's better to use a separate kind
of variable for that purpose.&nbsp;

<p>

The auxiliary variables described in the paper can be defined
semantically; they are not specific to the TLA+ language.&nbsp;
We hope in a later paper to define them in a language-independent
fashion, and to prove a completeness result similar to that of paper
<A HREF = "#abadi-existence">[92]</A>.&nbsp; We believe that the new kind of prophecy
variable is more powerful than the original one, and that completeness
does not require the hypothesis of finite internal nondeterminism
needed for the original prophecy variables.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "eatcs"><B>If You�re Not Writing a Program,
 Don�t Use a Programming Language</B><BR><i>Bulletin of EATCS (The 
 European Association for Theoretical Computer Science)</i> No. 125, June 2018.<BR><A HREF="http://bulletin.eatcs.org/index.php/beatcs/article/view/539">web publication</A><HR>


In January, 2018 I was invited to contribute an article to the
Distributed Computing column of the EATCS Bulletin.&nbsp; I replied that I
had nothing to say on that subject to the EATCS community, and I offered
instead to write something along the lines of
<A HREF = "#teaching-concurrency">[169]</A>.&nbsp; That offer was accepted, and I wrote
this article.&nbsp; It discusses in detail what I could only sketch in my
earlier article: how to describe algorithms with mathematics.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "recursive-ops"><B>Recursive Operator Definitions</B>&nbsp; (with Georges 
 Gonthier)<BR><i>Inria Research Report No. 9341</i>, May 2020.<BR>
<A HREF = "recursive-ops.pdf">PDF</A><HR>


We can recursively define the cardinality of a finite set by defining
the cardinality of the empty set to be 0 and the cardinality of any
other finite set to be 1 plus the cardinality of the set obtained from
it by removing one of its elements.&nbsp; Mathematicians will call
<i>cardinality</i> a function, until you remind them that a function
has a domain that must be a set, and the collection of all finite sets
is not a set.&nbsp; <i>Cardinality</i> is not a function; I call it an
operator.&nbsp; The TLA+ specification language originally allowed
recursive function definitions; it did not allow recursive operator
definitions because I didn't know how to define their semantics, and
apparently no one else did either.&nbsp; To appreciate the problem,
consider the recursive definition of an operator <i>Op</i> asserting,
for any <i>v</i>, that <i>Op</i>(<i>v</i>) equals some value not equal to 
<i>Op</i>(<i>v</i>).&nbsp; (It's easy to write this
definition in TLA+.)&nbsp; In 2005, I had an idea of how to define a
semantics for recursive operator definitions, and Georges Gonthier
converted my idea into a correct definition.&nbsp; In 2019 I decided that
the definition should be documented, and I persuaded Georges to join
me in writing this paper.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "toolbox"><B>The TLA+ Toolbox</B>&nbsp; (with Markus Kuppe and Daniel 
Ricketts)<BR>Proceedings of the Fifth Workshop on Formal Integrated Development 
               Environments (October 2019), pages 50-62.<BR>
<A HREF = "toolbox.pdf">PDF</A><HR>


The TLA+ Toolbox is an integrated development environment for the TLA+
tools.&nbsp; It was originally written by Simon Zambrovski under my
supervision.&nbsp; Shortly thereafter, Daniel Ricketts added a nice
interface for running the TLAPS prover from the Toolbox.&nbsp; In recent
years, Markus Kuppe has been improving the Toolbox.&nbsp; Kuppe and I
decided it was time to write a paper describing the most novel
features of the Toolbox.&nbsp; Kuppe wrote almost all of the paper, and
Ricketts and I edited what he had written. 


<BR>&nbsp;<P><LI> <A NAME = "simple"><B>Prophecy Made Simple</B>&nbsp; (with Stephan Merz)<BR><i>ACM Transactions on Programming Languages and Systems 44</i>, 2 (June 2022), 1-27. Published version 
<a 
   href="https://dl.acm.org/doi/10.1145/3492545">available here.<BR>
<A HREF = "simple.pdf">PDF</A><HR>


This paper provides what we hope is a simple explanation of the ideas
in <A HREF = "#auxiliary">[186]</A>.&nbsp; That paper contained an account of auxiliary
variables that was buried under a mountain of details on how to use
them in actual TLA+ specifications.&nbsp; Its most important contribution
was a new kind of prophecy variable that is more powerful and much
simpler to use than the original prophecy variable introduced in
<A HREF = "#abadi-existence">[92]</A>.&nbsp; This paper explains auxiliary variables
through a series of simple examples, concentrating on prophecy
variables.&nbsp; In addition, it explains prophecy constants-an idea
introduced by Hesselink in 2005-and sketches a completeness result.&nbsp;
It assumes no prior knowledge of TLA or auxiliary variables.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "hyper2"><B>Verifying Hyperproperties with TLA</B>&nbsp; (with Fred B.&nbsp;
Schneider)<BR>To appear in the <i>Proceedings of the 34th {IEEE}
Computer Security Foundations Symposium, {CSF} 2021</i>.<BR>
<A HREF = "hyper2.pdf">PDF</A><HR>


At a workshop in Norway in 2018, I heard Fred Schneider give a talk
about hyperproperties.&nbsp; An ordinary property is a predicate on system
executions; a hyperproperty is a predicate on sets of executions.&nbsp;
Hyperproperties have been used to express security
conditions--usually ones that forbid information leaks.&nbsp;

<P>A system specification can be written as a TLA formula that
is true just on possible executions of the system.&nbsp; When listening to
Schneider's talk, it occurred to me that for a hyperproperty depending
on only a finite number of executions, the assertion that a system
satisfies the hyperproperty can can be written as a TLA formula
containing multiple copies of the system's specification.&nbsp;

<P>Schneider and I spent part of the following two years
figuring out how to make the idea work and writing this paper about
it.&nbsp; We found that TLA and the TLA+ tools can in principle check if
high-level system designs satisfy all the security hyperproperties we
saw in the literature.&nbsp; However, TLA had to be used in a new way that
was not needed for verifying ordinary properties.&nbsp;
This paper won the USA National Security Agency's 
2022 Annual Best Scientific Cybersecurity Paper Competition.&nbsp;

<P>TLA+ files (and their pretty-printed versions)  containing 
the examples in the paper and their verifications are available
<a 
href="https://lamport.azurewebsites.net/tla/hyperproperties/hyper.html">here</a>.&nbsp;



<BR>&nbsp;<P><LI> <A NAME = "dbakery-cacm"><B>Deconstructing the Bakery to Build a Distributed
State Machine</B><BR><i>Communications of the ACM 65</i>, 9 &nbsp; (September 2022), 58-66.<BR>
<A HREF = "dbakery-cacm.pdf">PDF</A><HR>


The paper begins with: "the reader and I will journey between two
concurrent algorithms of the 1970s that are still studied today." The
journey goes from <A HREF = "#bakery">[12]</A> to an algorithm in <A HREF = "#time-clocks">[27]</A>,
each step being an algorithm derived from the preceding one.&nbsp;
I found
it a pleasant journey; I hope others will too.&nbsp;

<p>

These are mutual exclusion algorithms.&nbsp; A version of the paper
containing an appendix proving that an algorithm which generalizes the
starting point satisfies mutual exclusion is available
  
 <a HREF="dbakery-complete.pdf">here</a>.&nbsp;
 
<BR>&nbsp;<P><LI> <A NAME = "dijkstra"><B>Concurrent Algorithms</B><BR>Chapter 4 in <i>Edsger Wybe Dijkstra: His 
 Life, Work, and Legacy</i>, edited by Krzysztof R. Apt and Tony Hoare.&nbsp;
 ACM Books, volume 45.&nbsp; ACM / Morgan  Claypoo.<BR>
<A HREF = "dijkstra.pdf">PDF</A><HR>
 

This is a chapter in a biography of Edsger Dijkstra--one of
an ACM series of scientific biographies of Turing Award winners.&nbsp; The
chapter is a survey of his concurrent algorithms.&nbsp;
Writing it made me realize how great a computer scientist he was, and
how much I learned from his work.&nbsp; Any study of concurrency should
include these algorithms.&nbsp; I hope that the work done in the decades since
Dijkstra invented the algorithms enabled me to make them and
their significance easier to understand.&nbsp;


<BR>&nbsp;<P><LI> <A NAME = "rigor"><B>Making Math More Rigorous</B><BR>Section 4 of <i>Mathematical Proof Between Generations</i>
by 
Jonas Bayer et al. in <i>Notices of the American Mathematical Society</i>,
Volume~71, No.~1 (January 2024).<BR>
<A HREF = "rigor.pdf">PDF</A><HR>


I was invited along with nine other people to submit a section of the
paper in the <i>AMS Notices</i>.&nbsp; The paper is mainly about the
potential influence on mathematics of machine-checked proofs.&nbsp; I
contributed this section essentially as an advertisement for
papers <A HREF = "#lamport-how-to-write">[102]</A> and <A HREF = "#proof">[179]</A>.&nbsp; I don't
expect it to achieve anything, since I've given up trying to convince
mathematicians to write structured proofs.&nbsp; That they make it harder
to publish an incorrect result is too much of a disincentive.&nbsp; I
apparently didn't pay too much attention to what I had written,
because formulas (1) and (2) in the published version were egregiously
wrong; I corrected them in this version.&nbsp;
 
 
<BR>&nbsp;<P><LI> <A NAME = "statistics"><B>Some Data on the Frequency of Errors in Mathematics
   Papers</B><BR>Rejected by <i>The American Mathematical
Monthly</i> and the <i>Proceedings of the National Academy of
Sciences</i> (December 2023).<BR>
<A HREF = "statistics.pdf">PDF</A><HR>


This paper reports that in a non-random selection of 84 papers
published in one small field of mathematics, 1/3 of them contained
serious errors.&nbsp; The data are meager, but a Web search indicates they
are the only data on the subject.&nbsp;

<P>

While the data proves nothing, it does strongly suggest that there are
lots of errors in published mathematics paper.&nbsp; Since the frequency of
such errors is an interesting topic, and this very short paper (less
than one page in a journal format) presents the only data on this
topic, I think that it's worth publishing.&nbsp;

<P>

The reasons for rejection given by the editors and reviewers seemed
reasonable.&nbsp; However, the reaction to my proposals in
<A HREF = "#lamport-how-to-write">[102]</A> and <A HREF = "#proof">[179]</A> for writing proofs
taught me that mathematicians become irrational when I point out that
the proofs they write are not the paradigms of logical rigor they
pretend them to be.&nbsp; So, I am suspicious.&nbsp; I will let the reader
decide whether this paper should have been rejected.&nbsp;

<BR>&nbsp;<P><LI> <A NAME = "science-book"><B>A Science of Concurrent Programs</B><BR>To be published by
  Cambridge University Press.<BR><A HREF ="../tla/science-book.html">Available
   On-line</A><HR>


This book explains the scientific principles underlying the
 TLA+ 
language and its use for specifying and reasoning about concurrent
programs and systems.&nbsp; It contains a lot of math.&nbsp; All the math beyond
high school algebra is explained, but it will be tough going for
readers not familiar with the math taught in introductory university
math classes for computer science students, which includes topics like
sets and logic.&nbsp; The book contains little discussion of how
  TLA+ 
is used in practice, but it explains why 
  TLA+ 
is what it is.&nbsp;

</OL>
 
</BODY> </HTML>