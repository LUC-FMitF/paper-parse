
--- Page 1 ---
Cheap Paxos
Leslie Lamport and Mike Massa
Appeared in
The International Conference on Dependable Systems and Networks
(DSN 2004 )

--- Page 2 ---
Cheap Paxos
Leslie Lamport and Mike Massa
Microsoft
Abstract
Asynchronous algorithms for implementing a fault-
tolerant distributed system, which can make progress
despite the failure of any F processors, require 2F + 1
processors. Cheap Paxos, a variant of the Paxos al-
gorithm, guarantees liveness under the additional as-
sumption that the set of nonfaulty processors does not
“jump around” too fast, but uses only F + 1 main pro-
cessors that actually execute the system and F auxiliary
processors that are used only to handle the failure of a
main processor. The auxiliary processors take part in
reconﬁguring the system to remove the failed processor,
after which they can remain idle until another main
processor fails.
1
Introduction
The state-machine approach consists of describing a
system as a state machine that takes as input a se-
quence of client commands and produces a sequence of
states and outputs [4, 10]. The state machine is im-
plemented by a collection of servers.
It reduces the
problem of implementing a distributed system to that
of having the servers choose a sequence of commands.
Making the system reliable requires that all processors
agree on each command in the sequence, despite the
failure of some components. For asynchronous systems,
we require that consistency be maintained in the face
of any number of non-malicious (non-Byzantine) fail-
ures, and that progress be ensured when enough pro-
cessors are nonfaulty and can communicate with one
another in a timely manner [2]. The “classic” Paxos
algorithm is an eﬃcient, practical algorithm for achiev-
ing this [1, 5, 7].
Consider the problem of implementing a distributed
system that can make progress if all but one processor
is working. Previous algorithms, such as classic Paxos,
require three processors. Only two of those processors
need maintain the system state; but a third proces-
sor must participate in choosing the sequence of com-
mands. The following argument shows that this third
processor is necessary. Suppose the system is imple-
mented by only two processors, p and q, and suppose
that q fails. The requirement that the system continue
to make progress despite a single failed processor means
that p must continue operating the system. Now sup-
pose that p fails and then q is repaired. Since there
is only one failed processor, q must be able to resume
operating the system. But this is clearly impossible,
since q does not know the current state of the system
because it does not know what p did after q failed.
Some third processor is needed—for example, a disk
that can be accessed by both p and q.
Suppose we are willing to weaken the liveness re-
quirement, so that if q fails and then p fails before q is
repaired, then the system may halt until p is repaired.
Two processors are still not enough if we require that
consistency be maintained despite communication fail-
ure. With only two processors p and q, one processor
cannot distinguish failure of the other processor from
failure of the communication medium.
Since consis-
tency is lost if each processor continues operating the
system by itself, the system cannot allow each proces-
sor to continue just because it thinks that the other
processor has failed. A third processor is needed. How-
ever, that third processor does not have to participate
in choosing the sequence of commands. It must take
action only in case p or q fails, after which it does
nothing while either p or q continues to operate the
system by itself. The third processor can therefore be
a small/slow/cheap one, or a processor primarily de-
voted to other tasks.
This argument suggests that there exists a method
of implementing a one-fault tolerant system, satisfying
the consistency property of classic Paxos and a weaker
liveness property, using two main processors plus a
third auxiliary processor. This paper describes Cheap
Paxos, a generalization of such an algorithm that tol-
erates F faults with F + 1 main processors and F aux-
iliary processors. It maintains liveness under a sort of
“amoeba” assumption [3], under which the subnetwork
of working main processors does not move around too
quickly. The assumption can be described as follows.
A nonfaulty processor maintains a certain knowledge

--- Page 3 ---
of the system’s state. When a faulty processor is re-
paired, it can, in a ﬁnite length of time, re-acquire this
knowledge from any other processor that possesses it.
Liveness is maintained as long as there is at least one
main processor with knowledge of the system state and
F +1 processors (main or auxiliary) that are nonfaulty
and can communicate with one another in a timely
manner. Consistency is always maintained (assuming
non-malicious failures).
There are two threads of previous work that super-
ﬁcially resemble Cheap Paxos. The ﬁrst is the use of
main processors that are replaced by spares if they
fail [8].
Indeed, classic Paxos requires only F + 1
working processors to operate a system that tolerates
F faults; the remaining F processors can be used as
spares.
However, unlike the auxiliary processors of
Cheap Paxos, spares must have the necessary comput-
ing power to replace a failed main processor. The sec-
ond thread is the use of dynamic quorum algorithms for
maintaining multiple copies of a database. These algo-
rithms can employ “witness” processors that need not
maintain the data [9]. However, unlike the auxiliary
processors of Cheap Paxos, these witnesses participate
in each operation.
Two moderately recent developments in computing
may make Cheap Paxos useful. First, improvements
to hardware and operating systems make computers
less likely to crash. The weaker liveness guarantee of
Cheap Paxos may therefore still provide suﬃcient relia-
bility. Second, the widespread use of computers makes
it more likely that an organization will have additional
machines from which cycles can be “stolen” to imple-
ment the auxiliary processors.
One might think that the low cost of computers
would make Cheap Paxos uninteresting. However, we
have observed that people are no more willing to use
extra hardware to make a system simpler and more re-
liable than they were 40 years ago, even though that
hardware has become orders of magnitude cheaper.
The following section reviews Paxos, and Section 3
describes Cheap Paxos. The obligatory conclusion fol-
lows.
2
A Review of Paxos
The Paxos algorithm for implementing a distributed
state machine was introduced in [5]. We consider two
versions of Paxos. In the basic version, to which we
give the name Static Paxos, the set of servers is ﬁxed.
A variation that we call Dynamic Paxos, mentioned
brieﬂy in [5], uses state machine commands to change
the set of servers.
We begin by considering Static
Paxos; Dynamic Paxos is explained in Section 2.3.
2.1
The Paxos Consensus Algorithm
To implement a distributed system as a state machine,
the processors of the system must choose a sequence
of commands. This is done by executing a sequence
of instances of a consensus algorithm, the ith instance
choosing the ith command in the sequence. We now
review the Paxos consensus algorithm.
The goal of a consensus algorithm is for a collection
of processes to agree upon a value. It is most conve-
nient to phrase the consensus problem in terms of three
classes of agents: proposers that propose values, accep-
tors that cooperate to choose a single proposed value,
and learners that must learn what value has been cho-
sen [6]. A single processor can act as more than one
kind of agent. The safety properties that a consensus
algorithm must satisfy are:
Nontriviality
Only a value that has been proposed
may be chosen,
Consistency
Only a single value may be chosen.
Conservatism Only a chosen value may be learned.
There is also a liveness requirement that we do not try
to state precisely; it is discussed informally below.
The Paxos consensus algorithm has been discussed
elsewhere [1, 5, 6, 7], so we do not explain here exactly
how it works. Instead, we just describe its actions.
Paxos assumes an underlying procedure for select-
ing a leader.
Safety is guaranteed even if no leader
or multiple leaders are selected, but a unique leader
is required to ensure progress.
Proposers send their
proposals to the leader.
The consensus algorithm assumes predeﬁned sets of
acceptors called quorums.
The only requirement on
the quorums is that any two quorums have at least one
acceptor in common. Paxos also assumes a set of ballot
numbers, which for simplicity we take to be the natural
numbers. The ballot numbers are partitioned among
potential leaders, each possible leader having its own
disjoint set of ballot numbers.
The consensus algorithm has two phases, each with
two subphases. The algorithm’s actions are described
below. The algorithm sends messages between learners
and acceptors, and from acceptors to learners. Since
the same processor may be playing multiple roles, it
can send messages to itself.
Phase1a(l, b) Leader l chooses a number b from among
its ballot numbers and sends ⟨“1a”, b⟩messages to
the acceptors.
Phase1b(a, b) When acceptor a receives a ⟨“1a”, b⟩
message from a leader l, if it has not received any
message with a ballot number greater than b, then
2

--- Page 4 ---
it replies to l with a ⟨“1b”, b, . . .⟩message, where
the precise contents of the message do not concern
us. If a has received a message with ballot number
greater than b, it sends a reply to l indicating that
it is ignoring the ⟨“1a”, b⟩message. (Upon receiv-
ing that message, l will perform a Phase1a(l, b′)
action for b′ > b, if it still believes itself to be the
leader.)
Phase2a(l, b) If leader l has received ⟨“1b”, b, . . .⟩mes-
sages from a quorum of acceptors, then it sends a
⟨“2a”, b, v ⟩message to the acceptors where, de-
pending on the contents of those “1b” messages,
either:
• The value of v is determined by the “1b” mes-
sages, or
• l chooses v arbitrarily from among the pro-
posals it has received.
This action may not be performed twice for diﬀer-
ent values of v (with the same b).
Phase2b(a, b, v) If acceptor a receives a ⟨“2a”, b, v ⟩
message and it has not already received any mes-
sage with a ballot number greater than b, it sends
a ⟨“2b”, b, v ⟩message to every learner.
Learn(r, v, b) If learner r has received ⟨“2b”, b, v ⟩mes-
sages from a quorum of acceptors, then it learns
that the value v has been chosen.
In normal execution, the actions occur in the order
listed above, starting with the leader’s Phase1a ac-
tion. However, processes may fail, messages may be
lost or delivered out of order, and several processors
could simultaneously think they are the leader, caus-
ing “1a” and “2a” messages for several diﬀerent ballot
numbers to be sent concurrently. Nevertheless, the al-
gorithm maintains its three safety properties, nontrivi-
ality, consistency, and conservatism. (We are assuming
non-Byzantine failures in which a process can halt, but
does not perform incorrect actions.) Moreover, if there
is a single working processor l that believes itself to be
the leader, has received a proposal, and can communi-
cate with a quorum of acceptors, then some value will
eventually be chosen. Any learner that can communi-
cate with this quorum of acceptors will then learn the
chosen value.
We can allow failed processes to be restarted if they
have stable storage that survives a failure. Processes
must maintain the following amounts of information
in stable storage: an acceptor must keep two ballot
numbers and one proposed value, and a leader must
keep one ballot number (the largest one for which it
has performed a Phase2a action).
As described here, the algorithm never terminates.
A leader can at any time perform a Phase1a action for
a new ballot number. In an application, there will be
some point at which enough processes have learned the
chosen value, after which processes can forget all about
this instance of the algorithm, erasing any information
about it from their stable storage.
For later reference, we make the following observa-
tions.
O1. We can save messages at the cost of an extra mes-
sage delay by having a single distinguished learner
that informs the other learners when it ﬁnds out
that a value has been chosen. Acceptors then send
“2b” messages only to the distinguished learner.
In most applications, the roles of leader and dis-
tinguished learner are performed by the same pro-
cessor.
O2. A leader can send its “1a” and “2a” messages just
to a quorum of acceptors. As long as all acceptors
in that quorum are working and can communicate
with the leader and the learners, there is no need
for acceptors not in the quorum to do anything.
O3. Acceptors do not care what value is chosen. They
simply respond to “1a” and “2a” messages, using
their stable storage to ensure that, despite fail-
ures, only a single value can be chosen. However,
if an acceptor does learn what value has been cho-
sen, it can store the value in stable storage and
erase any other information it has saved there. If
the acceptor later receives a “1a” or “2a” message,
instead of performing its Phase1b or Phase2b ac-
tion, it can simply inform the leader of the chosen
value.
O4. Instead of sending the value v, the leader can send
a hash of v to some acceptors in its “2a” mes-
sages. (A hash is a function H from values to a
smaller set such that there is a negligible chance
that H (v) equals H (v ′) for two diﬀerent values
v and v ′.)
A learner will learn that v is cho-
sen if it receives “2b” messages for either v or its
hash from a quorum of acceptors, and at least
one of those messages contains v rather than its
hash. However, a leader could receive “1b” mes-
sages that tell it the hash of a value v that it must
use in its Phase2a action without telling it the ac-
tual value of v. If that happens, the leader cannot
execute its Phase2a action until it communicates
with some process that knows v.
3

--- Page 5 ---
2.2
Implementing a State Machine
In the state machine approach, a set of servers execute
commands submitted by clients. For simplicity, we as-
sume that each server keeps in stable storage the entire
sequence of state machine commands that have been
chosen so far.
In many applications, a server would
keep only a recent checkpoint of the state machine’s
state and the commands after that checkpoint.
In the traditional Paxos algorithm, the clients are
the proposers and each server acts as an acceptor, a
learner, and a potential leader in each instance of the
consensus algorithm. A quorum consists of a majority
of the servers. The leader receives client commands,
assigns each one a number, and tries to get the ith
command to be chosen by the ith instance of the Paxos
consensus algorithm.
To understand how Static Paxos works, suppose the
system has been operating for a while when the leader
fails. A new server l is then selected to be leader. Since
l is a learner, it should know most of the commands
that have already been chosen. Suppose it knows com-
mands 1–134, 138, and 139—that is, the commands
chosen in instances 1–134, 138, and 139 of the consen-
sus algorithm. (Such a gap in its knowledge is possible
because multiple instances of the consensus algorithm
can be executed concurrently.) Server l chooses a bal-
lot number b that it believes to be greater than any
ballot number used by previous leaders.
(The elec-
tion algorithm can be used to choose b as well as l.)
It then simultaneously executes Phase1a(b, l) for in-
stances 135–137 and for all instances greater than 139
of the consensus algorithm, sending “1a” messages to
all the servers. (Some of those messages are to itself,
since the leader is chosen from among the servers.) It
can obviously send these inﬁnitely many virtual mes-
sages in a single physical message.
Each server then simultaneously executes Phase1b
actions in response to those virtual “1a” messages,
sending inﬁnitely many virtual “1b” messages back to l.
Since those “1b” messages contain information only for
instances for which actions have been performed, those
virtual messages will contain only a ﬁnite amount of
information that can usually be ﬁt into a single real
message. By observation O3 above, if a server knows
that a command was already chosen by some instance,
it responds with the chosen command rather than a
“1b” message for that instance.
Suppose that, from these messages, l learned:
• The command that was chosen in instance 135
(sent by some server instead of an instance 135
“1b” message).
• Commands v 137 and v 140 that it must use as the
value v in its Phase2a(l, b) actions for instances
137 and 140, respectively.
• For instance 136 and for all instances greater than
140, it can use any proposed command v in its
Phase2a(l, b) action.
Leader l then does the following:
• It performs Phase2a(l, b) actions for instances 137
and 140, using the commands v 137 and v 140 deter-
mined by the “1b” messages it received.
• It performs the Phase2a(l, b) action for instance
136, using as the command v a special no-op state
machine command that does nothing.
• In some manner that does not concern us, it en-
sures that all servers know commands 1–135, 138,
and 139.
If a majority of the servers are working, they will per-
form Phase2b actions for instances 136, 137, and 140,
and all servers will learn the commands chosen for
all instances 1–140 of the consensus algorithm. How-
ever, even before that has happened, leader l can re-
sume normal operation. It assigns the number 141 to
the ﬁrst client command it receives, and it executes
Phase2a(l, b) for instance 141 using that command as
the value v. It assigns number 142 to the next client
command and executes Phase2a(l, b) for that instance
and that command as value v. And so on.
Since each server is a learner, it learns the sequence
of chosen commands. In most applications, the leader
will act as the distinguished learner (mentioned in ob-
servation O1 above) to which “2b” messages are sent.
Once a server has learned what command the ith com-
mand is, it can delete all other information about the
ith instance of the consensus protocol from its storage.
When a failed server is repaired, it must be brought
up to date so it knows all the commands that have
already been chosen. In principle, this is a straightfor-
ward matter of having the newly repaired server ob-
tain the information from some working server. If a
server maintains only recent commands and a check-
point of the state, then the repaired server must update
its saved checkpoint. If the state machine maintains a
large state, this must be done in such a way that only
the part of the state that has changed is sent to the
repaired server.
2.3
Dynamic Paxos
So far, we have described Static Paxos, in which the set
of acceptors and the quorums are constant and ﬁxed in
advance. A system that must continue working despite
the failure of any F processors then requires 2F + 1
servers. For example, with Static Paxos, it takes seven
4

--- Page 6 ---
servers to tolerate three failures. In many systems, the
best way to achieve the desired degree of fault tolerance
is to reconﬁgure the system to replace failed servers by
spares. With reconﬁguration, a system that uses three
active servers and two spares can tolerate a total of
three failures, if a failed server can be replaced by a
spare before another failure occurs.
Reconﬁguration
therefore allows fewer processors to tolerate the same
total number of failures, though not the same number
of simultaneous failures. (In most systems, simultane-
ous failures are much less likely than successive ones.)
In Dynamic Paxos, the set of acceptors and the
quorums are determined by the state machine itself.
Reconﬁguration is performed by state machine com-
mands. To explain how this works, let state k be the
state machine’s state after executing command k. For
k ≤0, deﬁne state k to be the initial state. For some
ﬁxed constant α, we let the acceptors and quorums
used for instance i of the consensus algorithm be deter-
mined by state i −α. Before performing any action for
instance i, a leader waits until it knows state i −α. In
other words, a leader must wait until it knows all com-
mands through command number i −α before it knows
to which acceptors it should send its “2a” messages for
the ith instance of the Paxos consensus algorithm.
As a simple example of how this might work, con-
sider a system with a ﬁxed set S of processors that can
act as servers. Let the set of servers currently execut-
ing the system (and acting as acceptors) be G, and let
a quorum consist of a majority of the processors in G.
Suppose we want a processor to be declared to have
failed or have been repaired if a majority of the pro-
cessors in G believe it has. The state machine’s state
would contain the set G together with a Boolean ar-
ray good, where good[p, q] indicates whether processor
p believes processor q is nonfaulty, for all p, q ∈S. A
processor r would issue a state machine command to
change the value of good[r, s] when it believes processor
s has failed or been repaired. Such a command would
set the new value good′ of the array good in the obvious
way, and it would set the new value G′ of G to equal the
set of all processors q ∈S such that good′[p, q] equals
true for a majority of processors p ∈G.
(Remem-
ber that a change to G caused by command number i
takes eﬀect beginning with instance i + α of the Paxos
consensus algorithm.)
In practice, deciding when to reconﬁgure a system
is not easy. Replacing servers that have not failed can
cause the system to run out of servers; but not re-
placing a failed server lowers the system’s tolerance to
additional failures.
One would probably not use so
naive an algorithm as the one just described. Instead,
one would use a more sophisticated algorithm, tuned
for the particular system. Any desired algorithm can
easily be implemented with the state machine.
When a new server is added to the system, it must
learn the current state machine state. This is essen-
tially the same problem as bringing a failed server up
to date, which is discussed in Section 2.2 above.
3
Cheap Paxos
3.1
The Algorithm
We now develop Cheap Paxos as an instance of Dy-
namic Paxos. In Cheap Paxos, we posit a system of
F +1 main processors and F auxiliary processors. The
main processors act as the servers in a distributed state
machine implementation. The auxiliary processors per-
form actions only in the event of the failure of a main
processor, after which the main processors continue to
operate the system by themselves.
The key to Cheap Paxos is observation O2. In nor-
mal operation of the Paxos consensus algorithm, the
leader sends only “2a” messages. By O2, those mes-
sages need be sent to and acted upon by only a quo-
rum of acceptors. Hence, to implement Cheap Paxos,
we use Dynamic Paxos to conﬁgure the system so that
the set of all working main processors forms a quorum.
As long as these processors continue working, they can
execute the system. If one of them fails, then the quo-
rum consisting only of main processors can no longer
succeed in choosing commands. A diﬀerent quorum,
containing one or more auxiliary processors, is then
used to (i) complete the execution of any of the in-
stances of the Paxos consensus algorithm that were in
progress when the failure occurred, and (ii) propose
and choose the necessary state machine commands to
reconﬁgure the system. The reconﬁguration removes
the failed processor and modiﬁes the set of quorums so
the remaining main processors form a quorum. These
main processors can then resume executing the system,
while the auxiliary processors once more become idle.
Cheap Paxos uses Dynamic Paxos, where the set
G of all processors that are currently acceptors is de-
termined by the state machine state. Let M be the
subset of G consisting of all the main processors in
G. We want M to be a quorum. Since the only re-
quirement on quorums is that any two of them have
a non-empty intersection, we can let M be a quorum
and let the other quorums consist of all sets contain-
ing a majority of the processors in G and at least one
processor in M . (If M contains only a single processor
p, then a quorum can consist of any set containing p,
of course including M itself.) We require that G con-
tain at least one main processor—a condition that is
satisﬁed by any sensible reconﬁguration algorithm be-
cause failure of all main processors implies that there
5

--- Page 7 ---
is no quorum of working processors, so no state ma-
chine commands can be chosen until a main processor
is repaired. (A non-sensible reconﬁguration algorithm
could gratuitously remove the last working main pro-
cessor from G.)
In normal operation, the processors in M execute
phase 2 of successive instances of the Paxos consensus
algorithm to choose the sequence of state machine com-
mands. They can perform reconﬁguration commands
to add a repaired main server to G and M . However,
if a main processor fails, then there will no longer be
a working quorum consisting only of main processors.
The following sequence of steps is then performed.
1. If the failed processor was the leader, a new leader
is selected from among the processors in M that
are still working.
2. The leader interrogates the other working main
processors to learn all chosen commands that any
main processor knows about.
3. The leader completes the execution of any in-
stances of the Paxos consensus algorithm that
were in progress when the main processor failed,
using a quorum that contains one or more aux-
iliary processors. If a new leader was chosen in
step 1, it does this as described in Section 2.2
above by choosing a new ballot number and ini-
tiating phase 1 with that ballot number for all rel-
evant instances of the consensus algorithm. If the
old leader is still working, it just sends to the aux-
iliary processors the same “2a” messages that it
had already sent to the main processors.
4. As in standard Dynamic Paxos, the working pro-
cessors propose and choose a sequence of state ma-
chine commands to reconﬁgure the system so the
failed main processor is removed from the set G of
acceptors (and hence from M ).
5. The leader proposes and gets chosen a sequence of
α no-op state machine commands. Let j be the
command number of the last of these commands.
After step 5, the new set G of acceptors chosen in step 4
is in eﬀect for subsequent instances of the consensus al-
gorithm. This means that the set M of main processors
in G constitute a quorum, so they can resume execut-
ing the system.
However, remember that the Paxos
consensus algorithm’s ability to recover from failures
rests on acceptors maintaining certain information in
stable storage.
To bound the amount of storage re-
quired by auxiliary processors, they need to be able to
forget the information they saved in steps 3–5, where
they participated in executing instances of the consen-
sus algorithm.
They can do that after the working
main processors have learned the commands chosen in
those instances. Therefore, before the system resumes
normal execution, the following steps should also be
performed.
6. The leader ensures that all processors in (the new)
M know all commands through command number
j (deﬁned in step 5).
7. The leader instructs all auxiliary processors to re-
member in stable storage that instances 1 through
j of the consensus algorithm have chosen com-
mands, and that they have performed no actions
for any other instances.
(They need not record
any of the chosen commands.) The auxiliary pro-
cessors can then erase from stable storage all in-
formation relevant to any of the ﬁrst j instances
of the consensus algorithm.
This sequence of steps describes what normally hap-
pens when a main processor fails.
A complete algo-
rithm must handle abnormal cases as well—for exam-
ple, if the leader fails while these steps are being per-
formed, or if two processors both believe they are the
leader and have diﬀerent views of what processor has
failed. But the actions performed in these steps are just
implementing Dynamic Paxos. (Steps 2 and 6 simply
disseminate knowledge of what commands have been
chosen.) The precise deﬁnition of these actions is there-
fore the same as in ordinary Paxos. The only diﬀerence
is that an auxiliary processor may not be able to re-
spond appropriately to a “1a” or “2a” message because
it has erased the needed information in step 7. In that
case, instead of replying with the chosen command as
indicated in observation O3, it must ignore the mes-
sage. (It could report to the leader why it is ignoring
the message, advising the leader to ask a main proces-
sor what command was chosen.)
The auxiliary processors are needed only in the event
of failure of one of the main processors, at which time
they must participate in the execution of only a small
number of instances of the Paxos consensus algorithm.
This would seem to imply that they do not need much
processing power. However, the consensus algorithm
requires them to write proposed commands to stable
storage. In some applications, such commands could
be quite big, and writing them to stable storage could
be expensive. If this is the case, we can apply obser-
vation O4 and have auxiliary processors receive and
store only hashes of proposed commands. Since every
quorum contains a main processor, a learner receiving
“2b” messages from a quorum must receive at least one
that contains the command rather than its hash. How-
ever, we need to prevent the problem mentioned in O4,
in which progress is prevented because a leader knows
only the hash of a value without knowing the value
6

--- Page 8 ---
itself. This is done by having the leader delay send-
ing a “2a” message with the hash of a value v to any
auxiliary processor until all the working main proces-
sors have acknowledged receipt of their “2a” messages
containing v.
As in ordinary Dynamic Paxos, we are not commit-
ted to any particular algorithm for determining that a
processor has failed or has been repaired.
Since the
reconﬁguration is performed by state machine com-
mands, any algorithm can be used. In practice, the al-
gorithm used by Cheap Paxos for determining whether
a main processor has failed will be quite diﬀerent from
that used in traditional Dynamic Paxos. In traditional
implementations of Dynamic Paxos, any majority of
acceptors constitutes a quorum, so system can continue
to make progress even if a server has failed. In that
case, one can aﬀord to wait to make sure the server
has really failed before reconﬁguring it out of the sys-
tem. But Cheap Paxos stops making progress as soon
as one main processor fails. It must therefore be more
aggressive in removing processors that may have failed.
Although this diﬀerence aﬀects the details of deciding
what processors are working, it does not change the
basic state machine algorithm.
We have been tacitly assuming that the set of all
processors (main plus auxiliary) is ﬁxed. Reconﬁgura-
tion can also be used to change the set of processors.
We are already reconﬁguring the set of acceptors to
remove failed main processors and add repaired ones.
Additional fault tolerance can be obtained by replacing
failed auxiliary processors with spares, just as in ordi-
nary Dynamic Paxos. This reconﬁguration can be per-
formed with state machine commands executed by only
the main processors; auxiliary processors need only pe-
riodically inform the main processors that that are still
working. A newly installed auxiliary processor needs
to remember in its stable storage only that it has not
performed any actions for any instances of the Paxos
consensus algorithm.
An auxiliary processor acts only as an acceptor, not
a learner, so it does not know what commands are cho-
sen.
Hence, if reconﬁguration can change the set of
auxiliary processors, an auxiliary processor does not
know whether it is an acceptor.
This makes no dif-
ference. As observed in O3, acceptors simply respond
to “1a” and “2a” messages. Only leaders and learn-
ers, which are roles played by main processors, need to
know the set of acceptors and the quorums.
3.2
Correctness of Cheap Paxos
Cheap Paxos uses the Paxos consensus algorithm to
choose commands. Its safety properties follow directly
from the safety properties of the consensus algorithm.
In particular, two diﬀerent servers can never disagree
about the value of the ith command, for any i.
The liveness properties of Cheap Paxos can also be
inferred from those of the Paxos consensus algorithm.
However, this is complicated because Cheap Paxos is
an implementation of Dynamic Paxos, in which liveness
depends on precisely how the reconﬁguration is per-
formed. For example, the system can make no progress
if a reconﬁguration selects a set of failed or nonexistent
processors as acceptors. Moreover, simply being able
to choose new commands doesn’t ensure progress. To
be able to execute the ith command, a server needs
to know not just that command, but also all previous
commands. For example, the system could make no
progress if command 1 had been chosen, but no work-
ing server knew its value. Cheap Paxos also has the
additional complication that auxiliary processors for-
get information that can be used by ordinary Paxos to
recover from certain failures.
To state the liveness property satisﬁed by Cheap
Paxos, we need some deﬁnitions. We call a set of pro-
cessors nonfaulty if they are all working and can com-
municate with one another in a timely fashion. Deﬁne
command number i to be recorded if some auxiliary
processor has stored in its stable storage the fact that
i has been chosen. (That is, the auxiliary processor
has recorded in step 7 of the reconﬁguration procedure
that all commands numbered through j have been cho-
sen, for some j≥i.) A command number i is said to be
active if i has not been recorded, but a “2a” message
has been sent for instance i of the consensus algorithm.
We deﬁne a main processor to be up-to-date if it knows
all recorded commands. (If auxiliary processors store
only hashes of values, then for a main processor p to
be up-to-date, it must also satisfy the following condi-
tion: For every active command number i and every
“2a” message sent to an auxiliary processor in instance
i of the consensus algorithm, p must have received its
corresponding “2a” message.)
We can now state the liveness property satisﬁed by
Cheap Paxos. Step 6 of the reconﬁguration procedure
assumes some method of propagating knowledge of cho-
sen commands. We assume that knowledge is contin-
ually exchanged among the main processors so that, if
p and q are main processors, p knows a command, and
the set {p, q} is nonfaulty for long enough, then q will
learn that command. The liveness property satisﬁed
by Cheap Paxos is then:
The system makes progress if there is a non-
faulty set of processors containing a unique
leader, at least one up-to-date main proces-
sor, and, for all active command numbers i, a
quorum for instance i of the consensus algo-
rithm.
7

--- Page 9 ---
Informally, we can view the set of nonfaulty main pro-
cessors as an “amoeba” that withdraws a pseudopod
when a processor fails and extends one when a pro-
cessor is repaired. It takes time for knowledge of cho-
sen commands to ﬂow into a new pseudopod. If the
amoeba were to move around too quickly, knowledge
could be lost because processors failed before their
knowledge could ﬂow to newly repaired processors.
Assuming that the total number of nonfaulty proces-
sors (main plus auxiliary) remains large enough, Cheap
Paxos guarantees that the system will continue to make
progress as long as the amoeba moves slowly enough
that such knowledge is not lost.
4
Conclusion
Cheap Paxos is a variant of the Paxos algorithm that
can make progress in the face of up to F failures by us-
ing F + 1 main processors plus F auxiliary processors.
Unlike the spare processors used in previous systems,
our auxiliary processors need do nothing except for a
brief period after a main processor fails.
The auxil-
iary processors therefore do not require nearly as much
processing power or storage as the main processors. By
using auxiliary processors, Cheap Paxos can lead to a
system that achieves greater fault tolerance than other
algorithms with the same number of main processors.
Acknowledgement
Butler Lampson pointed out to us the problem of large
commands, and its solution through observation O4.
He was also ﬁrst to observe that, in a conﬁguration with
a single working main processor, every set of processors
containing that processor is a quorum.
References
[1] R. De Prisco, B. Lampson, and N. Lynch.
Revisit-
ing the paxos algorithm.
Theoretical Comput. Sci.,
243:35–91, 2000.
[2] C. Dwork, N. Lynch, and L. Stockmeyer. Consensus in
the presence of partial synchrony. J. ACM, 35(2):288–
323, Apr. 1988.
[3] L. Lamport.
The implementation of reliable dis-
tributed multiprocess systems.
Computer Networks,
2:95–114, 1978.
[4] L. Lamport. Time, clocks, and the ordering of events
in a distributed system. Commun. ACM, 21(7):558–
565, July 1978.
[5] L. Lamport. The part-time parliament. ACM Trans.
Comput. Syst., 16(2):133–169, May 1998.
[6] L. Lamport. Paxos made simple. ACM SIGACT News
(Distributed Computing Column), 32(4 (Whole Num-
ber 121)):18–25, Dec. 2001.
[7] B. W. Lampson. How to build a highly available sys-
tem using consensus. In O. Babaoglu and K. Marzullo,
editors, Distributed Algorithms, volume 1151 of Lec-
ture Notes in Computer Science, pages 1–17, Berlin,
1996. Springer-Verlag.
[8] B. Liskov, S. Ghemawat, R. Gruber, P. Johnson, and
L. Shrira. Replication in the harp ﬁle system. In Pro-
ceedings of the thirteenth ACM symposium on Oper-
ating systems principles, pages 226–238. ACM Press,
1991.
[9] J.-F. Pˆaris and D. D. E. Long. Voting with regenerable
volatile witnesses. In Proceedings of the Seventh Inter-
national Conference on Data Engineering, April 8-12,
1991, Kobe, Japan, pages 112–119. IEEE Computer
Society, 1991.
[10] F. B. Schneider. Implementing fault-tolerant services
using the state machine approach: A tutorial. ACM
Computing Surveys, 22(4):299–319, Dec. 1990.
8
